{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìà Regresi√≥n Lineal con Una Variable\n",
    "\n",
    "## üìö Objetivos de Aprendizaje\n",
    "En este notebook aprender√°s:\n",
    "- Conceptos fundamentales de regresi√≥n lineal\n",
    "- Implementar y comprender la funci√≥n de costo\n",
    "- Desarrollar el algoritmo de gradient descent desde cero\n",
    "- Aplicar regresi√≥n lineal a un problema real de negocios\n",
    "\n",
    "## üéØ Problema de Negocio\n",
    "**Escenario**: Eres el CEO de una cadena de restaurantes y necesitas decidir en qu√© ciudades abrir nuevos locales. Tienes datos de:\n",
    "- üìä **Poblaci√≥n** de ciudades donde ya tienes restaurantes\n",
    "- üí∞ **Ganancias** mensuales de esos restaurantes\n",
    "\n",
    "**Objetivo**: Crear un modelo que prediga las ganancias potenciales bas√°ndose en la poblaci√≥n de la ciudad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "\n",
    "# Configuraci√≥n para mejores gr√°ficos\n",
    "plt.style.use('default')\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
    "print(\"üìä Listo para comenzar con regresi√≥n lineal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üìä Cargar y Explorar el Dataset\n",
    "\n",
    "Comenzaremos cargando nuestros datos de restaurantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìç FUNCI√ìN PARA CARGAR DATOS\n",
    "def cargar_datos_restaurantes():\n",
    "    \"\"\"\n",
    "    Simula datos reales de restaurantes\n",
    "    \n",
    "    Returns:\n",
    "        x_train: Poblaci√≥n de ciudades (en decenas de miles)\n",
    "        y_train: Ganancias mensuales (en miles de d√≥lares)\n",
    "    \"\"\"\n",
    "    # Datos sint√©ticos pero realistas\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Poblaci√≥n de ciudades (en decenas de miles)\n",
    "    x_train = np.array([6.1101, 5.5277, 8.5186, 7.0032, 5.8598, 8.3829, 7.4764,\n",
    "                       8.5781, 6.4862, 5.0546, 5.7107, 14.164, 5.734, 8.4084,\n",
    "                       5.6407, 5.3794, 6.3654, 5.1301, 6.4296, 7.0708, 6.1891,\n",
    "                       20.27, 5.4901, 6.3261, 5.5649, 18.945, 12.828, 10.957,\n",
    "                       13.176, 22.203, 5.2524, 6.5894, 9.2482, 5.8918, 8.2111,\n",
    "                       7.9334, 8.0959, 5.6063, 12.836, 6.3534, 5.4069, 6.8825,\n",
    "                       11.708, 5.7737, 7.8247, 7.0931, 5.0702, 5.8014, 11.7,\n",
    "                       5.5416, 7.5402, 5.3077, 7.4239, 7.6031, 6.3328, 6.3589,\n",
    "                       6.2742, 5.6397, 9.3102, 9.4536, 8.8254, 5.1793, 21.279,\n",
    "                       14.908, 18.959, 7.2182, 8.2951, 10.236, 5.4994, 20.341,\n",
    "                       10.136, 7.3345, 6.0062, 7.2259, 5.0269, 6.5479, 7.5386,\n",
    "                       5.0365, 10.274, 5.1077, 5.7292, 5.1884, 6.3557, 9.7687,\n",
    "                       6.5159, 8.5172, 9.1802, 6.002, 5.5204, 5.0594, 5.7077,\n",
    "                       7.6366, 5.8707, 5.3054, 8.2934, 13.394, 5.4369])\n",
    "    \n",
    "    # Ganancias correspondientes (en miles de d√≥lares)\n",
    "    y_train = np.array([17.592, 9.1302, 13.662, 11.854, 6.8233, 11.886, 4.3483,\n",
    "                       12.8, 6.5987, 4.8916, 3.5317, 14.754, 3.1626, 15.045,\n",
    "                       3.9899, 2.4445, 8.8368, 5.7652, 7.7956, 6.4125, 9.0936,\n",
    "                       22.638, 1.4421, 9.5582, 3.8235, 18.608, 19.833, 17.063,\n",
    "                       19.147, 25.927, 5.8707, 7.8247, 21.279, 17.929, 12.054,\n",
    "                       8.2951, 5.8014, 1.5757, 12.700, 8.4084, 12.394, 10.957,\n",
    "                       13.176, 3.1313, 7.3467, 1.8692, 1.8692, 18.945, 19.147,\n",
    "                       4.8234, 13.921, 4.9651, 5.4994, 1.8692, 6.2878, 18.758,\n",
    "                       8.2951, 1.8692, 12.700, 10.236, 12.700, 6.0815, 9.5447,\n",
    "                       9.1966, 20.175, 22.203, 18.046, 16.235, 18.758, 19.147,\n",
    "                       19.833, 19.147, 8.0959, 13.921, 4.9651, 1.8692, 14.046,\n",
    "                       6.1101, 17.929, 2.4445, 7.2259, 2.507, 5.4613, 21.013,\n",
    "                       16.235, 17.054, 10.136, 17.054, 13.921, 15.232, 1.8692,\n",
    "                       7.3467, 1.8692, 8.0959, 5.1099, 11.854, 2.4445])\n",
    "    \n",
    "    return x_train, y_train\n",
    "\n",
    "# Cargar los datos\n",
    "x_train, y_train = cargar_datos_restaurantes()\n",
    "\n",
    "print(f\"üìä Dataset cargado:\")\n",
    "print(f\"   ‚Ä¢ N√∫mero de ciudades: {len(x_train)}\")\n",
    "print(f\"   ‚Ä¢ Poblaci√≥n (primeras 5): {x_train[:5]}\")\n",
    "print(f\"   ‚Ä¢ Ganancias (primeras 5): {y_train[:5]}\")\n",
    "print(f\"\\nüìè Dimensiones:\")\n",
    "print(f\"   ‚Ä¢ x_train shape: {x_train.shape}\")\n",
    "print(f\"   ‚Ä¢ y_train shape: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 üìà Visualizaci√≥n de los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìç VISUALIZACI√ìN DEL DATASET\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot de los datos\n",
    "plt.scatter(x_train, y_train, marker='x', c='red', s=50, alpha=0.8, linewidth=2)\n",
    "\n",
    "# Personalizaci√≥n\n",
    "plt.title('üçΩÔ∏è Ganancias vs Poblaci√≥n por Ciudad', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Poblaci√≥n de la Ciudad (en decenas de miles)', fontsize=12)\n",
    "plt.ylabel('Ganancia Mensual (miles de $)', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# A√±adir algunas estad√≠sticas\n",
    "plt.text(0.02, 0.98, f'Ciudades: {len(x_train)}\\nPoblaci√≥n promedio: {np.mean(x_train):.1f}\\nGanancia promedio: ${np.mean(y_train):.1f}k', \n",
    "         transform=plt.gca().transAxes, verticalalignment='top',\n",
    "         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Observaciones del gr√°fico:\")\n",
    "print(\"   ‚Ä¢ Parece haber una relaci√≥n positiva entre poblaci√≥n y ganancias\")\n",
    "print(\"   ‚Ä¢ Los datos sugieren que una l√≠nea recta podr√≠a ajustar bien\")\n",
    "print(\"   ‚Ä¢ Hay algunos outliers pero el patr√≥n general es claro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üßÆ Fundamentos de Regresi√≥n Lineal\n",
    "\n",
    "### 2.1 El Modelo Matem√°tico\n",
    "\n",
    "La regresi√≥n lineal busca encontrar la **mejor l√≠nea recta** que represente la relaci√≥n entre:\n",
    "- **Variable independiente** (x): Poblaci√≥n\n",
    "- **Variable dependiente** (y): Ganancias\n",
    "\n",
    "**Ecuaci√≥n del modelo**:\n",
    "$$f_{w,b}(x) = wx + b$$\n",
    "\n",
    "Donde:\n",
    "- **w**: Pendiente (weight/peso) - cu√°nto aumentan las ganancias por cada unidad de poblaci√≥n\n",
    "- **b**: Intercepto (bias) - ganancia base cuando la poblaci√≥n es cero\n",
    "- **f_{w,b}(x)**: Predicci√≥n del modelo para entrada x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìç FUNCI√ìN DE PREDICCI√ìN\n",
    "\n",
    "def predecir(x, w, b):\n",
    "    \"\"\"\n",
    "    Hace predicciones usando regresi√≥n lineal\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Datos de entrada (poblaci√≥n)\n",
    "        w (scalar): Par√°metro peso/pendiente\n",
    "        b (scalar): Par√°metro bias/intercepto\n",
    "    \n",
    "    Returns:\n",
    "        y_pred (ndarray): Predicciones\n",
    "    \"\"\"\n",
    "    return w * x + b\n",
    "\n",
    "# Ejemplo con par√°metros iniciales\n",
    "w_inicial = 1.0\n",
    "b_inicial = 0.0\n",
    "\n",
    "# Hacer algunas predicciones\n",
    "ciudades_ejemplo = [5.0, 10.0, 15.0, 20.0]\n",
    "print(f\"üîÆ Predicciones con w={w_inicial}, b={b_inicial}:\")\n",
    "print(f\"{'Poblaci√≥n':>10} {'Predicci√≥n':>12}\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "for ciudad in ciudades_ejemplo:\n",
    "    pred = predecir(ciudad, w_inicial, b_inicial)\n",
    "    print(f\"{ciudad:>10.1f} ${pred:>10.1f}k\")\n",
    "\n",
    "print(f\"\\nüí° Con estos par√°metros, el modelo predice que por cada 10,000 habitantes\")\n",
    "print(f\"   adicionales, las ganancias aumentan en ${w_inicial * 1:.1f}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 üìä Visualizaci√≥n del Modelo Lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìç VISUALIZAR DIFERENTES MODELOS LINEALES\n",
    "\n",
    "# Diferentes combinaciones de par√°metros para explorar\n",
    "parametros = [\n",
    "    (0.5, 2, 'Pendiente baja'),\n",
    "    (1.0, 0, 'Pendiente media, sin intercepto'),\n",
    "    (1.5, -5, 'Pendiente alta, intercepto negativo'),\n",
    "    (0.8, 3, 'Modelo balanceado')\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, (w, b, titulo) in enumerate(parametros, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    \n",
    "    # Datos originales\n",
    "    plt.scatter(x_train, y_train, marker='x', c='red', s=30, alpha=0.6, label='Datos reales')\n",
    "    \n",
    "    # L√≠nea del modelo\n",
    "    x_line = np.linspace(0, 25, 100)\n",
    "    y_line = predecir(x_line, w, b)\n",
    "    plt.plot(x_line, y_line, 'b-', linewidth=2, label=f'f(x) = {w}x + {b}')\n",
    "    \n",
    "    plt.title(f'{titulo}\\nw={w}, b={b}', fontweight='bold')\n",
    "    plt.xlabel('Poblaci√≥n (decenas de miles)')\n",
    "    plt.ylabel('Ganancias (miles $)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(4, 24)\n",
    "    plt.ylim(-5, 30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üìä Observaciones:\")\n",
    "print(\"   ‚Ä¢ Diferentes par√°metros (w,b) producen l√≠neas muy diferentes\")\n",
    "print(\"   ‚Ä¢ Necesitamos un m√©todo sistem√°tico para encontrar los mejores par√°metros\")\n",
    "print(\"   ‚Ä¢ El 'mejor' modelo minimiza la distancia entre predicciones y datos reales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üí∞ Funci√≥n de Costo (Cost Function)\n",
    "\n",
    "### 3.1 ¬øQu√© es la Funci√≥n de Costo?\n",
    "\n",
    "La funci√≥n de costo mide **qu√© tan bien** nuestro modelo se ajusta a los datos:\n",
    "- **Costo bajo** = Predicciones cercanas a valores reales = Buen modelo\n",
    "- **Costo alto** = Predicciones lejanas a valores reales = Mal modelo\n",
    "\n",
    "**F√≥rmula de la funci√≥n de costo (Mean Squared Error)**:\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum_{i=0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Donde:\n",
    "- **m**: N√∫mero de ejemplos de entrenamiento\n",
    "- **f_{w,b}(x^{(i)})**: Predicci√≥n para el ejemplo i\n",
    "- **y^{(i)}**: Valor real para el ejemplo i\n",
    "- **1/2**: Factor de conveniencia (simplifica las derivadas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìç IMPLEMENTACI√ìN DE LA FUNCI√ìN DE COSTO\n",
    "\n",
    "def calcular_costo(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Calcula la funci√≥n de costo para regresi√≥n lineal\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Datos de entrada (poblaci√≥n)\n",
    "        y (ndarray): Valores objetivo (ganancias reales)\n",
    "        w (scalar): Par√°metro peso\n",
    "        b (scalar): Par√°metro bias\n",
    "    \n",
    "    Returns:\n",
    "        costo_total (float): El costo del modelo con par√°metros w,b\n",
    "    \"\"\"\n",
    "    # N√∫mero de ejemplos\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    # Inicializar costo\n",
    "    costo_total = 0\n",
    "    \n",
    "    # Calcular el costo para cada ejemplo\n",
    "    for i in range(m):\n",
    "        # Predicci√≥n para el ejemplo i\n",
    "        f_wb = w * x[i] + b\n",
    "        \n",
    "        # Error cuadr√°tico\n",
    "        error_cuadratico = (f_wb - y[i]) ** 2\n",
    "        \n",
    "        # Acumular costo\n",
    "        costo_total += error_cuadratico\n",
    "    \n",
    "    # Calcular costo promedio\n",
    "    costo_total = costo_total / (2 * m)\n",
    "    \n",
    "    return costo_total\n",
    "\n",
    "# Probar la funci√≥n con diferentes par√°metros\n",
    "parametros_prueba = [(0, 0), (1, 0), (1.5, -2), (0.8, 3)]\n",
    "\n",
    "print(f\"üß™ Pruebas de la funci√≥n de costo:\")\n",
    "print(f\"{'w':>6} {'b':>6} {'Costo':>12}\")\n",
    "print(\"-\" * 26)\n",
    "\n",
    "for w, b in parametros_prueba:\n",
    "    costo = calcular_costo(x_train, y_train, w, b)\n",
    "    print(f\"{w:>6.1f} {b:>6.1f} {costo:>12.2f}\")\n",
    "\n",
    "print(f\"\\nüí° Interpretaci√≥n:\")\n",
    "print(f\"   ‚Ä¢ Menores valores de costo indican mejor ajuste\")\n",
    "print(f\"   ‚Ä¢ El objetivo es encontrar w,b que minimicen J(w,b)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 üéØ Visualizaci√≥n de la Funci√≥n de Costo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìç VISUALIZAR C√ìMO CAMBIA EL COSTO\n",
    "\n",
    "# Fijamos b=0 y variamos w para ver c√≥mo cambia el costo\n",
    "w_valores = np.linspace(-2, 4, 100)\n",
    "costos = []\n",
    "\n",
    "b_fijo = 0\n",
    "for w in w_valores:\n",
    "    costo = calcular_costo(x_train, y_train, w, b_fijo)\n",
    "    costos.append(costo)\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Funci√≥n de costo vs w\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(w_valores, costos, 'b-', linewidth=2)\n",
    "plt.title(f'Funci√≥n de Costo vs w (b={b_fijo})', fontweight='bold')\n",
    "plt.xlabel('Par√°metro w')\n",
    "plt.ylabel('Costo J(w,b)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Marcar el m√≠nimo\n",
    "min_idx = np.argmin(costos)\n",
    "w_optimo = w_valores[min_idx]\n",
    "costo_minimo = costos[min_idx]\n",
    "plt.plot(w_optimo, costo_minimo, 'ro', markersize=10, label=f'M√≠nimo: w={w_optimo:.2f}')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Modelo con w √≥ptimo\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(x_train, y_train, marker='x', c='red', s=30, alpha=0.6, label='Datos reales')\n",
    "\n",
    "# L√≠nea con w √≥ptimo\n",
    "x_line = np.linspace(0, 25, 100)\n",
    "y_line = predecir(x_line, w_optimo, b_fijo)\n",
    "plt.plot(x_line, y_line, 'g-', linewidth=2, label=f'Mejor ajuste: w={w_optimo:.2f}, b={b_fijo}')\n",
    "\n",
    "plt.title('Modelo con w √ìptimo', fontweight='bold')\n",
    "plt.xlabel('Poblaci√≥n (decenas de miles)')\n",
    "plt.ylabel('Ganancias (miles $)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üéØ Resultado del an√°lisis:\")\n",
    "print(f\"   ‚Ä¢ w √≥ptimo (con b=0): {w_optimo:.3f}\")\n",
    "print(f\"   ‚Ä¢ Costo m√≠nimo: {costo_minimo:.3f}\")\n",
    "print(f\"   ‚Ä¢ La funci√≥n de costo tiene forma de par√°bola (convexa)\")\n",
    "print(f\"   ‚Ä¢ Existe un √∫nico m√≠nimo global\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üéØ Gradient Descent\n",
    "\n",
    "### 4.1 ¬øQu√© es Gradient Descent?\n",
    "\n",
    "**Gradient Descent** es un algoritmo de optimizaci√≥n que encuentra autom√°ticamente los mejores par√°metros w y b:\n",
    "\n",
    "1. **Inicia** con valores aleatorios de w y b\n",
    "2. **Calcula** qu√© tan \"empinada\" est√° la funci√≥n de costo (gradiente)\n",
    "3. **Da un paso** en la direcci√≥n que reduce el costo\n",
    "4. **Repite** hasta encontrar el m√≠nimo\n",
    "\n",
    "**Algoritmo**:\n",
    "$$\\begin{align}\n",
    "\\text{repetir hasta convergencia: } \\{ \\\\\n",
    "w &= w - \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\\\\n",
    "b &= b - \\alpha \\frac{\\partial J(w,b)}{\\partial b} \\\\\n",
    "\\}\n",
    "\\end{align}$$\n",
    "\n",
    "**Gradientes (derivadas parciales)**:\n",
    "$$\\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{m} \\sum_{i=0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)}$$\n",
    "$$\\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum_{i=0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})$$\n",
    "\n",
    "Donde **Œ±** es el **learning rate** (tasa de aprendizaje)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìç IMPLEMENTACI√ìN DE GRADIENTES\n",
    "\n",
    "def calcular_gradientes(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Calcula los gradientes (derivadas parciales) de la funci√≥n de costo\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Datos de entrada\n",
    "        y (ndarray): Valores objetivo\n",
    "        w (scalar): Par√°metro peso actual\n",
    "        b (scalar): Par√°metro bias actual\n",
    "    \n",
    "    Returns:\n",
    "        dj_dw (scalar): Gradiente respecto a w\n",
    "        dj_db (scalar): Gradiente respecto a b\n",
    "    \"\"\"\n",
    "    # N√∫mero de ejemplos\n",
    "    m = x.shape[0]\n",
    "    \n",
    "    # Inicializar gradientes\n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    # Calcular gradientes para cada ejemplo\n",
    "    for i in range(m):\n",
    "        # Predicci√≥n para el ejemplo i\n",
    "        f_wb = w * x[i] + b\n",
    "        \n",
    "        # Error para el ejemplo i\n",
    "        error = f_wb - y[i]\n",
    "        \n",
    "        # Acumular gradientes\n",
    "        dj_dw += error * x[i]  # Gradiente respecto a w\n",
    "        dj_db += error         # Gradiente respecto a b\n",
    "    \n",
    "    # Promediar gradientes\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    \n",
    "    return dj_dw, dj_db\n",
    "\n",
    "# Probar c√°lculo de gradientes\n",
    "w_test = 0.5\n",
    "b_test = 1.0\n",
    "\n",
    "dj_dw, dj_db = calcular_gradientes(x_train, y_train, w_test, b_test)\n",
    "costo_actual = calcular_costo(x_train, y_train, w_test, b_test)\n",
    "\n",
    "print(f\"üßÆ C√°lculo de gradientes:\")\n",
    "print(f\"   Par√°metros actuales: w={w_test}, b={b_test}\")\n",
    "print(f\"   Costo actual: {costo_actual:.3f}\")\n",
    "print(f\"   Gradiente dJ/dw: {dj_dw:.4f}\")\n",
    "print(f\"   Gradiente dJ/db: {dj_db:.4f}\")\n",
    "print(f\"\\nüí° Interpretaci√≥n:\")\n",
    "print(f\"   ‚Ä¢ dJ/dw > 0: El costo aumenta cuando w aumenta ‚Üí reducir w\")\n",
    "print(f\"   ‚Ä¢ dJ/db > 0: El costo aumenta cuando b aumenta ‚Üí reducir b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 üöÄ Implementaci√≥n Completa de Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìç ALGORITMO COMPLETO DE GRADIENT DESCENT\n",
    "\n",
    "def gradient_descent(x, y, w_inicial, b_inicial, learning_rate, num_iteraciones):\n",
    "    \"\"\"\n",
    "    Implementa el algoritmo de gradient descent para regresi√≥n lineal\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Datos de entrada\n",
    "        y (ndarray): Valores objetivo\n",
    "        w_inicial (scalar): Valor inicial de w\n",
    "        b_inicial (scalar): Valor inicial de b\n",
    "        learning_rate (float): Tasa de aprendizaje Œ±\n",
    "        num_iteraciones (int): N√∫mero de iteraciones\n",
    "    \n",
    "    Returns:\n",
    "        w (scalar): Par√°metro w optimizado\n",
    "        b (scalar): Par√°metro b optimizado\n",
    "        historial_costos (list): Historia del costo en cada iteraci√≥n\n",
    "        historial_w (list): Historia de w en cada iteraci√≥n\n",
    "    \"\"\"\n",
    "    # Inicializar par√°metros\n",
    "    w = copy.deepcopy(w_inicial)\n",
    "    b = b_inicial\n",
    "    \n",
    "    # Listas para almacenar historia\n",
    "    historial_costos = []\n",
    "    historial_w = []\n",
    "    \n",
    "    # Gradient descent loop\n",
    "    for i in range(num_iteraciones):\n",
    "        # Calcular gradientes\n",
    "        dj_dw, dj_db = calcular_gradientes(x, y, w, b)\n",
    "        \n",
    "        # Actualizar par√°metros\n",
    "        w = w - learning_rate * dj_dw\n",
    "        b = b - learning_rate * dj_db\n",
    "        \n",
    "        # Calcular costo actual\n",
    "        if i < 100000:  # Evitar usar mucha memoria\n",
    "            costo = calcular_costo(x, y, w, b)\n",
    "            historial_costos.append(costo)\n",
    "        \n",
    "        # Guardar par√°metros\n",
    "        historial_w.append(w)\n",
    "        \n",
    "        # Imprimir progreso\n",
    "        if i % math.ceil(num_iteraciones / 10) == 0:\n",
    "            costo_actual = calcular_costo(x, y, w, b)\n",
    "            print(f\"Iteraci√≥n {i:4d}: Costo = {costo_actual:8.4f}, w = {w:8.4f}, b = {b:8.4f}\")\n",
    "    \n",
    "    return w, b, historial_costos, historial_w\n",
    "\n",
    "# Ejecutar gradient descent\n",
    "print(\"üöÄ Ejecutando Gradient Descent...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Par√°metros del algoritmo\n",
    "w_inicial = 0.0\n",
    "b_inicial = 0.0\n",
    "learning_rate = 0.01\n",
    "iteraciones = 1500\n",
    "\n",
    "# Entrenar el modelo\n",
    "w_final, b_final, hist_costos, hist_w = gradient_descent(\n",
    "    x_train, y_train, w_inicial, b_inicial, learning_rate, iteraciones\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ Entrenamiento completado!\")\n",
    "print(f\"   Par√°metros finales: w = {w_final:.4f}, b = {b_final:.4f}\")\n",
    "print(f\"   Costo final: {hist_costos[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 üìä Visualizaci√≥n del Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìç VISUALIZAR EL PROCESO DE ENTRENAMIENTO\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: Evoluci√≥n del costo\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(hist_costos, 'b-', linewidth=2)\n",
    "plt.title('üîª Evoluci√≥n del Costo', fontweight='bold')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo J(w,b)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# A√±adir punto final\n",
    "plt.plot(len(hist_costos)-1, hist_costos[-1], 'ro', markersize=8)\n",
    "plt.annotate(f'Final: {hist_costos[-1]:.3f}', \n",
    "             xy=(len(hist_costos)-1, hist_costos[-1]),\n",
    "             xytext=(len(hist_costos)*0.7, hist_costos[-1]*1.2),\n",
    "             arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "# Subplot 2: Evoluci√≥n de w\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(hist_w, 'g-', linewidth=2)\n",
    "plt.title('üìà Evoluci√≥n de w', fontweight='bold')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Par√°metro w')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# L√≠nea horizontal en valor final\n",
    "plt.axhline(y=w_final, color='red', linestyle='--', alpha=0.7, \n",
    "            label=f'w final = {w_final:.3f}')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 3: Modelo final vs datos\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(x_train, y_train, marker='x', c='red', s=30, alpha=0.6, label='Datos reales')\n",
    "\n",
    "# L√≠nea del modelo entrenado\n",
    "x_line = np.linspace(0, 25, 100)\n",
    "y_line = predecir(x_line, w_final, b_final)\n",
    "plt.plot(x_line, y_line, 'b-', linewidth=3, \n",
    "         label=f'Modelo: f(x) = {w_final:.2f}x + {b_final:.2f}')\n",
    "\n",
    "plt.title('üéØ Modelo Final', fontweight='bold')\n",
    "plt.xlabel('Poblaci√≥n (decenas de miles)')\n",
    "plt.ylabel('Ganancias (miles $)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(4, 24)\n",
    "plt.ylim(-5, 30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä An√°lisis del entrenamiento:\")\n",
    "print(f\"   ‚Ä¢ El costo disminuy√≥ de {hist_costos[0]:.3f} a {hist_costos[-1]:.3f}\")\n",
    "print(f\"   ‚Ä¢ Reducci√≥n del costo: {((hist_costos[0] - hist_costos[-1])/hist_costos[0]*100):.1f}%\")\n",
    "print(f\"   ‚Ä¢ El par√°metro w convergi√≥ a {w_final:.3f}\")\n",
    "print(f\"   ‚Ä¢ El modelo final tiene un buen ajuste visual a los datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üîÆ Hacer Predicciones con el Modelo Entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìç HACER PREDICCIONES PARA NUEVAS CIUDADES\n",
    "\n",
    "# Ciudades candidatas para nuevos restaurantes\n",
    "nuevas_ciudades = [\n",
    "    (3.5, \"Ciudad Peque√±a (35,000 hab)\"),\n",
    "    (7.0, \"Ciudad Mediana (70,000 hab)\"),\n",
    "    (12.0, \"Ciudad Grande (120,000 hab)\"),\n",
    "    (18.0, \"Metr√≥poli (180,000 hab)\")\n",
    "]\n",
    "\n",
    "print(f\"üîÆ Predicciones para Nuevas Ciudades\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"Modelo: f(x) = {w_final:.3f}x + {b_final:.3f}\")\n",
    "print(f\"\")\n",
    "print(f\"{'Poblaci√≥n':>15} {'Predicci√≥n':>15} {'Ganancia Anual':>18}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "predicciones = []\n",
    "for poblacion, descripcion in nuevas_ciudades:\n",
    "    # Hacer predicci√≥n\n",
    "    ganancia_mensual = predecir(poblacion, w_final, b_final)\n",
    "    ganancia_anual = ganancia_mensual * 12\n",
    "    predicciones.append((poblacion, ganancia_mensual))\n",
    "    \n",
    "    print(f\"{descripcion:>15} ${ganancia_mensual:>10.1f}k ${ganancia_anual:>13.0f}k\")\n",
    "\n",
    "# Recomendaciones de negocio\n",
    "print(f\"\\nüíº Recomendaciones de Negocio:\")\n",
    "mejor_ciudad = max(predicciones, key=lambda x: x[1])\n",
    "print(f\"   ‚Ä¢ Mejor oportunidad: Ciudad con {mejor_ciudad[0]*10:.0f}k habitantes\")\n",
    "print(f\"   ‚Ä¢ Ganancia esperada: ${mejor_ciudad[1]:.1f}k mensuales\")\n",
    "print(f\"   ‚Ä¢ ROI anual estimado: ${mejor_ciudad[1]*12:.0f}k\")\n",
    "\n",
    "# An√°lisis de riesgo\n",
    "umbral_rentabilidad = 5.0  # $5k mensuales m√≠nimo\n",
    "ciudades_rentables = [p for p in predicciones if p[1] >= umbral_rentabilidad]\n",
    "print(f\"\\n‚ö†Ô∏è  An√°lisis de Riesgo:\")\n",
    "print(f\"   ‚Ä¢ Ciudades rentables (>{umbral_rentabilidad}k/mes): {len(ciudades_rentables)}/{len(predicciones)}\")\n",
    "print(f\"   ‚Ä¢ Umbral de poblaci√≥n m√≠nima: ~{(umbral_rentabilidad - b_final) / w_final * 10:.0f}k habitantes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 üìà Visualizaci√≥n de Predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìç VISUALIZAR PREDICCIONES\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Datos de entrenamiento\n",
    "plt.scatter(x_train, y_train, marker='x', c='blue', s=50, alpha=0.7, \n",
    "           label='Datos hist√≥ricos', linewidth=2)\n",
    "\n",
    "# Modelo entrenado\n",
    "x_line = np.linspace(0, 20, 100)\n",
    "y_line = predecir(x_line, w_final, b_final)\n",
    "plt.plot(x_line, y_line, 'b-', linewidth=3, alpha=0.8,\n",
    "         label=f'Modelo: f(x) = {w_final:.2f}x + {b_final:.2f}')\n",
    "\n",
    "# Nuevas predicciones\n",
    "for poblacion, ganancia in predicciones:\n",
    "    plt.scatter(poblacion, ganancia, s=150, c='red', marker='o', \n",
    "               edgecolor='darkred', linewidth=2, zorder=5)\n",
    "    plt.annotate(f'${ganancia:.1f}k', \n",
    "                xy=(poblacion, ganancia), \n",
    "                xytext=(poblacion+0.5, ganancia+2),\n",
    "                fontweight='bold',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "# L√≠nea de rentabilidad\n",
    "plt.axhline(y=umbral_rentabilidad, color='green', linestyle='--', linewidth=2, \n",
    "           alpha=0.7, label=f'Umbral rentabilidad (${umbral_rentabilidad}k/mes)')\n",
    "\n",
    "# Personalizaci√≥n\n",
    "plt.title('üéØ Modelo de Predicci√≥n: Ganancias vs Poblaci√≥n', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Poblaci√≥n de la Ciudad (en decenas de miles)', fontsize=12)\n",
    "plt.ylabel('Ganancia Mensual (miles de $)', fontsize=12)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, 20)\n",
    "plt.ylim(-2, 25)\n",
    "\n",
    "# A√±adir texto explicativo\n",
    "plt.text(0.5, 22, \n",
    "         'üî¥ Puntos rojos: Nuevas predicciones\\n'+ \n",
    "         '‚úï Puntos azules: Datos hist√≥ricos\\n'+\n",
    "         f'üìà Modelo R¬≤: {1 - (hist_costos[-1] / np.var(y_train)):.3f}',\n",
    "         fontsize=10,\n",
    "         bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä El gr√°fico muestra c√≥mo el modelo entrenado predice ganancias para nuevas ciudades\")\n",
    "print(f\"üéØ Las predicciones siguen la tendencia aprendida de los datos hist√≥ricos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üìä Evaluaci√≥n del Modelo\n",
    "\n",
    "### 6.1 M√©tricas de Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìç EVALUACI√ìN COMPLETA DEL MODELO\n",
    "\n",
    "def evaluar_modelo(x, y, w, b):\n",
    "    \"\"\"\n",
    "    Eval√∫a el rendimiento del modelo con m√∫ltiples m√©tricas\n",
    "    \"\"\"\n",
    "    # Hacer predicciones\n",
    "    predicciones = predecir(x, w, b)\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    n = len(y)\n",
    "    \n",
    "    # Error Cuadr√°tico Medio (MSE)\n",
    "    mse = np.sum((predicciones - y)**2) / n\n",
    "    \n",
    "    # Ra√≠z del Error Cuadr√°tico Medio (RMSE)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Error Absoluto Medio (MAE)\n",
    "    mae = np.sum(np.abs(predicciones - y)) / n\n",
    "    \n",
    "    # Coeficiente de Determinaci√≥n (R¬≤)\n",
    "    ss_res = np.sum((y - predicciones)**2)\n",
    "    ss_tot = np.sum((y - np.mean(y))**2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R¬≤': r2,\n",
    "        'predicciones': predicciones\n",
    "    }\n",
    "\n",
    "# Evaluar modelo\n",
    "metricas = evaluar_modelo(x_train, y_train, w_final, b_final)\n",
    "\n",
    "print(f\"üìä Evaluaci√≥n del Modelo\")\n",
    "print(f\"=\" * 30)\n",
    "print(f\"MSE (Error Cuadr√°tico Medio):     {metricas['MSE']:.3f}\")\n",
    "print(f\"RMSE (Ra√≠z Error Cuadr√°tico):     {metricas['RMSE']:.3f}\")\n",
    "print(f\"MAE (Error Absoluto Medio):       {metricas['MAE']:.3f}\")\n",
    "print(f\"R¬≤ (Coeficiente Determinaci√≥n):   {metricas['R¬≤']:.3f}\")\n",
    "\n",
    "print(f\"\\nüí° Interpretaci√≥n:\")\n",
    "print(f\"   ‚Ä¢ RMSE = {metricas['RMSE']:.1f}k: Error t√≠pico de ¬±{metricas['RMSE']:.1f}k en predicciones\")\n",
    "print(f\"   ‚Ä¢ MAE = {metricas['MAE']:.1f}k: Error promedio absoluto\")\n",
    "print(f\"   ‚Ä¢ R¬≤ = {metricas['R¬≤']:.3f}: El modelo explica {metricas['R¬≤']*100:.1f}% de la varianza\")\n",
    "\n",
    "if metricas['R¬≤'] > 0.8:\n",
    "    print(f\"   ‚úÖ Excelente ajuste (R¬≤ > 0.8)\")\n",
    "elif metricas['R¬≤'] > 0.6:\n",
    "    print(f\"   ‚úÖ Buen ajuste (R¬≤ > 0.6)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Ajuste mejorable (R¬≤ < 0.6)\")\n",
    "\n",
    "# An√°lisis de residuos\n",
    "residuos = y_train - metricas['predicciones']\n",
    "print(f\"\\nüìà An√°lisis de Residuos:\")\n",
    "print(f\"   ‚Ä¢ Media de residuos: {np.mean(residuos):.3f} (deber√≠a ser ~0)\")\n",
    "print(f\"   ‚Ä¢ Desviaci√≥n est√°ndar: {np.std(residuos):.3f}\")\n",
    "print(f\"   ‚Ä¢ Residuo m√°ximo: {np.max(np.abs(residuos)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 üìä Visualizaci√≥n de Residuos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìç AN√ÅLISIS VISUAL DE RESIDUOS\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: Predicciones vs Valores Reales\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(metricas['predicciones'], y_train, alpha=0.7, s=50)\n",
    "\n",
    "# L√≠nea diagonal perfecta\n",
    "min_val = min(np.min(metricas['predicciones']), np.min(y_train))\n",
    "max_val = max(np.max(metricas['predicciones']), np.max(y_train))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, \n",
    "         label='Predicci√≥n perfecta')\n",
    "\n",
    "plt.title('Predicciones vs Valores Reales', fontweight='bold')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Reales')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Residuos vs Predicciones\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(metricas['predicciones'], residuos, alpha=0.7, s=50)\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Residuos vs Predicciones', fontweight='bold')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Residuos (Real - Predicci√≥n)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Histograma de Residuos\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(residuos, bins=15, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Distribuci√≥n de Residuos', fontweight='bold')\n",
    "plt.xlabel('Residuos')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä An√°lisis visual:\")\n",
    "print(f\"   ‚Ä¢ Gr√°fico 1: Puntos cerca de la l√≠nea roja = buenas predicciones\")\n",
    "print(f\"   ‚Ä¢ Gr√°fico 2: Residuos distribuidos alrededor de 0 = modelo no sesgado\")\n",
    "print(f\"   ‚Ä¢ Gr√°fico 3: Distribuci√≥n aproximadamente normal = buen modelo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. üß™ Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: Experimentar con Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ EJERCICIO 1: Efecto del Learning Rate\n",
    "\n",
    "print(\"üß™ Ejercicio 1: Experimentar con diferentes Learning Rates\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5]\n",
    "iteraciones_exp = 500\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, lr in enumerate(learning_rates, 1):\n",
    "    # Entrenar con diferente learning rate\n",
    "    w_lr, b_lr, hist_costos_lr, _ = gradient_descent(\n",
    "        x_train, y_train, 0.0, 0.0, lr, iteraciones_exp\n",
    "    )\n",
    "    \n",
    "    # Subplot para cada learning rate\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.plot(hist_costos_lr, linewidth=2)\n",
    "    plt.title(f'Learning Rate = {lr}\\nFinal: w={w_lr:.3f}, b={b_lr:.3f}', \n",
    "              fontweight='bold')\n",
    "    plt.xlabel('Iteraciones')\n",
    "    plt.ylabel('Costo')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Anotar costo final\n",
    "    if hist_costos_lr:  # Si hay historia de costos\n",
    "        plt.annotate(f'Final: {hist_costos_lr[-1]:.3f}',\n",
    "                    xy=(len(hist_costos_lr)-1, hist_costos_lr[-1]),\n",
    "                    xytext=(len(hist_costos_lr)*0.6, hist_costos_lr[-1]*1.1),\n",
    "                    arrowprops=dict(arrowstyle='->', alpha=0.7))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Observaciones:\")\n",
    "print(f\"   ‚Ä¢ Learning rate muy peque√±o (0.001): Converge lentamente\")\n",
    "print(f\"   ‚Ä¢ Learning rate moderado (0.01): Convergencia estable\")\n",
    "print(f\"   ‚Ä¢ Learning rate alto (0.1): Convergencia m√°s r√°pida\")\n",
    "print(f\"   ‚Ä¢ Learning rate muy alto (0.5): Puede oscilar o diverger\")\n",
    "print(f\"\\nüí° La elecci√≥n del learning rate es crucial para un entrenamiento eficiente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Predicciones Personalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß™ EJERCICIO 2: Crear tus propias predicciones\n",
    "\n",
    "print(\"üß™ Ejercicio 2: Predicciones Personalizadas\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "def analizar_inversion(poblacion_miles, inversion_inicial, costos_mensuales):\n",
    "    \"\"\"\n",
    "    Analiza la viabilidad de invertir en una ciudad espec√≠fica\n",
    "    \n",
    "    Args:\n",
    "        poblacion_miles: Poblaci√≥n en miles\n",
    "        inversion_inicial: Costo inicial del restaurante\n",
    "        costos_mensuales: Costos operativos mensuales\n",
    "    \"\"\"\n",
    "    # Convertir poblaci√≥n a decenas de miles para el modelo\n",
    "    poblacion_modelo = poblacion_miles / 10\n",
    "    \n",
    "    # Predicci√≥n de ingresos\n",
    "    ingresos_mensuales = predecir(poblacion_modelo, w_final, b_final) * 1000  # Convertir a d√≥lares\n",
    "    ganancia_neta_mensual = ingresos_mensuales - costos_mensuales\n",
    "    ganancia_anual = ganancia_neta_mensual * 12\n",
    "    \n",
    "    # C√°lculo del tiempo de retorno de inversi√≥n\n",
    "    if ganancia_neta_mensual > 0:\n",
    "        tiempo_retorno_meses = inversion_inicial / ganancia_neta_mensual\n",
    "        tiempo_retorno_a√±os = tiempo_retorno_meses / 12\n",
    "    else:\n",
    "        tiempo_retorno_a√±os = float('inf')\n",
    "    \n",
    "    return {\n",
    "        'ingresos_mensuales': ingresos_mensuales,\n",
    "        'ganancia_neta_mensual': ganancia_neta_mensual,\n",
    "        'ganancia_anual': ganancia_anual,\n",
    "        'tiempo_retorno_a√±os': tiempo_retorno_a√±os,\n",
    "        'roi_anual': (ganancia_anual / inversion_inicial * 100) if inversion_inicial > 0 else 0\n",
    "    }\n",
    "\n",
    "# Casos de estudio\n",
    "casos = [\n",
    "    {\"ciudad\": \"Villa Peque√±a\", \"poblacion\": 45000, \"inversion\": 150000, \"costos\": 8000},\n",
    "    {\"ciudad\": \"Ciudad Media\", \"poblacion\": 85000, \"inversion\": 200000, \"costos\": 12000},\n",
    "    {\"ciudad\": \"Gran Ciudad\", \"poblacion\": 150000, \"inversion\": 300000, \"costos\": 18000}\n",
    "]\n",
    "\n",
    "print(f\"{'Ciudad':<15} {'Poblaci√≥n':<10} {'Ingresos/mes':<12} {'Ganancia/mes':<13} {'ROI Anual':<10} {'Retorno':<10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for caso in casos:\n",
    "    resultado = analizar_inversion(caso['poblacion'], caso['inversion'], caso['costos'])\n",
    "    \n",
    "    print(f\"{caso['ciudad']:<15} {caso['poblacion']:<10,} \"\n",
    "          f\"${resultado['ingresos_mensuales']:<11,.0f} \"\n",
    "          f\"${resultado['ganancia_neta_mensual']:<12,.0f} \"\n",
    "          f\"{resultado['roi_anual']:<9.1f}% \"\n",
    "          f\"{resultado['tiempo_retorno_a√±os']:<9.1f} a√±os\")\n",
    "\n",
    "print(f\"\\nüéØ Recomendaci√≥n basada en el an√°lisis:\")\n",
    "mejor_caso = max(casos, key=lambda x: analizar_inversion(x['poblacion'], x['inversion'], x['costos'])['roi_anual'])\n",
    "mejor_resultado = analizar_inversion(mejor_caso['poblacion'], mejor_caso['inversion'], mejor_caso['costos'])\n",
    "\n",
    "print(f\"   ‚Ä¢ Mejor opci√≥n: {mejor_caso['ciudad']}\")\n",
    "print(f\"   ‚Ä¢ ROI anual: {mejor_resultado['roi_anual']:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Tiempo de retorno: {mejor_resultado['tiempo_retorno_a√±os']:.1f} a√±os\")\n",
    "\n",
    "if mejor_resultado['roi_anual'] > 15:\n",
    "    print(f\"   ‚úÖ Inversi√≥n altamente recomendada\")\n",
    "elif mejor_resultado['roi_anual'] > 8:\n",
    "    print(f\"   ‚úÖ Inversi√≥n viable\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Inversi√≥n de alto riesgo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Resumen y Conceptos Clave\n",
    "\n",
    "### ‚úÖ Lo que has aprendido:\n",
    "\n",
    "#### 1. **Regresi√≥n Lineal Fundamentals**:\n",
    "   - **Modelo**: $f_{w,b}(x) = wx + b$\n",
    "   - **Objetivo**: Encontrar la mejor l√≠nea que ajuste los datos\n",
    "   - **Par√°metros**: w (pendiente) y b (intercepto)\n",
    "\n",
    "#### 2. **Funci√≥n de Costo**:\n",
    "   - **Prop√≥sito**: Medir qu√© tan bien el modelo predice los datos\n",
    "   - **F√≥rmula**: $J(w,b) = \\frac{1}{2m} \\sum_{i=0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$\n",
    "   - **Interpretaci√≥n**: Menor costo = mejor modelo\n",
    "\n",
    "#### 3. **Gradient Descent**:\n",
    "   - **Algoritmo**: Optimizaci√≥n iterativa para minimizar el costo\n",
    "   - **Actualizaci√≥n**: $w = w - \\alpha \\frac{\\partial J}{\\partial w}$, $b = b - \\alpha \\frac{\\partial J}{\\partial b}$\n",
    "   - **Learning Rate (Œ±)**: Controla el tama√±o de los pasos\n",
    "\n",
    "#### 4. **Implementaci√≥n Pr√°ctica**:\n",
    "   - C√°lculo de gradientes usando derivadas parciales\n",
    "   - Entrenamiento iterativo hasta convergencia\n",
    "   - Evaluaci√≥n usando m√©tricas (MSE, RMSE, R¬≤)\n",
    "\n",
    "### üéØ Aplicaci√≥n en el Mundo Real:\n",
    "- **Problema de negocio**: Predicci√≥n de ganancias para ubicaciones de restaurantes\n",
    "- **Variables**: Poblaci√≥n (entrada) ‚Üí Ganancias (salida)\n",
    "- **Resultado**: Modelo que predice ganancias con {metricas['R¬≤']*100:.1f}% de precisi√≥n\n",
    "\n",
    "### üöÄ Pr√≥ximos pasos:\n",
    "- **Regresi√≥n m√∫ltiple**: Modelos con varias variables de entrada\n",
    "- **Feature scaling**: Normalizaci√≥n para mejorar convergencia\n",
    "- **Regularizaci√≥n**: T√©cnicas para evitar overfitting\n",
    "\n",
    "### üí° Puntos clave para recordar:\n",
    "1. **La regresi√≥n lineal asume relaci√≥n lineal** entre entrada y salida\n",
    "2. **Gradient descent siempre converge** para funciones convexas como MSE\n",
    "3. **Learning rate es cr√≠tico**: muy alto = oscilaci√≥n, muy bajo = lentitud\n",
    "4. **Evaluaci√≥n es esencial**: usa m√∫ltiples m√©tricas para validar el modelo\n",
    "5. **Visualizaci√≥n ayuda**: siempre grafica datos y resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}