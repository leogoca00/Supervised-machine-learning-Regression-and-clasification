{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Scikit-Learn: Regresi√≥n Lineal en Producci√≥n\n",
    "\n",
    "## üìö Objetivos de Aprendizaje\n",
    "En este notebook aprender√°s:\n",
    "- **Transici√≥n** de implementaci√≥n manual a herramientas profesionales\n",
    "- **Scikit-Learn** para regresi√≥n lineal y polinomial\n",
    "- **Pipeline completo** de ML con preprocessing autom√°tico\n",
    "- **Validaci√≥n robusta** con cross-validation\n",
    "- **Comparaci√≥n** entre implementaci√≥n manual vs sklearn\n",
    "\n",
    "## üéØ ¬øPor qu√© Scikit-Learn?\n",
    "**Has aprendido los fundamentos** implementando desde cero:\n",
    "- ‚úÖ Gradient descent\n",
    "- ‚úÖ Feature scaling  \n",
    "- ‚úÖ Feature engineering\n",
    "- ‚úÖ Evaluaci√≥n de modelos\n",
    "\n",
    "**Ahora es momento de usar herramientas profesionales:**\n",
    "- üöÄ **Velocidad**: Implementaciones optimizadas en C/Cython\n",
    "- üõ°Ô∏è **Robustez**: Manejo de edge cases y errores\n",
    "- üîß **Funcionalidad**: Pipelines, cross-validation, m√©tricas\n",
    "- üìà **Escalabilidad**: Datasets grandes y algoritmos avanzados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor, Ridge, Lasso\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, validation_curve, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.datasets import make_regression, load_diabetes\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n\n",
    "plt.style.use('default')\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"‚úÖ Scikit-Learn y librer√≠as importadas\")\n",
    "print(f\"üì¶ Sklearn version: {__import__('sklearn').__version__}\")\n",
    "print(\"üöÄ Listo para ML profesional\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üîÑ Comparaci√≥n: Manual vs Scikit-Learn\n",
    "\n",
    "### 1.1 Implementaci√≥n Manual (Repaso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de implementaci√≥n manual (del notebook anterior)\n",
    "def gradient_descent_manual(X, y, learning_rate=0.01, n_iterations=1000):\n",
    "    \"\"\"Implementaci√≥n manual de gradient descent\"\"\"\n",
    "    m, n = X.shape\n",
    "    w = np.zeros(n)\n",
    "    b = 0\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Predicciones\n",
    "        y_pred = X @ w + b\n",
    "        \n",
    "        # Costo\n",
    "        cost = np.mean((y_pred - y)**2) / 2\n",
    "        costs.append(cost)\n",
    "        \n",
    "        # Gradientes\n",
    "        dw = X.T @ (y_pred - y) / m\n",
    "        db = np.mean(y_pred - y)\n",
    "        \n",
    "        # Actualizar par√°metros\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "    \n",
    "    return w, b, costs\n",
    "\n",
    "def preprocessing_manual(X):\n",
    "    \"\"\"Normalizaci√≥n manual\"\"\"\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    X_normalized = (X - mu) / sigma\n",
    "    return X_normalized, mu, sigma\n",
    "\n",
    "# Crear dataset sint√©tico para comparaci√≥n\n",
    "X_synthetic, y_synthetic = make_regression(\n",
    "    n_samples=100, n_features=5, noise=10, random_state=42\n",
    ")\n",
    "\n",
    "print(\"üî¨ Dataset sint√©tico creado:\")\n",
    "print(f\"   ‚Ä¢ Samples: {X_synthetic.shape[0]}\")\n",
    "print(f\"   ‚Ä¢ Features: {X_synthetic.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Target range: [{y_synthetic.min():.1f}, {y_synthetic.max():.1f}]\")\n",
    "\n",
    "# Dividir datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_synthetic, y_synthetic, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Divisi√≥n de datos:\")\n",
    "print(f\"   ‚Ä¢ Entrenamiento: {X_train.shape[0]} ejemplos\")\n",
    "print(f\"   ‚Ä¢ Prueba: {X_test.shape[0]} ejemplos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Comparaci√≥n Directa de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar implementaci√≥n manual vs sklearn\n",
    "print(\"‚ö° COMPARACI√ìN DE PERFORMANCE\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# 1. IMPLEMENTACI√ìN MANUAL\n",
    "print(\"\\nüîß Implementaci√≥n Manual:\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Preprocessing manual\n",
    "X_train_norm, mu, sigma = preprocessing_manual(X_train)\n",
    "X_test_norm = (X_test - mu) / sigma\n",
    "\n",
    "# Entrenar modelo manual\n",
    "w_manual, b_manual, costs_manual = gradient_descent_manual(\n",
    "    X_train_norm, y_train, learning_rate=0.1, n_iterations=1000\n",
    ")\n",
    "\n",
    "# Predicciones manuales\n",
    "y_pred_manual = X_test_norm @ w_manual + b_manual\n",
    "\n",
    "time_manual = time.time() - start_time\n",
    "mse_manual = np.mean((y_test - y_pred_manual)**2)\n",
    "r2_manual = 1 - np.sum((y_test - y_pred_manual)**2) / np.sum((y_test - np.mean(y_test))**2)\n",
    "\n",
    "print(f\"   Tiempo: {time_manual:.4f} segundos\")\n",
    "print(f\"   MSE: {mse_manual:.2f}\")\n",
    "print(f\"   R¬≤: {r2_manual:.4f}\")\n",
    "print(f\"   Costo final: {costs_manual[-1]:.4f}\")\n",
    "\n",
    "# 2. SCIKIT-LEARN (Linear Regression - Soluci√≥n Anal√≠tica)\n",
    "print(\"\\nüöÄ Scikit-Learn (LinearRegression):\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Crear pipeline con scaling autom√°tico\n",
    "sklearn_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Entrenar\n",
    "sklearn_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_sklearn = sklearn_pipeline.predict(X_test)\n",
    "\n",
    "time_sklearn = time.time() - start_time\n",
    "mse_sklearn = mean_squared_error(y_test, y_pred_sklearn)\n",
    "r2_sklearn = r2_score(y_test, y_pred_sklearn)\n",
    "\n",
    "print(f\"   Tiempo: {time_sklearn:.4f} segundos\")\n",
    "print(f\"   MSE: {mse_sklearn:.2f}\")\n",
    "print(f\"   R¬≤: {r2_sklearn:.4f}\")\n",
    "\n",
    "# 3. SCIKIT-LEARN (SGDRegressor - Gradient Descent)\n",
    "print(\"\\n‚ö° Scikit-Learn (SGDRegressor):\")\n",
    "start_time = time.time()\n",
    "\n",
    "sgd_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', SGDRegressor(max_iter=1000, alpha=0.01, random_state=42))\n",
    "])\n",
    "\n",
    "sgd_pipeline.fit(X_train, y_train)\n",
    "y_pred_sgd = sgd_pipeline.predict(X_test)\n",
    "\n",
    "time_sgd = time.time() - start_time\n",
    "mse_sgd = mean_squared_error(y_test, y_pred_sgd)\n",
    "r2_sgd = r2_score(y_test, y_pred_sgd)\n",
    "\n",
    "print(f\"   Tiempo: {time_sgd:.4f} segundos\")\n",
    "print(f\"   MSE: {mse_sgd:.2f}\")\n",
    "print(f\"   R¬≤: {r2_sgd:.4f}\")\n",
    "\n",
    "# RESUMEN COMPARATIVO\n",
    "print(\"\\nüìä RESUMEN COMPARATIVO:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'M√©todo':<20} {'Tiempo':<10} {'MSE':<8} {'R¬≤':<8} {'Speedup':<8}\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Manual':<20} {time_manual:.4f}s   {mse_manual:<8.2f} {r2_manual:<8.4f} {'1.0x':<8}\")\n",
    "print(f\"{'LinearRegression':<20} {time_sklearn:.4f}s   {mse_sklearn:<8.2f} {r2_sklearn:<8.4f} {time_manual/time_sklearn:<8.1f}x\")\n",
    "print(f\"{'SGDRegressor':<20} {time_sgd:.4f}s   {mse_sgd:<8.2f} {r2_sgd:<8.4f} {time_manual/time_sgd:<8.1f}x\")\n",
    "\n",
    "print(\"\\nüí° Observaciones:\")\n",
    "print(\"   ‚Ä¢ LinearRegression: Soluci√≥n anal√≠tica exacta, muy r√°pida\")\n",
    "print(\"   ‚Ä¢ SGDRegressor: Gradient descent optimizado, escalable\")\n",
    "print(\"   ‚Ä¢ Manual: Educativo pero m√°s lento\")\n",
    "print(\"   ‚Ä¢ Sklearn maneja autom√°ticamente edge cases y optimizaciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üîß Scikit-Learn Fundamentals\n",
    "\n",
    "### 2.1 API Consistente de Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostraci√≥n del API consistente de sklearn\n",
    "print(\"üîß API CONSISTENTE DE SCIKIT-LEARN\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Todos los estimadores siguen el mismo patr√≥n:\n",
    "# 1. fit(X, y) - Entrenar\n",
    "# 2. predict(X) - Predecir\n",
    "# 3. score(X, y) - Evaluar\n",
    "\n",
    "# Crear diferentes modelos con el mismo API\n",
    "modelos = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'SGD Regressor': SGDRegressor(max_iter=1000, random_state=42),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=1.0)\n",
    "}\n",
    "\n",
    "# Preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"{'Modelo':<18} {'R¬≤ Train':<10} {'R¬≤ Test':<10} {'MSE Test':<10} {'Caracter√≠sticas':<20}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "resultados_modelos = {}\n",
    "\n",
    "for nombre, modelo in modelos.items():\n",
    "    # 1. FIT - Entrenar\n",
    "    modelo.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # 2. PREDICT - Predecir\n",
    "    y_pred_train = modelo.predict(X_train_scaled)\n",
    "    y_pred_test = modelo.predict(X_test_scaled)\n",
    "    \n",
    "    # 3. SCORE - Evaluar\n",
    "    r2_train = modelo.score(X_train_scaled, y_train)\n",
    "    r2_test = modelo.score(X_test_scaled, y_test)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    \n",
    "    # Caracter√≠sticas espec√≠ficas del modelo\n",
    "    if hasattr(modelo, 'coef_'):\n",
    "        n_features_used = np.sum(np.abs(modelo.coef_) > 1e-10)\n",
    "        caracteristica = f\"{n_features_used}/{len(modelo.coef_)} features\"\n",
    "    else:\n",
    "        caracteristica = \"N/A\"\n",
    "    \n",
    "    resultados_modelos[nombre] = {\n",
    "        'modelo': modelo,\n",
    "        'r2_train': r2_train,\n",
    "        'r2_test': r2_test,\n",
    "        'mse_test': mse_test,\n",
    "        'y_pred': y_pred_test\n",
    "    }\n",
    "    \n",
    "    print(f\"{nombre:<18} {r2_train:<10.4f} {r2_test:<10.4f} {mse_test:<10.2f} {caracteristica:<20}\")\n",
    "\n",
    "print(\"\\nüéØ Beneficios del API consistente:\")\n",
    "print(\"   ‚Ä¢ Mismos m√©todos para todos los modelos\")\n",
    "print(\"   ‚Ä¢ F√°cil intercambio entre algoritmos\")\n",
    "print(\"   ‚Ä¢ Pipelines y automatizaci√≥n simples\")\n",
    "print(\"   ‚Ä¢ Curva de aprendizaje reducida\")\n",
    "\n",
    "# Demostrar intercambiabilidad\n",
    "print(f\"\\nüîÑ Ejemplo de intercambiabilidad:\")\n",
    "mejor_modelo_nombre = min(resultados_modelos.keys(), \n",
    "                         key=lambda k: resultados_modelos[k]['mse_test'])\n",
    "mejor_modelo = resultados_modelos[mejor_modelo_nombre]['modelo']\n",
    "\n",
    "print(f\"   Mejor modelo: {mejor_modelo_nombre}\")\n",
    "print(f\"   MSE: {resultados_modelos[mejor_modelo_nombre]['mse_test']:.2f}\")\n",
    "print(f\"   R¬≤ test: {resultados_modelos[mejor_modelo_nombre]['r2_test']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pipelines: Automatizaci√≥n del Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostraci√≥n de Pipelines para automatizar el workflow\n",
    "print(\"üîÑ PIPELINES: AUTOMATIZACI√ìN DEL WORKFLOW\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Problema com√∫n: M√∫ltiples pasos de preprocessing\n",
    "print(\"‚ùå Problema sin Pipeline:\")\n",
    "print(\"   1. scaler = StandardScaler()\")\n",
    "print(\"   2. X_train_scaled = scaler.fit_transform(X_train)\")\n",
    "print(\"   3. X_test_scaled = scaler.transform(X_test)\")\n",
    "print(\"   4. model = LinearRegression()\")\n",
    "print(\"   5. model.fit(X_train_scaled, y_train)\")\n",
    "print(\"   6. predictions = model.predict(X_test_scaled)\")\n",
    "print(\"   ‚Üí M√∫ltiples objetos, propenso a errores\\n\")\n",
    "\n",
    "print(\"‚úÖ Soluci√≥n con Pipeline:\")\n",
    "\n",
    "# Pipeline simple\n",
    "pipeline_simple = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Una sola l√≠nea para entrenar todo el pipeline\n",
    "pipeline_simple.fit(X_train, y_train)\n",
    "y_pred_pipeline = pipeline_simple.predict(X_test)\n",
    "\n",
    "print(\"   pipeline.fit(X_train, y_train)\")\n",
    "print(\"   predictions = pipeline.predict(X_test)\")\n",
    "print(\"   ‚Üí Un solo objeto, autom√°tico, sin errores\\n\")\n",
    "\n",
    "# Pipeline complejo con feature engineering\n",
    "pipeline_complejo = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('feature_selection', 'passthrough'),  # Placeholder\n",
    "    ('regressor', Ridge(alpha=1.0))\n",
    "])\n",
    "\n",
    "pipeline_complejo.fit(X_train, y_train)\n",
    "y_pred_complejo = pipeline_complejo.predict(X_test)\n",
    "\n",
    "# Comparar resultados\n",
    "r2_simple = r2_score(y_test, y_pred_pipeline)\n",
    "r2_complejo = r2_score(y_test, y_pred_complejo)\n",
    "mse_simple = mean_squared_error(y_test, y_pred_pipeline)\n",
    "mse_complejo = mean_squared_error(y_test, y_pred_complejo)\n",
    "\n",
    "print(f\"üìä Comparaci√≥n de Pipelines:\")\n",
    "print(f\"{'Pipeline':<15} {'R¬≤':<8} {'MSE':<8} {'Features':<10}\")\n",
    "print(\"-\" * 45)\n",
    "print(f\"{'Simple':<15} {r2_simple:<8.4f} {mse_simple:<8.2f} {X_train.shape[1]:<10}\")\n",
    "print(f\"{'Complejo':<15} {r2_complejo:<8.4f} {mse_complejo:<8.2f} {pipeline_complejo.named_steps['poly_features'].transform(X_train).shape[1]:<10}\")\n",
    "\n",
    "# Beneficios de pipelines\n",
    "print(f\"\\nüéØ Beneficios de Pipelines:\")\n",
    "print(f\"   ‚úÖ Previene data leakage\")\n",
    "print(f\"   ‚úÖ C√≥digo m√°s limpio y mantenible\")\n",
    "print(f\"   ‚úÖ F√°cil de usar en cross-validation\")\n",
    "print(f\"   ‚úÖ Reproducibilidad garantizada\")\n",
    "print(f\"   ‚úÖ Deployment simplificado\")\n",
    "\n",
    "# Inspeccionar pasos del pipeline\n",
    "print(f\"\\nüîç Inspecci√≥n del Pipeline Complejo:\")\n",
    "for step_name, step_estimator in pipeline_complejo.named_steps.items():\n",
    "    print(f\"   {step_name}: {type(step_estimator).__name__}\")\n",
    "    \n",
    "# Acceder a componentes espec√≠ficos\n",
    "poly_features = pipeline_complejo.named_steps['poly_features']\n",
    "scaler = pipeline_complejo.named_steps['scaler']\n",
    "regressor = pipeline_complejo.named_steps['regressor']\n",
    "\n",
    "print(f\"\\nüìà Detalles del modelo final:\")\n",
    "print(f\"   Features polinomiales: Grado {poly_features.degree}\")\n",
    "print(f\"   Normalizador: {scaler.mean_[:3]:.2f}... (primeros 3 valores)\")\n",
    "print(f\"   Regularizaci√≥n Ridge: Alpha = {regressor.alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üéØ Validaci√≥n Robusta con Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation robusta para evaluaci√≥n de modelos\n",
    "print(\"üéØ VALIDACI√ìN ROBUSTA CON CROSS-VALIDATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Cargar dataset real para demostraci√≥n m√°s realista\n",
    "diabetes_data = load_diabetes()\n",
    "X_diabetes, y_diabetes = diabetes_data.data, diabetes_data.target\n",
    "\n",
    "print(f\"üìä Dataset Diabetes:\")\n",
    "print(f\"   ‚Ä¢ Samples: {X_diabetes.shape[0]}\")\n",
    "print(f\"   ‚Ä¢ Features: {X_diabetes.shape[1]} ({list(diabetes_data.feature_names)})\")\n",
    "print(f\"   ‚Ä¢ Target: Progresi√≥n de diabetes (continuo)\")\n",
    "print(f\"   ‚Ä¢ Rango target: [{y_diabetes.min():.1f}, {y_diabetes.max():.1f}]\")\n",
    "\n",
    "# Crear diferentes pipelines para comparar\n",
    "pipelines = {\n",
    "    'Linear': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', LinearRegression())\n",
    "    ]),\n",
    "    \n",
    "    'Polynomial (deg=2)': Pipeline([\n",
    "        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', LinearRegression())\n",
    "    ]),\n",
    "    \n",
    "    'Ridge (Œ±=1.0)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Ridge(alpha=1.0))\n",
    "    ]),\n",
    "    \n",
    "    'Lasso (Œ±=1.0)': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Lasso(alpha=1.0))\n",
    "    ]),\n",
    "    \n",
    "    'SGD': Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', SGDRegressor(max_iter=1000, random_state=42))\n",
    "    ])\n",
    "}\n",
    "\n",
    "# Cross-validation para cada pipeline\n",
    "cv_resultados = {}\n",
    "cv_folds = 5\n",
    "\n",
    "print(f\"\\nüîÑ Cross-Validation ({cv_folds}-fold):\")\n",
    "print(f\"{'Modelo':<20} {'CV Mean':<10} {'CV Std':<8} {'Min':<8} {'Max':<8}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for nombre, pipeline in pipelines.items():\n",
    "    # Realizar cross-validation\n",
    "    cv_scores = cross_val_score(pipeline, X_diabetes, y_diabetes, \n",
    "                               cv=cv_folds, scoring='r2')\n",
    "    \n",
    "    cv_mean = cv_scores.mean()\n",
    "    cv_std = cv_scores.std()\n",
    "    cv_min = cv_scores.min()\n",
    "    cv_max = cv_scores.max()\n",
    "    \n",
    "    cv_resultados[nombre] = {\n",
    "        'scores': cv_scores,\n",
    "        'mean': cv_mean,\n",
    "        'std': cv_std,\n",
    "        'pipeline': pipeline\n",
    "    }\n",
    "    \n",
    "    print(f\"{nombre:<20} {cv_mean:<10.4f} {cv_std:<8.4f} {cv_min:<8.4f} {cv_max:<8.4f}\")\n",
    "\n",
    "# Identificar mejor modelo\n",
    "mejor_modelo_cv = max(cv_resultados.keys(), key=lambda k: cv_resultados[k]['mean'])\n",
    "mejor_pipeline = cv_resultados[mejor_modelo_cv]['pipeline']\n",
    "\n",
    "print(f\"\\nüèÜ Mejor modelo: {mejor_modelo_cv}\")\n",
    "print(f\"   CV Score: {cv_resultados[mejor_modelo_cv]['mean']:.4f} ¬± {cv_resultados[mejor_modelo_cv]['std']:.4f}\")\n",
    "\n",
    "# Visualizar resultados de CV\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Box plot de scores de CV\n",
    "plt.subplot(2, 2, 1)\n",
    "scores_data = [cv_resultados[nombre]['scores'] for nombre in pipelines.keys()]\n",
    "plt.boxplot(scores_data, labels=list(pipelines.keys()))\n",
    "plt.title('Distribuci√≥n de CV Scores')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Comparaci√≥n de medias con barras de error\n",
    "plt.subplot(2, 2, 2)\n",
    "nombres = list(cv_resultados.keys())\n",
    "medias = [cv_resultados[nombre]['mean'] for nombre in nombres]\n",
    "stds = [cv_resultados[nombre]['std'] for nombre in nombres]\n",
    "\n",
    "bars = plt.bar(range(len(nombres)), medias, yerr=stds, capsize=5, alpha=0.7)\n",
    "plt.title('CV Scores con Intervalos de Confianza')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.xticks(range(len(nombres)), nombres, rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Resaltar el mejor modelo\n",
    "mejor_idx = nombres.index(mejor_modelo_cv)\n",
    "bars[mejor_idx].set_color('gold')\n",
    "bars[mejor_idx].set_edgecolor('black')\n",
    "bars[mejor_idx].set_linewidth(2)\n",
    "\n",
    "# Learning curves para el mejor modelo\n",
    "plt.subplot(2, 2, 3)\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(\n",
    "    mejor_pipeline, X_diabetes, y_diabetes, cv=5, \n",
    "    train_sizes=np.linspace(0.1, 1.0, 10), scoring='r2'\n",
    ")\n",
    "\n",
    "train_mean = train_scores.mean(axis=1)\n",
    "train_std = train_scores.std(axis=1)\n",
    "val_mean = val_scores.mean(axis=1)\n",
    "val_std = val_scores.std(axis=1)\n",
    "\n",
    "plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training')\n",
    "plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='blue')\n",
    "plt.plot(train_sizes, val_mean, 'o-', color='red', label='Validation')\n",
    "plt.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2, color='red')\n",
    "\n",
    "plt.title(f'Learning Curves: {mejor_modelo_cv}')\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Validation curves para regularizaci√≥n (si aplica)\n",
    "plt.subplot(2, 2, 4)\n",
    "if 'Ridge' in mejor_modelo_cv or 'Lasso' in mejor_modelo_cv:\n",
    "    # Crear pipeline temporal para validation curve\n",
    "    temp_pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', Ridge() if 'Ridge' in mejor_modelo_cv else Lasso())\n",
    "    ])\n",
    "    \n",
    "    param_range = np.logspace(-3, 2, 10)\n",
    "    train_scores_val, test_scores_val = validation_curve(\n",
    "        temp_pipeline, X_diabetes, y_diabetes, \n",
    "        param_name='regressor__alpha', param_range=param_range,\n",
    "        cv=5, scoring='r2'\n",
    "    )\n",
    "    \n",
    "    train_mean_val = train_scores_val.mean(axis=1)\n",
    "    test_mean_val = test_scores_val.mean(axis=1)\n",
    "    \n",
    "    plt.semilogx(param_range, train_mean_val, 'o-', color='blue', label='Training')\n",
    "    plt.semilogx(param_range, test_mean_val, 'o-', color='red', label='Validation')\n",
    "    plt.title('Validation Curve (Alpha)')\n",
    "    plt.xlabel('Alpha (Regularization)')\n",
    "    plt.ylabel('R¬≤ Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'Validation Curve\\nno aplicable\\npara este modelo', \n",
    "             ha='center', va='center', transform=plt.gca().transAxes, \n",
    "             fontsize=12, bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.5))\n",
    "    plt.title('Validation Curve')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Insights de Cross-Validation:\")\n",
    "print(f\"   ‚Ä¢ CV proporciona estimaci√≥n robusta del rendimiento\")\n",
    "print(f\"   ‚Ä¢ Desviaci√≥n est√°ndar indica estabilidad del modelo\")\n",
    "print(f\"   ‚Ä¢ Learning curves muestran overfitting/underfitting\")\n",
    "print(f\"   ‚Ä¢ Validation curves ayudan a optimizar hiperpar√°metros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üîç Grid Search: Optimizaci√≥n Autom√°tica de Hiperpar√°metros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search para optimizaci√≥n autom√°tica de hiperpar√°metros\n",
    "print(\"üîç GRID SEARCH: OPTIMIZACI√ìN DE HIPERPAR√ÅMETROS\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Definir pipeline con m√∫ltiples pasos a optimizar\n",
    "pipeline_optimizable = Pipeline([\n",
    "    ('poly', PolynomialFeatures()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', Ridge())\n",
    "])\n",
    "\n",
    "# Definir grid de par√°metros a buscar\n",
    "param_grid = {\n",
    "    'poly__degree': [1, 2, 3],  # Grado de caracter√≠sticas polinomiales\n",
    "    'poly__include_bias': [True, False],  # Incluir t√©rmino bias\n",
    "    'regressor__alpha': [0.1, 1.0, 10.0, 100.0],  # Regularizaci√≥n Ridge\n",
    "}\n",
    "\n",
    "print(f\"üìã Configuraci√≥n de Grid Search:\")\n",
    "print(f\"   ‚Ä¢ Pipeline: PolynomialFeatures ‚Üí StandardScaler ‚Üí Ridge\")\n",
    "print(f\"   ‚Ä¢ Par√°metros a optimizar:\")\n",
    "for param, valores in param_grid.items():\n",
    "    print(f\"     - {param}: {valores}\")\n",
    "\n",
    "total_combinaciones = np.prod([len(valores) for valores in param_grid.values()])\n",
    "print(f\"   ‚Ä¢ Total combinaciones: {total_combinaciones}\")\n",
    "print(f\"   ‚Ä¢ CV folds: 5\")\n",
    "print(f\"   ‚Ä¢ Total entrenamientos: {total_combinaciones * 5}\")\n",
    "\n",
    "# Ejecutar Grid Search\n",
    "print(f\"\\n‚öôÔ∏è Ejecutando Grid Search...\")\n",
    "start_time = time.time()\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    pipeline_optimizable, \n",
    "    param_grid, \n",
    "    cv=5, \n",
    "    scoring='r2', \n",
    "    n_jobs=-1,  # Usar todos los cores disponibles\n",
    "    verbose=1   # Mostrar progreso\n",
    ")\n",
    "\n",
    "grid_search.fit(X_diabetes, y_diabetes)\n",
    "grid_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n‚úÖ Grid Search completado en {grid_time:.2f} segundos\")\n",
    "\n",
    "# Resultados del mejor modelo\n",
    "print(f\"\\nüèÜ MEJORES RESULTADOS:\")\n",
    "print(f\"   ‚Ä¢ Mejor score: {grid_search.best_score_:.4f}\")\n",
    "print(f\"   ‚Ä¢ Mejores par√°metros:\")\n",
    "for param, valor in grid_search.best_params_.items():\n",
    "    print(f\"     - {param}: {valor}\")\n",
    "\n",
    "# Entrenar modelo final y evaluar\n",
    "mejor_modelo_grid = grid_search.best_estimator_\n",
    "\n",
    "# Divisi√≥n train/test para evaluaci√≥n final\n",
    "X_train_diab, X_test_diab, y_train_diab, y_test_diab = train_test_split(\n",
    "    X_diabetes, y_diabetes, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "mejor_modelo_grid.fit(X_train_diab, y_train_diab)\n",
    "y_pred_final = mejor_modelo_grid.predict(X_test_diab)\n",
    "\n",
    "# M√©tricas finales\n",
    "r2_final = r2_score(y_test_diab, y_pred_final)\n",
    "mse_final = mean_squared_error(y_test_diab, y_pred_final)\n",
    "mae_final = mean_absolute_error(y_test_diab, y_pred_final)\n",
    "\n",
    "print(f\"\\nüìä Evaluaci√≥n en test set:\")\n",
    "print(f\"   ‚Ä¢ R¬≤: {r2_final:.4f}\")\n",
    "print(f\"   ‚Ä¢ MSE: {mse_final:.2f}\")\n",
    "print(f\"   ‚Ä¢ MAE: {mae_final:.2f}\")\n",
    "\n",
    "# Analizar top 10 combinaciones\n",
    "resultados_df = pd.DataFrame(grid_search.cv_results_)\n",
    "top_10 = resultados_df.nlargest(10, 'mean_test_score')[[\n",
    "    'params', 'mean_test_score', 'std_test_score', 'rank_test_score'\n",
    "]]\n",
    "\n",
    "print(f\"\\nüìà Top 10 combinaciones:\")\n",
    "print(f\"{'Rank':<5} {'Score':<8} {'Std':<8} {'Par√°metros':<50}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for idx, row in top_10.iterrows():\n",
    "    params_str = str(row['params'])[:50] + \"...\" if len(str(row['params'])) > 50 else str(row['params'])\n",
    "    print(f\"{int(row['rank_test_score']):<5} {row['mean_test_score']:<8.4f} {row['std_test_score']:<8.4f} {params_str:<50}\")\n",
    "\n",
    "# Visualizar resultados del grid search\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Heatmap de results para degree vs alpha\n",
    "plt.subplot(2, 3, 1)\n",
    "pivot_data = resultados_df.pivot_table(\n",
    "    values='mean_test_score', \n",
    "    index='param_poly__degree', \n",
    "    columns='param_regressor__alpha',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "plt.imshow(pivot_data.values, cmap='viridis', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title('Heatmap: Degree vs Alpha')\n",
    "plt.xlabel('Alpha (log scale)')\n",
    "plt.ylabel('Polynomial Degree')\n",
    "plt.xticks(range(len(pivot_data.columns)), [f'{alpha:.1f}' for alpha in pivot_data.columns])\n",
    "plt.yticks(range(len(pivot_data.index)), pivot_data.index)\n",
    "\n",
    "# Distribuci√≥n de scores\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.hist(resultados_df['mean_test_score'], bins=15, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(grid_search.best_score_, color='red', linestyle='--', \n",
    "           label=f'Best: {grid_search.best_score_:.4f}')\n",
    "plt.title('Distribuci√≥n de Scores')\n",
    "plt.xlabel('Mean CV Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Score vs polynomial degree\n",
    "plt.subplot(2, 3, 3)\n",
    "degree_scores = resultados_df.groupby('param_poly__degree')['mean_test_score'].agg(['mean', 'std'])\n",
    "plt.errorbar(degree_scores.index, degree_scores['mean'], \n",
    "            yerr=degree_scores['std'], fmt='o-', capsize=5)\n",
    "plt.title('Score vs Polynomial Degree')\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Mean CV Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Score vs alpha\n",
    "plt.subplot(2, 3, 4)\n",
    "alpha_scores = resultados_df.groupby('param_regressor__alpha')['mean_test_score'].agg(['mean', 'std'])\n",
    "plt.errorbar(alpha_scores.index, alpha_scores['mean'], \n",
    "            yerr=alpha_scores['std'], fmt='o-', capsize=5)\n",
    "plt.xscale('log')\n",
    "plt.title('Score vs Regularization (Alpha)')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Mean CV Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Predicciones vs realidad (test set)\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.scatter(y_test_diab, y_pred_final, alpha=0.7)\n",
    "plt.plot([y_test_diab.min(), y_test_diab.max()], \n",
    "         [y_test_diab.min(), y_test_diab.max()], 'r--', linewidth=2)\n",
    "plt.title(f'Predictions vs Reality\\nR¬≤ = {r2_final:.4f}')\n",
    "plt.xlabel('True Values')\n",
    "plt.ylabel('Predictions')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuos\n",
    "plt.subplot(2, 3, 6)\n",
    "residuos = y_test_diab - y_pred_final\n",
    "plt.scatter(y_pred_final, residuos, alpha=0.7)\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Residuals Plot')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüí° Conclusiones de Grid Search:\")\n",
    "print(f\"   ‚Ä¢ Automatiza b√∫squeda exhaustiva de hiperpar√°metros\")\n",
    "print(f\"   ‚Ä¢ Cross-validation previene overfitting en selecci√≥n\")\n",
    "print(f\"   ‚Ä¢ Paralelizaci√≥n acelera el proceso significativamente\")\n",
    "print(f\"   ‚Ä¢ Visualizaci√≥n ayuda a entender impacto de par√°metros\")\n",
    "print(f\"   ‚Ä¢ Modelo final tiene rendimiento optimizado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üìä Comparaci√≥n Final: Manual vs Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n final exhaustiva entre implementaci√≥n manual y sklearn\n",
    "print(\"üìä COMPARACI√ìN FINAL: MANUAL VS SKLEARN\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Crear dataset de benchmark\n",
    "X_benchmark, y_benchmark = make_regression(\n",
    "    n_samples=1000, n_features=10, noise=20, random_state=42\n",
    ")\n",
    "\n",
    "X_train_bench, X_test_bench, y_train_bench, y_test_bench = train_test_split(\n",
    "    X_benchmark, y_benchmark, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìà Dataset de benchmark:\")\n",
    "print(f\"   ‚Ä¢ Training: {X_train_bench.shape[0]} samples, {X_train_bench.shape[1]} features\")\n",
    "print(f\"   ‚Ä¢ Test: {X_test_bench.shape[0]} samples\")\n",
    "\n",
    "# Funciones mejoradas para implementaci√≥n manual\n",
    "def implementacion_manual_completa(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Implementaci√≥n manual completa con mejores pr√°cticas\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Preprocessing\n",
    "    mu = np.mean(X_train, axis=0)\n",
    "    sigma = np.std(X_train, axis=0)\n",
    "    sigma[sigma == 0] = 1  # Evitar divisi√≥n por cero\n",
    "    \n",
    "    X_train_scaled = (X_train - mu) / sigma\n",
    "    X_test_scaled = (X_test - mu) / sigma\n",
    "    \n",
    "    # 2. Gradient descent mejorado\n",
    "    m, n = X_train_scaled.shape\n",
    "    w = np.random.normal(0, 0.01, n)  # Inicializaci√≥n aleatoria peque√±a\n",
    "    b = 0\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    n_iterations = 1000\n",
    "    tolerance = 1e-6\n",
    "    \n",
    "    costs = []\n",
    "    prev_cost = float('inf')\n",
    "    \n",
    "    for i in range(n_iterations):\n",
    "        # Forward pass\n",
    "        y_pred = X_train_scaled @ w + b\n",
    "        \n",
    "        # Costo\n",
    "        cost = np.mean((y_pred - y_train)**2) / 2\n",
    "        costs.append(cost)\n",
    "        \n",
    "        # Early stopping\n",
    "        if abs(prev_cost - cost) < tolerance:\n",
    "            break\n",
    "        prev_cost = cost\n",
    "        \n",
    "        # Gradientes\n",
    "        dw = X_train_scaled.T @ (y_pred - y_train) / m\n",
    "        db = np.mean(y_pred - y_train)\n",
    "        \n",
    "        # Update\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "    \n",
    "    # 3. Predicciones\n",
    "    y_pred_train = X_train_scaled @ w + b\n",
    "    y_pred_test = X_test_scaled @ w + b\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'y_pred_train': y_pred_train,\n",
    "        'y_pred_test': y_pred_test,\n",
    "        'training_time': training_time,\n",
    "        'n_iterations': i + 1,\n",
    "        'final_cost': costs[-1],\n",
    "        'costs': costs,\n",
    "        'weights': w,\n",
    "        'bias': b\n",
    "    }\n",
    "\n",
    "def implementacion_sklearn_completa(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Implementaci√≥n sklearn completa con mejores pr√°cticas\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Pipeline optimizado\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('regressor', LinearRegression())\n",
    "    ])\n",
    "    \n",
    "    # Entrenar\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'y_pred_train': y_pred_train,\n",
    "        'y_pred_test': y_pred_test,\n",
    "        'training_time': training_time,\n",
    "        'pipeline': pipeline,\n",
    "        'weights': pipeline.named_steps['regressor'].coef_,\n",
    "        'bias': pipeline.named_steps['regressor'].intercept_\n",
    "    }\n",
    "\n",
    "# Ejecutar ambas implementaciones\n",
    "print(f\"\\n‚öôÔ∏è Ejecutando comparaci√≥n...\")\n",
    "\n",
    "resultado_manual = implementacion_manual_completa(\n",
    "    X_train_bench, X_test_bench, y_train_bench, y_test_bench\n",
    ")\n",
    "\n",
    "resultado_sklearn = implementacion_sklearn_completa(\n",
    "    X_train_bench, X_test_bench, y_train_bench, y_test_bench\n",
    ")\n",
    "\n",
    "# Calcular m√©tricas\n",
    "def calcular_metricas(y_true, y_pred, nombre):\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return {'r2': r2, 'mse': mse, 'mae': mae, 'nombre': nombre}\n",
    "\n",
    "metricas_manual_train = calcular_metricas(y_train_bench, resultado_manual['y_pred_train'], 'Manual Train')\n",
    "metricas_manual_test = calcular_metricas(y_test_bench, resultado_manual['y_pred_test'], 'Manual Test')\n",
    "metricas_sklearn_train = calcular_metricas(y_train_bench, resultado_sklearn['y_pred_train'], 'Sklearn Train')\n",
    "metricas_sklearn_test = calcular_metricas(y_test_bench, resultado_sklearn['y_pred_test'], 'Sklearn Test')\n",
    "\n",
    "# Mostrar comparaci√≥n detallada\n",
    "print(f\"\\nüìä RESULTADOS DETALLADOS:\")\n",
    "print(f\"\\n{'M√©trica':<15} {'Manual Train':<12} {'Manual Test':<12} {'Sklearn Train':<13} {'Sklearn Test':<13}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'R¬≤':<15} {metricas_manual_train['r2']:<12.4f} {metricas_manual_test['r2']:<12.4f} {metricas_sklearn_train['r2']:<13.4f} {metricas_sklearn_test['r2']:<13.4f}\")\n",
    "print(f\"{'MSE':<15} {metricas_manual_train['mse']:<12.2f} {metricas_manual_test['mse']:<12.2f} {metricas_sklearn_train['mse']:<13.2f} {metricas_sklearn_test['mse']:<13.2f}\")\n",
    "print(f\"{'MAE':<15} {metricas_manual_train['mae']:<12.2f} {metricas_manual_test['mae']:<12.2f} {metricas_sklearn_train['mae']:<13.2f} {metricas_sklearn_test['mae']:<13.2f}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  COMPARACI√ìN DE TIEMPOS:\")\n",
    "print(f\"   Manual: {resultado_manual['training_time']:.4f} segundos ({resultado_manual['n_iterations']} iteraciones)\")\n",
    "print(f\"   Sklearn: {resultado_sklearn['training_time']:.4f} segundos (soluci√≥n anal√≠tica)\")\n",
    "print(f\"   Speedup: {resultado_manual['training_time'] / resultado_sklearn['training_time']:.1f}x m√°s r√°pido sklearn\")\n",
    "\n",
    "# Comparar par√°metros aprendidos\n",
    "diferencia_weights = np.mean(np.abs(resultado_manual['weights'] - resultado_sklearn['weights']))\n",
    "diferencia_bias = abs(resultado_manual['bias'] - resultado_sklearn['bias'])\n",
    "\n",
    "print(f\"\\nüîç CONVERGENCIA DE PAR√ÅMETROS:\")\n",
    "print(f\"   Diferencia promedio en pesos: {diferencia_weights:.6f}\")\n",
    "print(f\"   Diferencia en bias: {diferencia_bias:.6f}\")\n",
    "print(f\"   ‚úÖ Convergencia exitosa\" if diferencia_weights < 0.01 else \"‚ùå Diferencias significativas\")\n",
    "\n",
    "# Visualizaci√≥n final comparativa\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "# Convergencia del costo (manual)\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(resultado_manual['costs'])\n",
    "plt.title(f'Convergencia Manual\\n({resultado_manual[\"n_iterations\"]} iteraciones)')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Cost')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Comparaci√≥n de predicciones en test\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.scatter(y_test_bench, resultado_manual['y_pred_test'], alpha=0.6, label='Manual', s=20)\n",
    "plt.scatter(y_test_bench, resultado_sklearn['y_pred_test'], alpha=0.6, label='Sklearn', s=20)\n",
    "plt.plot([y_test_bench.min(), y_test_bench.max()], \n",
    "         [y_test_bench.min(), y_test_bench.max()], 'r--', linewidth=2)\n",
    "plt.title('Predicciones vs Realidad')\n",
    "plt.xlabel('Valores Reales')\n",
    "plt.ylabel('Predicciones')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuos comparativos\n",
    "plt.subplot(2, 3, 3)\n",
    "residuos_manual = y_test_bench - resultado_manual['y_pred_test']\n",
    "residuos_sklearn = y_test_bench - resultado_sklearn['y_pred_test']\n",
    "plt.hist(residuos_manual, bins=15, alpha=0.7, label='Manual', density=True)\n",
    "plt.hist(residuos_sklearn, bins=15, alpha=0.7, label='Sklearn', density=True)\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Distribuci√≥n de Residuos')\n",
    "plt.xlabel('Residuos')\n",
    "plt.ylabel('Densidad')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Comparaci√≥n de pesos\n",
    "plt.subplot(2, 3, 4)\n",
    "indices = range(len(resultado_manual['weights']))\n",
    "plt.bar(np.array(indices) - 0.2, resultado_manual['weights'], width=0.4, alpha=0.7, label='Manual')\n",
    "plt.bar(np.array(indices) + 0.2, resultado_sklearn['weights'], width=0.4, alpha=0.7, label='Sklearn')\n",
    "plt.title('Comparaci√≥n de Pesos')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Peso')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# M√©tricas comparativas\n",
    "plt.subplot(2, 3, 5)\n",
    "metricas = ['R¬≤ Train', 'R¬≤ Test', 'MSE Train', 'MSE Test']\n",
    "valores_manual = [metricas_manual_train['r2'], metricas_manual_test['r2'], \n",
    "                 metricas_manual_train['mse']/1000, metricas_manual_test['mse']/1000]  # MSE/1000 para escala\n",
    "valores_sklearn = [metricas_sklearn_train['r2'], metricas_sklearn_test['r2'],\n",
    "                  metricas_sklearn_train['mse']/1000, metricas_sklearn_test['mse']/1000]\n",
    "\n",
    "x_pos = np.arange(len(metricas))\n",
    "plt.bar(x_pos - 0.2, valores_manual, width=0.4, alpha=0.7, label='Manual')\n",
    "plt.bar(x_pos + 0.2, valores_sklearn, width=0.4, alpha=0.7, label='Sklearn')\n",
    "plt.title('M√©tricas Comparativas')\n",
    "plt.ylabel('Valor (MSE√∑1000)')\n",
    "plt.xticks(x_pos, metricas, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Tiempo de entrenamiento\n",
    "plt.subplot(2, 3, 6)\n",
    "tiempos = [resultado_manual['training_time'], resultado_sklearn['training_time']]\n",
    "nombres = ['Manual\\n(Gradient Descent)', 'Sklearn\\n(Analytical)']\n",
    "colores = ['lightcoral', 'lightgreen']\n",
    "bars = plt.bar(nombres, tiempos, color=colores, alpha=0.7, edgecolor='black')\n",
    "plt.title('Tiempo de Entrenamiento')\n",
    "plt.ylabel('Segundos')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# A√±adir valores en las barras\n",
    "for bar, tiempo in zip(bars, tiempos):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.001,\n",
    "             f'{tiempo:.4f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Conclusiones finales\n",
    "print(f\"\\nüéØ CONCLUSIONES FINALES:\")\n",
    "print(f\"\"\"\n",
    "üìö VALOR EDUCATIVO DE IMPLEMENTACI√ìN MANUAL:\n",
    "   ‚úÖ Comprensi√≥n profunda de algoritmos\n",
    "   ‚úÖ Entendimiento de gradient descent\n",
    "   ‚úÖ Insight sobre convergencia y optimizaci√≥n\n",
    "   ‚úÖ Base s√≥lida para algoritmos avanzados\n",
    "\n",
    "üöÄ VENTAJAS DE SCIKIT-LEARN EN PRODUCCI√ìN:\n",
    "   ‚úÖ Velocidad: Hasta {resultado_manual['training_time'] / resultado_sklearn['training_time']:.0f}x m√°s r√°pido\n",
    "   ‚úÖ Robustez: Manejo autom√°tico de edge cases\n",
    "   ‚úÖ Precisi√≥n num√©rica: Algoritmos optimizados\n",
    "   ‚úÖ Ecosistema: Pipelines, CV, m√©tricas integradas\n",
    "   ‚úÖ Mantenibilidad: C√≥digo m√°s limpio y est√°ndar\n",
    "   ‚úÖ Escalabilidad: Optimizado para datasets grandes\n",
    "\n",
    "üéì RECOMENDACI√ìN:\n",
    "   ‚Ä¢ Aprende los fundamentos implementando manualmente\n",
    "   ‚Ä¢ Usa scikit-learn para proyectos reales\n",
    "   ‚Ä¢ Combina ambos enfoques para m√°ximo aprendizaje\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\nüèÅ Has completado exitosamente el journey de Machine Learning:\")\n",
    "print(f\"   Manual ‚Üí Optimizado ‚Üí Producci√≥n\")\n",
    "print(f\"   ¬°Ahora tienes las herramientas para proyectos ML profesionales!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Resumen Final del Journey Completo\n",
    "\n",
    "### ‚úÖ Journey Completado: De Manual a Profesional\n",
    "\n",
    "#### üó∫Ô∏è **El Camino Recorrido**:\n",
    "1. **Fundamentos Python/NumPy** ‚Üí Herramientas b√°sicas\n",
    "2. **Regresi√≥n Lineal Manual** ‚Üí Entendimiento profundo\n",
    "3. **M√∫ltiples Variables** ‚Üí Escalabilidad de conceptos\n",
    "4. **Feature Scaling** ‚Üí Optimizaci√≥n de convergencia\n",
    "5. **Feature Engineering** ‚Üí Captura de patrones complejos\n",
    "6. **Scikit-Learn** ‚Üí Herramientas profesionales\n",
    "\n",
    "#### üéØ **Competencias Desarrolladas**:\n",
    "\n",
    "**Nivel Fundamental** ‚úÖ:\n",
    "- Gradient descent desde cero\n",
    "- Funciones de costo y optimizaci√≥n\n",
    "- Feature scaling y normalizaci√≥n\n",
    "- Evaluaci√≥n de modelos (R¬≤, MSE, MAE)\n",
    "\n",
    "**Nivel Intermedio** ‚úÖ:\n",
    "- Regresi√≥n multivariable\n",
    "- Feature engineering y polinomial\n",
    "- Overfitting vs underfitting\n",
    "- Train/validation/test splits\n",
    "\n",
    "**Nivel Profesional** ‚úÖ:\n",
    "- Scikit-learn pipelines\n",
    "- Cross-validation robusta\n",
    "- Grid search autom√°tico\n",
    "- Workflow completo de ML\n",
    "\n",
    "### üöÄ **Pr√≥ximos Pasos Sugeridos**:\n",
    "\n",
    "#### **Profundizar en ML**:\n",
    "- **Clasificaci√≥n**: Regresi√≥n log√≠stica, SVM\n",
    "- **Regularizaci√≥n**: Ridge, Lasso, Elastic Net\n",
    "- **Ensemble Methods**: Random Forest, Gradient Boosting\n",
    "- **Deep Learning**: Redes neuronales con TensorFlow/PyTorch\n",
    "\n",
    "#### **Proyectos Reales**:\n",
    "- Competencias en Kaggle\n",
    "- An√°lisis de datasets reales\n",
    "- Deployment de modelos (Flask, FastAPI)\n",
    "- MLOps y automatizaci√≥n\n",
    "\n",
    "#### **Especializaci√≥n**:\n",
    "- **Computer Vision**: CNNs, detecci√≥n de objetos\n",
    "- **NLP**: Transformers, BERT, GPT\n",
    "- **Time Series**: ARIMA, LSTM, Prophet\n",
    "- **Reinforcement Learning**: Q-learning, Policy Gradients\n",
    "\n",
    "### üíº **Para Aplicaciones Profesionales**:\n",
    "\n",
    "#### **Usa Siempre**:\n",
    "- ‚úÖ **Scikit-learn** para modelos cl√°sicos de ML\n",
    "- ‚úÖ **Pandas** para manipulaci√≥n de datos\n",
    "- ‚úÖ **Cross-validation** para validaci√≥n robusta\n",
    "- ‚úÖ **Pipelines** para workflows reproducibles\n",
    "- ‚úÖ **Grid/Random Search** para optimizaci√≥n\n",
    "\n",
    "#### **Best Practices Profesionales**:\n",
    "1. **Exploraci√≥n de datos** antes de modelar\n",
    "2. **Divisi√≥n apropiada** de train/validation/test\n",
    "3. **Feature engineering** basado en dominio\n",
    "4. **Validaci√≥n cruzada** para selecci√≥n de modelo\n",
    "5. **Monitoreo de rendimiento** en producci√≥n\n",
    "\n",
    "### üéì **Lo que te Diferencia Ahora**:\n",
    "- **Entiendes los fundamentos** (no solo usas librer√≠as)\n",
    "- **Puedes debuggear** algoritmos cuando fallan\n",
    "- **Implementas soluciones** desde cero cuando es necesario\n",
    "- **Optimizas** hyperpar√°metros sistem√°ticamente\n",
    "- **Eval√∫as modelos** con rigor estad√≠stico\n",
    "\n",
    "### üèÜ **¬°Felicitaciones!**\n",
    "\n",
    "Has completado un journey comprehensivo de Machine Learning que te lleva desde los **fundamentos matem√°ticos** hasta las **herramientas profesionales**. \n",
    "\n",
    "**Ahora tienes:**\n",
    "- üß† **Comprensi√≥n profunda** de c√≥mo funcionan los algoritmos\n",
    "- üõ†Ô∏è **Herramientas profesionales** para proyectos reales\n",
    "- üìä **Metodolog√≠a rigurosa** para evaluaci√≥n de modelos\n",
    "- üöÄ **Base s√≥lida** para aprender algoritmos avanzados\n",
    "\n",
    "**¬°Est√°s listo para hacer Machine Learning de nivel profesional!** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}