{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Feature Engineering y Regresi√≥n Polinomial\n",
    "\n",
    "## üìö Objetivos de Aprendizaje\n",
    "En este notebook aprender√°s:\n",
    "- **Qu√© es** feature engineering y por qu√© es crucial\n",
    "- **C√≥mo crear** features polinomiales para capturar relaciones no lineales\n",
    "- **Cu√°ndo usar** regresi√≥n polinomial vs lineal\n",
    "- **C√≥mo evitar** overfitting y underfitting\n",
    "- **T√©cnicas avanzadas** de creaci√≥n de features\n",
    "\n",
    "## üéØ Problema Motivador\n",
    "**Limitaci√≥n de regresi√≥n lineal**: Solo puede modelar relaciones lineales\n",
    "\n",
    "**¬øQu√© pasa cuando los datos tienen patrones no lineales?**\n",
    "- Relaciones cuadr√°ticas: y = x¬≤\n",
    "- Relaciones c√∫bicas: y = x¬≥ \n",
    "- Interacciones entre features: y = x‚ÇÅ √ó x‚ÇÇ\n",
    "- Patrones sinusoidales: y = sin(x)\n",
    "\n",
    "**Soluci√≥n**: Feature Engineering + Regresi√≥n Polinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Configuraci√≥n\n",
    "plt.style.use('default')\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Librer√≠as importadas correctamente\")\n",
    "print(\"Listo para feature engineering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üìä Motivaci√≥n: Limitaciones de la Regresi√≥n Lineal\n",
    "\n",
    "### 1.1 Datos con Patrones No Lineales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datos con diferentes patrones no lineales\n",
    "def generar_datos_no_lineales():\n",
    "    \"\"\"Genera datasets con diferentes patrones no lineales\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Dataset 1: Relaci√≥n cuadr√°tica\n",
    "    x1 = np.linspace(0, 4, 20)\n",
    "    y1 = 1 + x1**2 + np.random.normal(0, 2, len(x1))\n",
    "    \n",
    "    # Dataset 2: Relaci√≥n c√∫bica\n",
    "    x2 = np.linspace(-2, 2, 25)\n",
    "    y2 = x2**3 - x2**2 + 2*x2 + 5 + np.random.normal(0, 1, len(x2))\n",
    "    \n",
    "    # Dataset 3: Relaci√≥n sinusoidal\n",
    "    x3 = np.linspace(0, 4*np.pi, 30)\n",
    "    y3 = 2 + np.sin(x3) + 0.5*x3 + np.random.normal(0, 0.3, len(x3))\n",
    "    \n",
    "    # Dataset 4: Relaci√≥n exponencial\n",
    "    x4 = np.linspace(0, 3, 18)\n",
    "    y4 = 2 * np.exp(0.5 * x4) + np.random.normal(0, 1, len(x4))\n",
    "    \n",
    "    return (x1, y1), (x2, y2), (x3, y3), (x4, y4)\n",
    "\n",
    "# Funciones auxiliares de regresi√≥n lineal (del notebook anterior)\n",
    "def zscore_normalize_features(X):\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    mu = np.mean(X, axis=0)\n",
    "    sigma = np.std(X, axis=0)\n",
    "    X_norm = (X - mu) / sigma\n",
    "    return X_norm, mu, sigma\n",
    "\n",
    "def calcular_costo(X, y, w, b):\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    m = X.shape[0]\n",
    "    predictions = X @ w + b\n",
    "    cost = np.sum((predictions - y)**2) / (2 * m)\n",
    "    return cost\n",
    "\n",
    "def gradient_descent_simple(X, y, w_init, b_init, alpha, iterations):\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1, 1)\n",
    "    m, n = X.shape\n",
    "    w = copy.deepcopy(w_init)\n",
    "    b = b_init\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        predictions = X @ w + b\n",
    "        errors = predictions - y\n",
    "        \n",
    "        dw = (X.T @ errors) / m\n",
    "        db = np.sum(errors) / m\n",
    "        \n",
    "        w = w - alpha * dw\n",
    "        b = b - alpha * db\n",
    "    \n",
    "    return w, b\n",
    "\n",
    "# Generar datasets\n",
    "datasets = generar_datos_no_lineales()\n",
    "nombres = ['Cuadr√°tica (y = x¬≤)', 'C√∫bica (y = x¬≥)', 'Sinusoidal (y = sin(x))', 'Exponencial (y = eÀ£)']\n",
    "\n",
    "# Visualizar datasets y ajustes lineales\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for i, ((x, y), nombre) in enumerate(zip(datasets, nombres)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    \n",
    "    # Datos originales\n",
    "    plt.scatter(x, y, alpha=0.7, s=50, color='red', label='Datos reales')\n",
    "    \n",
    "    # Ajuste lineal\n",
    "    x_norm, mu, sigma = zscore_normalize_features(x)\n",
    "    w_linear, b_linear = gradient_descent_simple(x_norm, y, np.array([0.]), 0., 0.1, 1000)\n",
    "    \n",
    "    # Predicciones lineales\n",
    "    x_plot = np.linspace(np.min(x), np.max(x), 100)\n",
    "    x_plot_norm = (x_plot - mu) / sigma\n",
    "    y_linear = x_plot_norm * w_linear[0] + b_linear\n",
    "    \n",
    "    plt.plot(x_plot, y_linear, 'b-', linewidth=2, label='Ajuste Lineal')\n",
    "    \n",
    "    # Calcular R¬≤ para el ajuste lineal\n",
    "    y_pred_linear = x_norm.flatten() * w_linear[0] + b_linear\n",
    "    ss_res = np.sum((y - y_pred_linear)**2)\n",
    "    ss_tot = np.sum((y - np.mean(y))**2)\n",
    "    r2_linear = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    plt.title(f'{nombre}\\nR¬≤ Lineal = {r2_linear:.3f}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observaciones:\")\n",
    "print(\"  ‚Ä¢ La regresi√≥n lineal falla en capturar patrones no lineales\")\n",
    "print(\"  ‚Ä¢ R¬≤ bajo indica mal ajuste para datos curvos\")\n",
    "print(\"  ‚Ä¢ Necesitamos una forma de modelar relaciones no lineales\")\n",
    "print(\"\\nSoluci√≥n: Feature Engineering + Regresi√≥n Polinomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üé® Conceptos de Feature Engineering\n",
    "\n",
    "### 2.1 ¬øQu√© es Feature Engineering?\n",
    "\n",
    "**Definici√≥n**: Proceso de crear nuevas features a partir de las existentes para mejorar el rendimiento del modelo.\n",
    "\n",
    "**Idea clave**: Aunque usamos regresi√≥n *lineal*, podemos modelar relaciones *no lineales* creando features no lineales.\n",
    "\n",
    "**Ejemplo**: \n",
    "- Feature original: x\n",
    "- Features engineered: x¬≤, x¬≥, ‚àöx, log(x), sin(x)\n",
    "- Modelo: y = w‚ÇÅx + w‚ÇÇx¬≤ + w‚ÇÉx¬≥ + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostraci√≥n conceptual de feature engineering\n",
    "def crear_features_polinomiales(x, grado):\n",
    "    \"\"\"\n",
    "    Crea features polinomiales hasta el grado especificado\n",
    "    \n",
    "    Args:\n",
    "        x (array): Feature original\n",
    "        grado (int): Grado m√°ximo del polinomio\n",
    "        \n",
    "    Returns:\n",
    "        X_poly (array): Matriz con features polinomiales\n",
    "    \"\"\"\n",
    "    if x.ndim == 1:\n",
    "        x = x.reshape(-1, 1)\n",
    "    \n",
    "    m = x.shape[0]\n",
    "    X_poly = np.ones((m, grado + 1))  # Incluir t√©rmino constante\n",
    "    \n",
    "    for i in range(1, grado + 1):\n",
    "        X_poly[:, i] = (x.flatten()) ** i\n",
    "    \n",
    "    return X_poly[:, 1:]  # Excluir t√©rmino constante (lo maneja b)\n",
    "\n",
    "# Ejemplo con el primer dataset (cuadr√°tico)\n",
    "x_cuad, y_cuad = datasets[0]\n",
    "\n",
    "print(\"Ejemplo: Transformaci√≥n de Features\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Features originales\n",
    "print(f\"Feature original (x):\")\n",
    "print(f\"Primeros 5 valores: {x_cuad[:5]}\")\n",
    "print(f\"Shape: {x_cuad.shape}\")\n",
    "\n",
    "# Features polinomiales de grado 2\n",
    "X_poly_2 = crear_features_polinomiales(x_cuad, 2)\n",
    "print(f\"\\nFeatures polinomiales (grado 2):\")\n",
    "print(f\"Columnas: [x, x¬≤]\")\n",
    "print(f\"Primeras 5 filas:\")\n",
    "print(X_poly_2[:5])\n",
    "print(f\"Shape: {X_poly_2.shape}\")\n",
    "\n",
    "# Features polinomiales de grado 3\n",
    "X_poly_3 = crear_features_polinomiales(x_cuad, 3)\n",
    "print(f\"\\nFeatures polinomiales (grado 3):\")\n",
    "print(f\"Columnas: [x, x¬≤, x¬≥]\")\n",
    "print(f\"Primeras 5 filas:\")\n",
    "print(X_poly_3[:5])\n",
    "print(f\"Shape: {X_poly_3.shape}\")\n",
    "\n",
    "print(f\"\\nObservaci√≥n clave:\")\n",
    "print(f\"‚Ä¢ Transformamos 1 feature original en m√∫ltiples features\")\n",
    "print(f\"‚Ä¢ El modelo sigue siendo 'lineal' en los par√°metros w\")\n",
    "print(f\"‚Ä¢ Pero puede capturar relaciones no lineales en x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Regresi√≥n Polinomial en Acci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar diferentes grados polinomiales\n",
    "def ajustar_polinomio(x, y, grado):\n",
    "    \"\"\"\n",
    "    Ajusta un polinomio de grado especificado a los datos\n",
    "    \"\"\"\n",
    "    # Crear features polinomiales\n",
    "    X_poly = crear_features_polinomiales(x, grado)\n",
    "    \n",
    "    # Normalizar features\n",
    "    X_norm, mu, sigma = zscore_normalize_features(X_poly)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    w_init = np.zeros(X_norm.shape[1])\n",
    "    w, b = gradient_descent_simple(X_norm, y, w_init, 0., 0.1, 1000)\n",
    "    \n",
    "    return w, b, mu, sigma\n",
    "\n",
    "def predecir_polinomio(x_new, w, b, mu, sigma, grado):\n",
    "    \"\"\"\n",
    "    Hace predicciones con modelo polinomial\n",
    "    \"\"\"\n",
    "    X_poly = crear_features_polinomiales(x_new, grado)\n",
    "    X_norm = (X_poly - mu) / sigma\n",
    "    return X_norm @ w + b\n",
    "\n",
    "# Probar diferentes grados en el dataset cuadr√°tico\n",
    "grados = [1, 2, 3, 5]\n",
    "colores = ['blue', 'green', 'red', 'purple']\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Datos originales\n",
    "plt.scatter(x_cuad, y_cuad, alpha=0.7, s=50, color='black', \n",
    "           label='Datos reales', zorder=5)\n",
    "\n",
    "# Ajustar y visualizar cada grado\n",
    "x_plot = np.linspace(np.min(x_cuad), np.max(x_cuad), 100)\n",
    "resultados = []\n",
    "\n",
    "for grado, color in zip(grados, colores):\n",
    "    # Ajustar modelo\n",
    "    w, b, mu, sigma = ajustar_polinomio(x_cuad, y_cuad, grado)\n",
    "    \n",
    "    # Predicciones\n",
    "    y_plot = predecir_polinomio(x_plot, w, b, mu, sigma, grado)\n",
    "    y_pred = predecir_polinomio(x_cuad, w, b, mu, sigma, grado)\n",
    "    \n",
    "    # Calcular R¬≤\n",
    "    ss_res = np.sum((y_cuad - y_pred)**2)\n",
    "    ss_tot = np.sum((y_cuad - np.mean(y_cuad))**2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    \n",
    "    # Visualizar\n",
    "    plt.plot(x_plot, y_plot, color=color, linewidth=2, \n",
    "             label=f'Grado {grado} (R¬≤ = {r2:.3f})')\n",
    "    \n",
    "    resultados.append({\n",
    "        'grado': grado,\n",
    "        'r2': r2,\n",
    "        'w': w,\n",
    "        'b': b\n",
    "    })\n",
    "\n",
    "plt.title('Comparaci√≥n de Grados Polinomiales\\nDatos Cuadr√°ticos Reales')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Mostrar resultados tabulados\n",
    "print(\"Resultados por grado polinomial:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"{'Grado':<6} {'R¬≤':<8} {'Interpretaci√≥n':<20}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for resultado in resultados:\n",
    "    grado = resultado['grado']\n",
    "    r2 = resultado['r2']\n",
    "    \n",
    "    if r2 < 0.7:\n",
    "        interp = \"Underfitting\"\n",
    "    elif r2 > 0.95 and grado > 3:\n",
    "        interp = \"Posible Overfitting\"\n",
    "    else:\n",
    "        interp = \"Buen ajuste\"\n",
    "    \n",
    "    print(f\"{grado:<6} {r2:<8.3f} {interp:<20}\")\n",
    "\n",
    "print(f\"\\nObservaciones:\")\n",
    "print(f\"  ‚Ä¢ Grado 1 (lineal): Underfitting claro\")\n",
    "print(f\"  ‚Ä¢ Grado 2: Perfecto para datos cuadr√°ticos\")\n",
    "print(f\"  ‚Ä¢ Grados altos: Mejor R¬≤ pero riesgo de overfitting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. üîç Exploraci√≥n Sistem√°tica de Grados Polinomiales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lisis sistem√°tico de todos los datasets\n",
    "def analizar_grados_sistematico(datasets, nombres, grados_prueba):\n",
    "    \"\"\"\n",
    "    Analiza diferentes grados polinomiales para m√∫ltiples datasets\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    resultados_completos = []\n",
    "    \n",
    "    for dataset_idx, ((x, y), nombre_dataset) in enumerate(zip(datasets, nombres)):\n",
    "        # Subplot para este dataset\n",
    "        plt.subplot(2, 2, dataset_idx + 1)\n",
    "        \n",
    "        # Datos originales\n",
    "        plt.scatter(x, y, alpha=0.7, s=30, color='black', label='Datos reales')\n",
    "        \n",
    "        colores = plt.cm.viridis(np.linspace(0, 1, len(grados_prueba)))\n",
    "        x_plot = np.linspace(np.min(x), np.max(x), 100)\n",
    "        \n",
    "        dataset_resultados = []\n",
    "        \n",
    "        for grado, color in zip(grados_prueba, colores):\n",
    "            try:\n",
    "                # Ajustar modelo\n",
    "                w, b, mu, sigma = ajustar_polinomio(x, y, grado)\n",
    "                \n",
    "                # Predicciones\n",
    "                y_pred = predecir_polinomio(x, w, b, mu, sigma, grado)\n",
    "                y_plot = predecir_polinomio(x_plot, w, b, mu, sigma, grado)\n",
    "                \n",
    "                # M√©tricas\n",
    "                ss_res = np.sum((y - y_pred)**2)\n",
    "                ss_tot = np.sum((y - np.mean(y))**2)\n",
    "                r2 = 1 - (ss_res / ss_tot)\n",
    "                \n",
    "                mse = np.mean((y - y_pred)**2)\n",
    "                \n",
    "                # Solo mostrar algunos grados para claridad visual\n",
    "                if grado in [1, 2, 3, 6]:\n",
    "                    plt.plot(x_plot, y_plot, color=color, linewidth=2, \n",
    "                            label=f'Grado {grado} (R¬≤={r2:.2f})')\n",
    "                \n",
    "                dataset_resultados.append({\n",
    "                    'grado': grado,\n",
    "                    'r2': r2,\n",
    "                    'mse': mse,\n",
    "                    'dataset': nombre_dataset\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error con grado {grado} en {nombre_dataset}: {e}\")\n",
    "        \n",
    "        resultados_completos.extend(dataset_resultados)\n",
    "        \n",
    "        plt.title(f'{nombre_dataset}', fontsize=12)\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return resultados_completos\n",
    "\n",
    "# Ejecutar an√°lisis sistem√°tico\n",
    "grados_prueba = range(1, 11)  # Grados 1 a 10\n",
    "resultados_completos = analizar_grados_sistematico(datasets, nombres, grados_prueba)\n",
    "\n",
    "print(\"An√°lisis sistem√°tico completado\")\n",
    "print(f\"Total de experimentos: {len(resultados_completos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 An√°lisis de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar tendencias de R¬≤ vs grado\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Organizar datos por dataset\n",
    "datasets_resultados = {}\n",
    "for resultado in resultados_completos:\n",
    "    dataset = resultado['dataset']\n",
    "    if dataset not in datasets_resultados:\n",
    "        datasets_resultados[dataset] = {'grados': [], 'r2': [], 'mse': []}\n",
    "    datasets_resultados[dataset]['grados'].append(resultado['grado'])\n",
    "    datasets_resultados[dataset]['r2'].append(resultado['r2'])\n",
    "    datasets_resultados[dataset]['mse'].append(resultado['mse'])\n",
    "\n",
    "# Subplot 1: R¬≤ vs Grado\n",
    "plt.subplot(2, 2, 1)\n",
    "for dataset, datos in datasets_resultados.items():\n",
    "    plt.plot(datos['grados'], datos['r2'], 'o-', linewidth=2, markersize=6, \n",
    "             label=dataset.split('(')[0].strip())\n",
    "\n",
    "plt.title('R¬≤ vs Grado Polinomial')\n",
    "plt.xlabel('Grado Polinomial')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.ylim(0, 1.05)\n",
    "\n",
    "# Subplot 2: MSE vs Grado\n",
    "plt.subplot(2, 2, 2)\n",
    "for dataset, datos in datasets_resultados.items():\n",
    "    plt.plot(datos['grados'], datos['mse'], 'o-', linewidth=2, markersize=6, \n",
    "             label=dataset.split('(')[0].strip())\n",
    "\n",
    "plt.title('MSE vs Grado Polinomial')\n",
    "plt.xlabel('Grado Polinomial')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Subplot 3: Mejor grado por dataset\n",
    "plt.subplot(2, 2, 3)\n",
    "mejores_grados = []\n",
    "nombres_datasets = []\n",
    "mejores_r2 = []\n",
    "\n",
    "for dataset, datos in datasets_resultados.items():\n",
    "    # Encontrar grado con mejor R¬≤ (pero penalizar grados muy altos)\n",
    "    r2_array = np.array(datos['r2'])\n",
    "    grados_array = np.array(datos['grados'])\n",
    "    \n",
    "    # Penalizar grados altos (regularizaci√≥n simple)\n",
    "    penalizacion = 0.01 * grados_array\n",
    "    r2_penalizado = r2_array - penalizacion\n",
    "    \n",
    "    mejor_idx = np.argmax(r2_penalizado)\n",
    "    mejores_grados.append(grados_array[mejor_idx])\n",
    "    nombres_datasets.append(dataset.split('(')[0].strip())\n",
    "    mejores_r2.append(r2_array[mejor_idx])\n",
    "\n",
    "bars = plt.bar(nombres_datasets, mejores_grados, \n",
    "               color=['skyblue', 'lightgreen', 'lightcoral', 'lightsalmon'],\n",
    "               alpha=0.7, edgecolor='black')\n",
    "\n",
    "plt.title('Mejor Grado por Dataset\\n(con penalizaci√≥n por complejidad)')\n",
    "plt.xlabel('Dataset')\n",
    "plt.ylabel('Grado √ìptimo')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# A√±adir valores en las barras\n",
    "for bar, grado, r2 in zip(bars, mejores_grados, mejores_r2):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.1,\n",
    "             f'{grado}\\n(R¬≤={r2:.2f})', ha='center', va='bottom')\n",
    "\n",
    "# Subplot 4: Distribuci√≥n de complejidad\n",
    "plt.subplot(2, 2, 4)\n",
    "todos_grados = [resultado['grado'] for resultado in resultados_completos]\n",
    "todos_r2 = [resultado['r2'] for resultado in resultados_completos]\n",
    "\n",
    "plt.scatter(todos_grados, todos_r2, alpha=0.6, s=50)\n",
    "plt.title('Complejidad vs Rendimiento\\n(Todos los experimentos)')\n",
    "plt.xlabel('Grado Polinomial')\n",
    "plt.ylabel('R¬≤ Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# L√≠nea de tendencia\n",
    "z = np.polyfit(todos_grados, todos_r2, 2)\n",
    "p = np.poly1d(z)\n",
    "x_trend = np.linspace(1, 10, 100)\n",
    "plt.plot(x_trend, p(x_trend), 'r--', alpha=0.8, linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis estad√≠stico\n",
    "print(\"An√°lisis de Grados √ìptimos:\")\n",
    "print(\"=\" * 30)\n",
    "for dataset, grado, r2 in zip(nombres_datasets, mejores_grados, mejores_r2):\n",
    "    print(f\"{dataset:<12}: Grado {grado} (R¬≤ = {r2:.3f})\")\n",
    "\n",
    "print(f\"\\nPatrones observados:\")\n",
    "print(f\"  ‚Ä¢ Datos cuadr√°ticos: Mejor con grado 2-3\")\n",
    "print(f\"  ‚Ä¢ Datos c√∫bicos: Mejor con grado 3-4\")\n",
    "print(f\"  ‚Ä¢ Datos complejos: Requieren grados m√°s altos\")\n",
    "print(f\"  ‚Ä¢ Overfitting: Grados muy altos (>7) no siempre mejoran\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ‚ö†Ô∏è Overfitting vs Underfitting\n",
    "\n",
    "### 4.1 Demostraci√≥n del Problema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset con poco ruido para demostrar overfitting claramente\n",
    "np.random.seed(123)\n",
    "x_demo = np.linspace(0, 1, 15)\n",
    "y_demo = 4*x_demo**2 - 2*x_demo + 1 + np.random.normal(0, 0.1, len(x_demo))\n",
    "\n",
    "# Probar grados extremos\n",
    "grados_extremos = [1, 2, 8, 12]\n",
    "nombres_extremos = ['Underfitting (Grado 1)', 'Apropiado (Grado 2)', \n",
    "                   'Overfitting (Grado 8)', 'Overfitting Severo (Grado 12)']\n",
    "\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for i, (grado, nombre) in enumerate(zip(grados_extremos, nombres_extremos)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    \n",
    "    # Datos de entrenamiento\n",
    "    plt.scatter(x_demo, y_demo, alpha=0.8, s=60, color='red', \n",
    "               label='Datos entrenamiento', zorder=5)\n",
    "    \n",
    "    # Ajustar modelo\n",
    "    w, b, mu, sigma = ajustar_polinomio(x_demo, y_demo, grado)\n",
    "    \n",
    "    # Predicciones en datos de entrenamiento\n",
    "    y_pred_train = predecir_polinomio(x_demo, w, b, mu, sigma, grado)\n",
    "    \n",
    "    # Calcular R¬≤ en entrenamiento\n",
    "    r2_train = 1 - np.sum((y_demo - y_pred_train)**2) / np.sum((y_demo - np.mean(y_demo))**2)\n",
    "    \n",
    "    # L√≠nea suave para visualizaci√≥n\n",
    "    x_plot = np.linspace(0, 1, 100)\n",
    "    y_plot = predecir_polinomio(x_plot, w, b, mu, sigma, grado)\n",
    "    \n",
    "    plt.plot(x_plot, y_plot, 'b-', linewidth=2, label='Modelo')\n",
    "    \n",
    "    # Funci√≥n verdadera (para referencia)\n",
    "    y_true = 4*x_plot**2 - 2*x_plot + 1\n",
    "    plt.plot(x_plot, y_true, 'g--', linewidth=2, alpha=0.7, label='Funci√≥n verdadera')\n",
    "    \n",
    "    plt.title(f'{nombre}\\nR¬≤ entrenamiento = {r2_train:.3f}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(-1, 3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Caracter√≠sticas de cada caso:\")\n",
    "print(\"-\" * 40)\n",
    "print(\"1. Underfitting (Grado 1):\")\n",
    "print(\"   ‚Ä¢ Modelo demasiado simple\")\n",
    "print(\"   ‚Ä¢ No captura la curvatura real\")\n",
    "print(\"   ‚Ä¢ Bias alto, varianza baja\")\n",
    "\n",
    "print(\"\\n2. Ajuste Apropiado (Grado 2):\")\n",
    "print(\"   ‚Ä¢ Complejidad correcta para los datos\")\n",
    "print(\"   ‚Ä¢ Captura el patr√≥n sin sobreajustar\")\n",
    "print(\"   ‚Ä¢ Balance √≥ptimo bias-varianza\")\n",
    "\n",
    "print(\"\\n3. Overfitting (Grados 8+):\")\n",
    "print(\"   ‚Ä¢ Modelo demasiado complejo\")\n",
    "print(\"   ‚Ä¢ Memoriza ruido en datos de entrenamiento\")\n",
    "print(\"   ‚Ä¢ Bias bajo, varianza alta\")\n",
    "print(\"   ‚Ä¢ Mal rendimiento en datos nuevos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Simulaci√≥n de Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular divisi√≥n train/test para demostrar overfitting\n",
    "def simular_train_test_split():\n",
    "    \"\"\"\n",
    "    Crea datos de entrenamiento y prueba para demostrar overfitting\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Funci√≥n verdadera: cuadr√°tica con ruido\n",
    "    def funcion_verdadera(x):\n",
    "        return 2 + x - 0.5*x**2 + 0.1*x**3\n",
    "    \n",
    "    # Datos de entrenamiento (pocos puntos)\n",
    "    x_train = np.linspace(0, 2, 12)\n",
    "    y_train = funcion_verdadera(x_train) + np.random.normal(0, 0.15, len(x_train))\n",
    "    \n",
    "    # Datos de prueba (m√°s puntos, sin ruido para ver patr√≥n real)\n",
    "    x_test = np.linspace(0, 2, 20)\n",
    "    y_test = funcion_verdadera(x_test) + np.random.normal(0, 0.1, len(x_test))\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, funcion_verdadera\n",
    "\n",
    "# Generar datos\n",
    "x_train, y_train, x_test, y_test, func_verdadera = simular_train_test_split()\n",
    "\n",
    "# Evaluar diferentes grados\n",
    "grados_evaluar = range(1, 10)\n",
    "resultados_train_test = []\n",
    "\n",
    "for grado in grados_evaluar:\n",
    "    # Entrenar en datos de entrenamiento\n",
    "    w, b, mu, sigma = ajustar_polinomio(x_train, y_train, grado)\n",
    "    \n",
    "    # Evaluar en entrenamiento\n",
    "    y_pred_train = predecir_polinomio(x_train, w, b, mu, sigma, grado)\n",
    "    mse_train = np.mean((y_train - y_pred_train)**2)\n",
    "    r2_train = 1 - np.sum((y_train - y_pred_train)**2) / np.sum((y_train - np.mean(y_train))**2)\n",
    "    \n",
    "    # Evaluar en prueba\n",
    "    y_pred_test = predecir_polinomio(x_test, w, b, mu, sigma, grado)\n",
    "    mse_test = np.mean((y_test - y_pred_test)**2)\n",
    "    r2_test = 1 - np.sum((y_test - y_pred_test)**2) / np.sum((y_test - np.mean(y_test))**2)\n",
    "    \n",
    "    resultados_train_test.append({\n",
    "        'grado': grado,\n",
    "        'mse_train': mse_train,\n",
    "        'mse_test': mse_test,\n",
    "        'r2_train': r2_train,\n",
    "        'r2_test': r2_test,\n",
    "        'gap': mse_test - mse_train\n",
    "    })\n",
    "\n",
    "# Visualizar curvas de aprendizaje\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: MSE Train vs Test\n",
    "plt.subplot(2, 2, 1)\n",
    "grados = [r['grado'] for r in resultados_train_test]\n",
    "mse_train = [r['mse_train'] for r in resultados_train_test]\n",
    "mse_test = [r['mse_test'] for r in resultados_train_test]\n",
    "\n",
    "plt.plot(grados, mse_train, 'o-', color='blue', linewidth=2, \n",
    "         markersize=6, label='MSE Entrenamiento')\n",
    "plt.plot(grados, mse_test, 'o-', color='red', linewidth=2, \n",
    "         markersize=6, label='MSE Prueba')\n",
    "\n",
    "plt.title('Curvas de Aprendizaje: MSE')\n",
    "plt.xlabel('Grado Polinomial')\n",
    "plt.ylabel('MSE')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Marcar punto de overfitting\n",
    "# Encontrar donde MSE test empieza a subir\n",
    "min_mse_test_idx = np.argmin(mse_test)\n",
    "mejor_grado = grados[min_mse_test_idx]\n",
    "plt.axvline(x=mejor_grado, color='green', linestyle='--', alpha=0.7, \n",
    "           label=f'√ìptimo (Grado {mejor_grado})')\n",
    "plt.legend()\n",
    "\n",
    "# Subplot 2: Gap Train-Test\n",
    "plt.subplot(2, 2, 2)\n",
    "gaps = [r['gap'] for r in resultados_train_test]\n",
    "plt.plot(grados, gaps, 'o-', color='purple', linewidth=2, markersize=6)\n",
    "plt.title('Gap: MSE Test - MSE Train\\n(Indicador de Overfitting)')\n",
    "plt.xlabel('Grado Polinomial')\n",
    "plt.ylabel('Gap MSE')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "\n",
    "# Subplot 3: Mejor modelo (grado √≥ptimo)\n",
    "plt.subplot(2, 2, 3)\n",
    "w_opt, b_opt, mu_opt, sigma_opt = ajustar_polinomio(x_train, y_train, mejor_grado)\n",
    "\n",
    "plt.scatter(x_train, y_train, color='blue', s=60, label='Train', alpha=0.8)\n",
    "plt.scatter(x_test, y_test, color='red', s=60, label='Test', alpha=0.8)\n",
    "\n",
    "x_plot = np.linspace(0, 2, 100)\n",
    "y_plot_opt = predecir_polinomio(x_plot, w_opt, b_opt, mu_opt, sigma_opt, mejor_grado)\n",
    "y_verdadera = func_verdadera(x_plot)\n",
    "\n",
    "plt.plot(x_plot, y_plot_opt, 'g-', linewidth=2, label=f'Modelo (Grado {mejor_grado})')\n",
    "plt.plot(x_plot, y_verdadera, 'k--', linewidth=2, alpha=0.7, label='Funci√≥n Verdadera')\n",
    "\n",
    "plt.title(f'Modelo √ìptimo (Grado {mejor_grado})')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 4: Modelo con overfitting\n",
    "plt.subplot(2, 2, 4)\n",
    "grado_over = 8  # Grado alto para mostrar overfitting\n",
    "w_over, b_over, mu_over, sigma_over = ajustar_polinomio(x_train, y_train, grado_over)\n",
    "\n",
    "plt.scatter(x_train, y_train, color='blue', s=60, label='Train', alpha=0.8)\n",
    "plt.scatter(x_test, y_test, color='red', s=60, label='Test', alpha=0.8)\n",
    "\n",
    "y_plot_over = predecir_polinomio(x_plot, w_over, b_over, mu_over, sigma_over, grado_over)\n",
    "plt.plot(x_plot, y_plot_over, 'r-', linewidth=2, label=f'Modelo (Grado {grado_over})')\n",
    "plt.plot(x_plot, y_verdadera, 'k--', linewidth=2, alpha=0.7, label='Funci√≥n Verdadera')\n",
    "\n",
    "plt.title(f'Overfitting (Grado {grado_over})')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabla de resultados\n",
    "print(\"An√°lisis Train/Test Split:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"{'Grado':<6} {'MSE Train':<10} {'MSE Test':<10} {'Gap':<8} {'Status':<15}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for resultado in resultados_train_test:\n",
    "    grado = resultado['grado']\n",
    "    mse_tr = resultado['mse_train']\n",
    "    mse_te = resultado['mse_test']\n",
    "    gap = resultado['gap']\n",
    "    \n",
    "    if grado == mejor_grado:\n",
    "        status = \"√ìPTIMO\"\n",
    "    elif gap < 0.01:\n",
    "        status = \"Underfitting\"\n",
    "    elif gap > 0.05:\n",
    "        status = \"Overfitting\"\n",
    "    else:\n",
    "        status = \"Aceptable\"\n",
    "    \n",
    "    print(f\"{grado:<6} {mse_tr:<10.4f} {mse_te:<10.4f} {gap:<8.4f} {status:<15}\")\n",
    "\n",
    "print(f\"\\nConclusiones clave:\")\n",
    "print(f\"  ‚Ä¢ Grado √≥ptimo: {mejor_grado} (menor MSE en test)\")\n",
    "print(f\"  ‚Ä¢ Overfitting: MSE train ‚Üì, MSE test ‚Üë\")\n",
    "print(f\"  ‚Ä¢ Gap grande indica sobreajuste\")\n",
    "print(f\"  ‚Ä¢ Validaci√≥n en test es crucial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üé® T√©cnicas Avanzadas de Feature Engineering\n",
    "\n",
    "### 5.1 Features de Interacci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostraci√≥n con m√∫ltiples variables y t√©rminos de interacci√≥n\n",
    "def crear_features_interaccion(X):\n",
    "    \"\"\"\n",
    "    Crea features de interacci√≥n para dos variables\n",
    "    \n",
    "    Para X = [x1, x2], crea:\n",
    "    [x1, x2, x1¬≤, x1*x2, x2¬≤]\n",
    "    \"\"\"\n",
    "    if X.shape[1] != 2:\n",
    "        raise ValueError(\"Esta funci√≥n requiere exactamente 2 features\")\n",
    "    \n",
    "    x1 = X[:, 0]\n",
    "    x2 = X[:, 1]\n",
    "    \n",
    "    # Features originales + interacciones + polinomiales\n",
    "    X_extended = np.column_stack([\n",
    "        x1,           # x1\n",
    "        x2,           # x2\n",
    "        x1**2,        # x1¬≤\n",
    "        x1*x2,        # x1*x2 (interacci√≥n)\n",
    "        x2**2         # x2¬≤\n",
    "    ])\n",
    "    \n",
    "    feature_names = ['x1', 'x2', 'x1¬≤', 'x1*x2', 'x2¬≤']\n",
    "    \n",
    "    return X_extended, feature_names\n",
    "\n",
    "# Crear dataset sint√©tico 2D con interacciones\n",
    "np.random.seed(1)\n",
    "n_samples = 50\n",
    "\n",
    "# Variables independientes\n",
    "x1 = np.random.uniform(-2, 2, n_samples)\n",
    "x2 = np.random.uniform(-2, 2, n_samples)\n",
    "X_base = np.column_stack([x1, x2])\n",
    "\n",
    "# Variable dependiente con interacci√≥n real\n",
    "y_interaccion = 3 + 2*x1 - x2 + 0.5*x1**2 + 1.5*x1*x2 - 0.3*x2**2 + np.random.normal(0, 0.5, n_samples)\n",
    "\n",
    "print(\"Dataset con interacciones creado:\")\n",
    "print(f\"  ‚Ä¢ Samples: {n_samples}\")\n",
    "print(f\"  ‚Ä¢ Funci√≥n real: y = 3 + 2x‚ÇÅ - x‚ÇÇ + 0.5x‚ÇÅ¬≤ + 1.5x‚ÇÅx‚ÇÇ - 0.3x‚ÇÇ¬≤\")\n",
    "print(f\"  ‚Ä¢ T√©rmino clave: 1.5x‚ÇÅx‚ÇÇ (interacci√≥n)\")\n",
    "\n",
    "# Comparar modelos con y sin interacciones\n",
    "modelos_comparacion = {\n",
    "    'Lineal Simple': X_base,\n",
    "    'Con Interacciones': crear_features_interaccion(X_base)[0]\n",
    "}\n",
    "\n",
    "resultados_modelos = {}\n",
    "\n",
    "for nombre_modelo, X_modelo in modelos_comparacion.items():\n",
    "    # Normalizar features\n",
    "    X_norm, mu, sigma = zscore_normalize_features(X_modelo)\n",
    "    \n",
    "    # Entrenar\n",
    "    w_init = np.zeros(X_norm.shape[1])\n",
    "    w, b = gradient_descent_simple(X_norm, y_interaccion, w_init, 0., 0.1, 1000)\n",
    "    \n",
    "    # Evaluar\n",
    "    y_pred = X_norm @ w + b\n",
    "    mse = np.mean((y_interaccion - y_pred)**2)\n",
    "    r2 = 1 - np.sum((y_interaccion - y_pred)**2) / np.sum((y_interaccion - np.mean(y_interaccion))**2)\n",
    "    \n",
    "    resultados_modelos[nombre_modelo] = {\n",
    "        'mse': mse,\n",
    "        'r2': r2,\n",
    "        'w': w,\n",
    "        'b': b,\n",
    "        'n_features': X_modelo.shape[1]\n",
    "    }\n",
    "\n",
    "# Mostrar comparaci√≥n\n",
    "print(f\"\\nComparaci√≥n de modelos:\")\n",
    "print(f\"{'Modelo':<20} {'Features':<9} {'MSE':<8} {'R¬≤':<8}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for nombre, resultado in resultados_modelos.items():\n",
    "    print(f\"{nombre:<20} {resultado['n_features']:<9} {resultado['mse']:<8.3f} {resultado['r2']:<8.3f}\")\n",
    "\n",
    "# An√°lisis de importancia de features (modelo con interacciones)\n",
    "X_inter, feature_names_inter = crear_features_interaccion(X_base)\n",
    "w_inter = resultados_modelos['Con Interacciones']['w']\n",
    "\n",
    "print(f\"\\nImportancia de features (modelo con interacciones):\")\n",
    "print(f\"{'Feature':<8} {'Peso':<10} {'Importancia':<12}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for feature, peso in zip(feature_names_inter, w_inter):\n",
    "    importancia = abs(peso)\n",
    "    print(f\"{feature:<8} {peso:<10.3f} {importancia:<12.3f}\")\n",
    "\n",
    "mejora_r2 = resultados_modelos['Con Interacciones']['r2'] - resultados_modelos['Lineal Simple']['r2']\n",
    "print(f\"\\nMejora con interacciones: +{mejora_r2:.3f} en R¬≤\")\n",
    "\n",
    "if abs(w_inter[3]) > 0.1:  # Peso de x1*x2\n",
    "    print(f\"‚úÖ T√©rmino de interacci√≥n x1*x2 es significativo ({w_inter[3]:.3f})\")\n",
    "else:\n",
    "    print(f\"‚ùå T√©rmino de interacci√≥n x1*x2 no es significativo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualizaci√≥n de Superficie de Decisi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar superficie de decisi√≥n 3D\n",
    "def plot_superficie_3d(X, y, modelo_params, titulo):\n",
    "    \"\"\"\n",
    "    Plotea superficie 3D del modelo\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Subplot 1: Vista 3D\n",
    "    ax1 = fig.add_subplot(121, projection='3d')\n",
    "    \n",
    "    # Puntos reales\n",
    "    ax1.scatter(X[:, 0], X[:, 1], y, c='red', s=50, alpha=0.7)\n",
    "    \n",
    "    # Crear mesh para superficie\n",
    "    x1_range = np.linspace(X[:, 0].min(), X[:, 0].max(), 20)\n",
    "    x2_range = np.linspace(X[:, 1].min(), X[:, 1].max(), 20)\n",
    "    X1_mesh, X2_mesh = np.meshgrid(x1_range, x2_range)\n",
    "    \n",
    "    # Crear features para el mesh\n",
    "    mesh_points = np.column_stack([X1_mesh.ravel(), X2_mesh.ravel()])\n",
    "    \n",
    "    if 'con_interaccion' in titulo.lower():\n",
    "        mesh_features, _ = crear_features_interaccion(mesh_points)\n",
    "    else:\n",
    "        mesh_features = mesh_points\n",
    "    \n",
    "    # Normalizar usando las mismas estad√≠sticas\n",
    "    mesh_norm = (mesh_features - modelo_params['mu']) / modelo_params['sigma']\n",
    "    \n",
    "    # Predicciones\n",
    "    Z = mesh_norm @ modelo_params['w'] + modelo_params['b']\n",
    "    Z = Z.reshape(X1_mesh.shape)\n",
    "    \n",
    "    # Superficie\n",
    "    ax1.plot_surface(X1_mesh, X2_mesh, Z, alpha=0.6, cmap='viridis')\n",
    "    \n",
    "    ax1.set_xlabel('x1')\n",
    "    ax1.set_ylabel('x2')\n",
    "    ax1.set_zlabel('y')\n",
    "    ax1.set_title(f'{titulo}\\n3D View')\n",
    "    \n",
    "    # Subplot 2: Vista de contorno\n",
    "    ax2 = fig.add_subplot(122)\n",
    "    contour = ax2.contour(X1_mesh, X2_mesh, Z, levels=15, cmap='viridis', alpha=0.6)\n",
    "    ax2.clabel(contour, inline=True, fontsize=8)\n",
    "    \n",
    "    # Puntos reales con color seg√∫n valor y\n",
    "    scatter = ax2.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='plasma', edgecolor='black')\n",
    "    plt.colorbar(scatter, ax=ax2)\n",
    "    \n",
    "    ax2.set_xlabel('x1')\n",
    "    ax2.set_ylabel('x2')\n",
    "    ax2.set_title(f'{titulo}\\nContour View')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Preparar par√°metros para visualizaci√≥n\n",
    "# Modelo lineal simple\n",
    "X_lineal_norm, mu_lineal, sigma_lineal = zscore_normalize_features(X_base)\n",
    "w_lineal, b_lineal = gradient_descent_simple(X_lineal_norm, y_interaccion, \n",
    "                                           np.zeros(2), 0., 0.1, 1000)\n",
    "\n",
    "params_lineal = {\n",
    "    'w': w_lineal,\n",
    "    'b': b_lineal,\n",
    "    'mu': mu_lineal,\n",
    "    'sigma': sigma_lineal\n",
    "}\n",
    "\n",
    "# Modelo con interacciones\n",
    "X_inter_full, _ = crear_features_interaccion(X_base)\n",
    "X_inter_norm, mu_inter, sigma_inter = zscore_normalize_features(X_inter_full)\n",
    "w_inter_full, b_inter_full = gradient_descent_simple(X_inter_norm, y_interaccion,\n",
    "                                                    np.zeros(5), 0., 0.1, 1000)\n",
    "\n",
    "params_inter = {\n",
    "    'w': w_inter_full,\n",
    "    'b': b_inter_full,\n",
    "    'mu': mu_inter,\n",
    "    'sigma': sigma_inter\n",
    "}\n",
    "\n",
    "# Visualizar ambos modelos\n",
    "print(\"Superficies de decisi√≥n:\")\n",
    "print(\"=\" * 25)\n",
    "plot_superficie_3d(X_base, y_interaccion, params_lineal, \"Modelo Lineal Simple\")\n",
    "plot_superficie_3d(X_base, y_interaccion, params_inter, \"Modelo con Interacciones\")\n",
    "\n",
    "print(\"Diferencias observadas:\")\n",
    "print(\"  ‚Ä¢ Modelo lineal: Superficie plana\")\n",
    "print(\"  ‚Ä¢ Modelo con interacciones: Superficie curva\")\n",
    "print(\"  ‚Ä¢ Interacciones capturan relaciones m√°s complejas\")\n",
    "print(\"  ‚Ä¢ Mejor ajuste a los datos reales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üß™ Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: Selecci√≥n Autom√°tica de Grado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 1: Implementar selecci√≥n autom√°tica de grado polinomial\n",
    "print(\"üß™ EJERCICIO 1: Selecci√≥n Autom√°tica de Grado Polinomial\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def seleccion_automatica_grado(x, y, grado_max=10, criterio='aic'):\n",
    "    \"\"\"\n",
    "    Selecciona autom√°ticamente el mejor grado polinomial usando criterios estad√≠sticos\n",
    "    \n",
    "    Args:\n",
    "        x, y: Datos\n",
    "        grado_max: Grado m√°ximo a probar\n",
    "        criterio: 'aic', 'bic', o 'cv' (cross-validation simplificado)\n",
    "    \n",
    "    Returns:\n",
    "        mejor_grado: Grado √≥ptimo seg√∫n el criterio\n",
    "        resultados: Informaci√≥n detallada de todos los grados\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    resultados = []\n",
    "    \n",
    "    for grado in range(1, grado_max + 1):\n",
    "        try:\n",
    "            # Ajustar modelo\n",
    "            w, b, mu, sigma = ajustar_polinomio(x, y, grado)\n",
    "            y_pred = predecir_polinomio(x, w, b, mu, sigma, grado)\n",
    "            \n",
    "            # M√©tricas b√°sicas\n",
    "            mse = np.mean((y - y_pred)**2)\n",
    "            r2 = 1 - np.sum((y - y_pred)**2) / np.sum((y - np.mean(y))**2)\n",
    "            \n",
    "            # N√∫mero de par√°metros (incluyendo bias)\n",
    "            k = grado + 1\n",
    "            \n",
    "            # Criterios de informaci√≥n\n",
    "            if mse > 0:\n",
    "                log_likelihood = -n/2 * np.log(2 * np.pi * mse) - n/2\n",
    "                aic = -2 * log_likelihood + 2 * k\n",
    "                bic = -2 * log_likelihood + k * np.log(n)\n",
    "            else:\n",
    "                aic = float('inf')\n",
    "                bic = float('inf')\n",
    "            \n",
    "            # Cross-validation simplificado (Leave-One-Out aproximado)\n",
    "            if criterio == 'cv' and n > 5:\n",
    "                cv_errors = []\n",
    "                indices = np.arange(n)\n",
    "                step = max(1, n // 5)  # 5-fold aproximado\n",
    "                \n",
    "                for i in range(0, n, step):\n",
    "                    # Train set (sin algunos puntos)\n",
    "                    mask = np.ones(n, dtype=bool)\n",
    "                    mask[i:i+step] = False\n",
    "                    \n",
    "                    if np.sum(mask) > grado:  # Suficientes puntos para entrenar\n",
    "                        x_train_cv = x[mask]\n",
    "                        y_train_cv = y[mask]\n",
    "                        x_val_cv = x[~mask]\n",
    "                        y_val_cv = y[~mask]\n",
    "                        \n",
    "                        # Entrenar en subset\n",
    "                        w_cv, b_cv, mu_cv, sigma_cv = ajustar_polinomio(x_train_cv, y_train_cv, grado)\n",
    "                        y_pred_cv = predecir_polinomio(x_val_cv, w_cv, b_cv, mu_cv, sigma_cv, grado)\n",
    "                        \n",
    "                        cv_errors.append(np.mean((y_val_cv - y_pred_cv)**2))\n",
    "                \n",
    "                cv_score = np.mean(cv_errors) if cv_errors else float('inf')\n",
    "            else:\n",
    "                cv_score = mse  # Fallback\n",
    "            \n",
    "            resultados.append({\n",
    "                'grado': grado,\n",
    "                'mse': mse,\n",
    "                'r2': r2,\n",
    "                'aic': aic,\n",
    "                'bic': bic,\n",
    "                'cv_score': cv_score,\n",
    "                'n_params': k\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en grado {grado}: {e}\")\n",
    "    \n",
    "    # Seleccionar mejor grado seg√∫n criterio\n",
    "    if criterio == 'aic':\n",
    "        mejor_idx = np.argmin([r['aic'] for r in resultados])\n",
    "    elif criterio == 'bic':\n",
    "        mejor_idx = np.argmin([r['bic'] for r in resultados])\n",
    "    elif criterio == 'cv':\n",
    "        mejor_idx = np.argmin([r['cv_score'] for r in resultados])\n",
    "    else:\n",
    "        mejor_idx = np.argmax([r['r2'] for r in resultados])  # Fallback a R¬≤\n",
    "    \n",
    "    mejor_grado = resultados[mejor_idx]['grado']\n",
    "    \n",
    "    return mejor_grado, resultados\n",
    "\n",
    "# Probar con el dataset cuadr√°tico\n",
    "criterios = ['aic', 'bic', 'cv']\n",
    "resultados_criterios = {}\n",
    "\n",
    "for criterio in criterios:\n",
    "    mejor_grado, detalles = seleccion_automatica_grado(x_cuad, y_cuad, \n",
    "                                                      grado_max=8, criterio=criterio)\n",
    "    resultados_criterios[criterio] = {\n",
    "        'mejor_grado': mejor_grado,\n",
    "        'detalles': detalles\n",
    "    }\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"Resultados de selecci√≥n autom√°tica:\")\n",
    "print(f\"{'Criterio':<10} {'Mejor Grado':<12} {'Interpretaci√≥n':<20}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for criterio, resultado in resultados_criterios.items():\n",
    "    grado = resultado['mejor_grado']\n",
    "    \n",
    "    if criterio == 'aic':\n",
    "        interp = \"Balance sesgo-varianza\"\n",
    "    elif criterio == 'bic':\n",
    "        interp = \"Penaliza complejidad\"\n",
    "    else:\n",
    "        interp = \"Rendimiento predictivo\"\n",
    "    \n",
    "    print(f\"{criterio.upper():<10} {grado:<12} {interp:<20}\")\n",
    "\n",
    "# Visualizar evoluci√≥n de criterios\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "detalles_aic = resultados_criterios['aic']['detalles']\n",
    "\n",
    "grados = [d['grado'] for d in detalles_aic]\n",
    "aics = [d['aic'] for d in detalles_aic]\n",
    "bics = [d['bic'] for d in detalles_aic]\n",
    "cvs = [d['cv_score'] for d in detalles_aic]\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(grados, aics, 'o-', linewidth=2, markersize=6)\n",
    "plt.title('AIC vs Grado')\n",
    "plt.xlabel('Grado Polinomial')\n",
    "plt.ylabel('AIC (menor es mejor)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(x=resultados_criterios['aic']['mejor_grado'], color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(grados, bics, 'o-', color='green', linewidth=2, markersize=6)\n",
    "plt.title('BIC vs Grado')\n",
    "plt.xlabel('Grado Polinomial')\n",
    "plt.ylabel('BIC (menor es mejor)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(x=resultados_criterios['bic']['mejor_grado'], color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(grados, cvs, 'o-', color='purple', linewidth=2, markersize=6)\n",
    "plt.title('CV Score vs Grado')\n",
    "plt.xlabel('Grado Polinomial')\n",
    "plt.ylabel('CV MSE (menor es mejor)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(x=resultados_criterios['cv']['mejor_grado'], color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nConclusiones:\")\n",
    "print(f\"  ‚Ä¢ AIC: Balance entre ajuste y complejidad\")\n",
    "print(f\"  ‚Ä¢ BIC: Penaliza m√°s la complejidad (grados menores)\")\n",
    "print(f\"  ‚Ä¢ CV: Enfoque en capacidad predictiva real\")\n",
    "print(f\"  ‚Ä¢ Para datos cuadr√°ticos: Todos sugieren grados 2-3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Features Personalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 2: Crear y probar features personalizadas\n",
    "print(\"üß™ EJERCICIO 2: Features Personalizadas y Transformaciones\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "def crear_features_personalizadas(x, tipo='completo'):\n",
    "    \"\"\"\n",
    "    Crea diferentes tipos de features engineered\n",
    "    \n",
    "    Args:\n",
    "        x: Feature original\n",
    "        tipo: 'polinomial', 'trigonometrico', 'logaritmico', 'completo'\n",
    "    \"\"\"\n",
    "    x = np.array(x).flatten()\n",
    "    \n",
    "    if tipo == 'polinomial':\n",
    "        # Solo features polinomiales\n",
    "        features = np.column_stack([\n",
    "            x,           # x¬π\n",
    "            x**2,        # x¬≤\n",
    "            x**3,        # x¬≥\n",
    "            x**0.5       # ‚àöx (para x positivos)\n",
    "        ])\n",
    "        nombres = ['x', 'x¬≤', 'x¬≥', '‚àöx']\n",
    "        \n",
    "    elif tipo == 'trigonometrico':\n",
    "        # Features trigonom√©tricas\n",
    "        features = np.column_stack([\n",
    "            x,\n",
    "            np.sin(x),\n",
    "            np.cos(x),\n",
    "            np.sin(2*x),\n",
    "            np.cos(2*x)\n",
    "        ])\n",
    "        nombres = ['x', 'sin(x)', 'cos(x)', 'sin(2x)', 'cos(2x)']\n",
    "        \n",
    "    elif tipo == 'logaritmico':\n",
    "        # Features logar√≠tmicas (para x positivos)\n",
    "        x_pos = np.maximum(x, 0.01)  # Evitar log(0)\n",
    "        features = np.column_stack([\n",
    "            x,\n",
    "            np.log(x_pos),\n",
    "            np.log(x_pos + 1),\n",
    "            x_pos * np.log(x_pos)\n",
    "        ])\n",
    "        nombres = ['x', 'log(x)', 'log(x+1)', 'x¬∑log(x)']\n",
    "        \n",
    "    elif tipo == 'completo':\n",
    "        # Combinaci√≥n de diferentes tipos\n",
    "        x_pos = np.maximum(x, 0.01)\n",
    "        features = np.column_stack([\n",
    "            x,                    # Lineal\n",
    "            x**2,                 # Cuadr√°tico\n",
    "            x**3,                 # C√∫bico\n",
    "            np.sqrt(x_pos),       # Ra√≠z\n",
    "            np.sin(x),            # Seno\n",
    "            np.cos(x),            # Coseno\n",
    "            np.log(x_pos + 1),    # Logar√≠tmico\n",
    "            x * np.sin(x)         # Interacci√≥n\n",
    "        ])\n",
    "        nombres = ['x', 'x¬≤', 'x¬≥', '‚àöx', 'sin(x)', 'cos(x)', 'log(x+1)', 'x¬∑sin(x)']\n",
    "    \n",
    "    return features, nombres\n",
    "\n",
    "# Probar con el dataset sinusoidal (m√°s interesante para features trigonom√©tricas)\n",
    "x_sinus, y_sinus = datasets[2]\n",
    "\n",
    "tipos_features = ['polinomial', 'trigonometrico', 'logaritmico', 'completo']\n",
    "resultados_features = {}\n",
    "\n",
    "print(f\"Probando diferentes tipos de features en datos sinusoidales...\")\n",
    "print(f\"\\n{'Tipo Features':<15} {'N Features':<10} {'R¬≤':<8} {'MSE':<10} {'Interpretaci√≥n':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for tipo in tipos_features:\n",
    "    try:\n",
    "        # Crear features\n",
    "        X_custom, nombres_custom = crear_features_personalizadas(x_sinus, tipo)\n",
    "        \n",
    "        # Normalizar\n",
    "        X_norm, mu, sigma = zscore_normalize_features(X_custom)\n",
    "        \n",
    "        # Entrenar\n",
    "        w_init = np.zeros(X_norm.shape[1])\n",
    "        w, b = gradient_descent_simple(X_norm, y_sinus, w_init, 0., 0.1, 1000)\n",
    "        \n",
    "        # Evaluar\n",
    "        y_pred = X_norm @ w + b\n",
    "        mse = np.mean((y_sinus - y_pred)**2)\n",
    "        r2 = 1 - np.sum((y_sinus - y_pred)**2) / np.sum((y_sinus - np.mean(y_sinus))**2)\n",
    "        \n",
    "        # Interpretaci√≥n\n",
    "        if r2 > 0.9:\n",
    "            interp = \"Excelente\"\n",
    "        elif r2 > 0.7:\n",
    "            interp = \"Bueno\"\n",
    "        elif r2 > 0.5:\n",
    "            interp = \"Regular\"\n",
    "        else:\n",
    "            interp = \"Pobre\"\n",
    "        \n",
    "        resultados_features[tipo] = {\n",
    "            'X': X_custom,\n",
    "            'nombres': nombres_custom,\n",
    "            'w': w,\n",
    "            'b': b,\n",
    "            'mu': mu,\n",
    "            'sigma': sigma,\n",
    "            'r2': r2,\n",
    "            'mse': mse\n",
    "        }\n",
    "        \n",
    "        print(f\"{tipo:<15} {X_custom.shape[1]:<10} {r2:<8.3f} {mse:<10.3f} {interp:<15}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{tipo:<15} ERROR: {str(e)[:30]}\")\n",
    "\n",
    "# Visualizar el mejor modelo\n",
    "mejor_tipo = max(resultados_features.keys(), key=lambda k: resultados_features[k]['r2'])\n",
    "mejor_resultado = resultados_features[mejor_tipo]\n",
    "\n",
    "print(f\"\\nMejor enfoque: {mejor_tipo} (R¬≤ = {mejor_resultado['r2']:.3f})\")\n",
    "\n",
    "# Visualizar comparaci√≥n\n",
    "plt.figure(figsize=(16, 12))\n",
    "\n",
    "for i, (tipo, resultado) in enumerate(resultados_features.items()):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    \n",
    "    # Datos originales\n",
    "    plt.scatter(x_sinus, y_sinus, alpha=0.7, s=30, color='red', label='Datos reales')\n",
    "    \n",
    "    # Predicciones del modelo\n",
    "    x_plot = np.linspace(np.min(x_sinus), np.max(x_sinus), 100)\n",
    "    X_plot_custom, _ = crear_features_personalizadas(x_plot, tipo)\n",
    "    X_plot_norm = (X_plot_custom - resultado['mu']) / resultado['sigma']\n",
    "    y_plot = X_plot_norm @ resultado['w'] + resultado['b']\n",
    "    \n",
    "    plt.plot(x_plot, y_plot, 'b-', linewidth=2, label='Modelo')\n",
    "    \n",
    "    plt.title(f'{tipo.title()}\\nR¬≤ = {resultado[\"r2\"]:.3f}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis de importancia de features del mejor modelo\n",
    "print(f\"\\nAn√°lisis del mejor modelo ({mejor_tipo}):\")\n",
    "print(f\"{'Feature':<12} {'Peso':<10} {'Importancia':<12}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "pesos = mejor_resultado['w']\n",
    "nombres = mejor_resultado['nombres']\n",
    "importancias = np.abs(pesos)\n",
    "\n",
    "# Ordenar por importancia\n",
    "indices_ordenados = np.argsort(importancias)[::-1]\n",
    "\n",
    "for idx in indices_ordenados:\n",
    "    print(f\"{nombres[idx]:<12} {pesos[idx]:<10.3f} {importancias[idx]:<12.3f}\")\n",
    "\n",
    "print(f\"\\nConclusiones del ejercicio:\")\n",
    "print(f\"  ‚Ä¢ Features trigonom√©tricas son ideales para datos sinusoidales\")\n",
    "print(f\"  ‚Ä¢ Features polinomiales funcionan bien para curvas suaves\")\n",
    "print(f\"  ‚Ä¢ Combinar tipos puede mejorar el rendimiento\")\n",
    "print(f\"  ‚Ä¢ La elecci√≥n de features debe basarse en el dominio del problema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Resumen y Conceptos Clave\n",
    "\n",
    "### ‚úÖ Lo que has aprendido:\n",
    "\n",
    "#### 1. **Feature Engineering Fundamentals**:\n",
    "   - **Concepto**: Crear nuevas features a partir de las existentes\n",
    "   - **Objetivo**: Capturar relaciones no lineales con modelos lineales\n",
    "   - **Principio**: El modelo sigue siendo lineal en los par√°metros w\n",
    "\n",
    "#### 2. **Regresi√≥n Polinomial**:\n",
    "   - **Transformaci√≥n**: x ‚Üí [x, x¬≤, x¬≥, ..., x‚Åø]\n",
    "   - **Modelo**: y = w‚ÇÅx + w‚ÇÇx¬≤ + w‚ÇÉx¬≥ + ... + b\n",
    "   - **Flexibilidad**: Puede aproximar funciones complejas\n",
    "\n",
    "#### 3. **Selecci√≥n de Grado**:\n",
    "   - **Underfitting**: Grado muy bajo ‚Üí Bias alto\n",
    "   - **Overfitting**: Grado muy alto ‚Üí Varianza alta  \n",
    "   - **Grado √≥ptimo**: Balance sesgo-varianza\n",
    "   - **Criterios**: AIC, BIC, Cross-validation\n",
    "\n",
    "#### 4. **Tipos de Features**:\n",
    "   - **Polinomiales**: x¬≤, x¬≥, ‚àöx\n",
    "   - **Trigonom√©tricas**: sin(x), cos(x)\n",
    "   - **Logar√≠tmicas**: log(x), log(x+1)\n",
    "   - **Interacciones**: x‚ÇÅ √ó x‚ÇÇ\n",
    "   - **Personalizadas**: Seg√∫n dominio espec√≠fico\n",
    "\n",
    "#### 5. **Evaluaci√≥n y Validaci√≥n**:\n",
    "   - **Train/Test split**: Detectar overfitting\n",
    "   - **M√©tricas**: R¬≤, MSE, criterios de informaci√≥n\n",
    "   - **Visualizaci√≥n**: Curvas de aprendizaje, superficies 3D\n",
    "\n",
    "### üéØ Aplicaciones Pr√°cticas:\n",
    "- **Datos cuadr√°ticos**: Mejor con grado 2-3\n",
    "- **Datos sinusoidales**: Features trigonom√©tricas ideales\n",
    "- **Datos exponenciales**: Transformaciones logar√≠tmicas\n",
    "- **Interacciones**: T√©rminos x‚ÇÅ √ó x‚ÇÇ para relaciones complejas\n",
    "\n",
    "### üöÄ T√©cnicas Avanzadas Cubiertas:\n",
    "- **Selecci√≥n autom√°tica de grado** usando AIC/BIC\n",
    "- **Features de interacci√≥n** para m√∫ltiples variables\n",
    "- **Visualizaci√≥n 3D** de superficies de decisi√≥n\n",
    "- **Cross-validation** para validaci√≥n robusta\n",
    "\n",
    "### üí° Best Practices:\n",
    "1. **Analizar los datos** antes de elegir features\n",
    "2. **Empezar simple** y aumentar complejidad gradualmente\n",
    "3. **Usar validaci√≥n cruzada** para selecci√≥n de modelo\n",
    "4. **Monitorear overfitting** con train/test split\n",
"   5. **Normalizar features** antes de aplicar polinomios\n",
"   6. **Validar con datos de prueba** para detectar overfitting\n",
"   7. **Interpretar resultados** en el contexto del problema\n",
"\n",
"### ‚ö†Ô∏è Cuidados Importantes:\n",
"- **Overfitting**: Grados muy altos memorizan ruido\n",
"- **Underfitting**: Grados muy bajos no capturan patrones\n",
"- **Multicolinealidad**: Features polinomiales est√°n correlacionadas\n",
"- **Escalabilidad**: N√∫mero de features crece exponencialmente\n",
"- **Interpretabilidad**: Modelos complejos son dif√≠ciles de explicar\n",
"\n",
"### üîÆ Cu√°ndo Usar Cada T√©cnica:\n",
"\n",
"#### **Regresi√≥n Polinomial**:\n",
"- ‚úÖ Datos con curvatura clara\n",
"- ‚úÖ Relaciones suaves y continuas  \n",
"- ‚úÖ Dataset peque√±o a mediano\n",
"- ‚ùå Datos muy ruidosos\n",
"- ‚ùå Extrapolaci√≥n fuera del rango\n",
"\n",
"#### **Features Trigonom√©tricas**:\n",
"- ‚úÖ Patrones c√≠clicos o peri√≥dicos\n",
"- ‚úÖ Datos de series temporales\n",
"- ‚úÖ Fen√≥menos oscilatorios\n",
"- ‚ùå Relaciones estrictamente mon√≥tonas\n",
"\n",
"#### **Features Logar√≠tmicas**:\n",
"- ‚úÖ Crecimiento exponencial\n",
"- ‚úÖ Datos con gran rango din√°mico\n",
"- ‚úÖ Relaciones multiplicativas\n",
"- ‚ùå Datos con valores negativos o cero\n",
"\n",
"#### **Features de Interacci√≥n**:\n",
"- ‚úÖ Variables que se influencian mutuamente\n",
"- ‚úÖ Efectos combinados evidentes\n",
"- ‚úÖ An√°lisis de sensibilidad\n",
"- ‚ùå Muchas variables (explosi√≥n combinatoria)\n",
"\n",
"### üéØ Selecci√≥n de Grado Polinomial:\n",
"\n",
"| **Criterio** | **Descripci√≥n** | **Cu√°ndo Usar** |\n",
"|--------------|-----------------|------------------|\n",
"| **R¬≤** | Bondad de ajuste b√°sico | Exploraci√≥n inicial |\n",
"| **AIC** | Penaliza complejidad suavemente | Balance general |\n",
"| **BIC** | Penaliza complejidad fuertemente | Modelos parsimoniosos |\n",
"| **Cross-validation** | Estimaci√≥n real de generalizaci√≥n | Selecci√≥n final |\n",
"| **Hold-out test** | Validaci√≥n independiente | Evaluaci√≥n definitiva |\n",
"\n",
"### üìä Diagn√≥stico de Problemas:\n",
"\n",
"#### **S√≠ntomas de Underfitting**:\n",
"- R¬≤ bajo en entrenamiento Y prueba\n",
"- Residuos con patrones sistem√°ticos  \n",
"- Curva de predicci√≥n muy simple\n",
"- **Soluci√≥n**: Aumentar complejidad del modelo\n",
"\n",
"#### **S√≠ntomas de Overfitting**:\n",
"- R¬≤ alto en entrenamiento, bajo en prueba\n",
"- Gap grande entre train/test performance\n",
"- Curva de predicci√≥n muy ondulada\n",
"- **Soluci√≥n**: Reducir complejidad o regularizar\n",
"\n",
"#### **S√≠ntomas de Buen Ajuste**:\n",
"- R¬≤ similar en entrenamiento y prueba\n",
"- Residuos aleatorios sin patrones\n",
"- Curva suave que captura tendencias\n",
"- **Acci√≥n**: ¬°Modelo listo para uso!\n",
"\n",
"### üî¨ Workflow Recomendado:\n",
"\n",
"```python\n",
"# 1. AN√ÅLISIS EXPLORATORIO\n",
"plt.scatter(x, y)  # Visualizar relaci√≥n\n",
"# ¬øLineal? ¬øCuadr√°tica? ¬øSinusoidal?\n",
"\n",
"# 2. DIVISI√ìN DE DATOS\n",
"X_train, X_test, y_train, y_test = train_test_split(...)\n",
"\n",
"# 3. FEATURE ENGINEERING\n",
"X_poly = PolynomialFeatures(degree=2)\n",
"X_train_poly = X_poly.fit_transform(X_train)\n",
"\n",
"# 4. NORMALIZACI√ìN\n",
"scaler = StandardScaler()\n",
"X_train_scaled = scaler.fit_transform(X_train_poly)\n",
"\n",
"# 5. ENTRENAMIENTO\n",
"model = LinearRegression()\n",
"model.fit(X_train_scaled, y_train)\n",
"\n",
"# 6. VALIDACI√ìN\n",
"cv_scores = cross_val_score(model, X_train_scaled, y_train)\n",
"\n",
"# 7. EVALUACI√ìN FINAL\n",
"X_test_poly = X_poly.transform(X_test)\n",
"X_test_scaled = scaler.transform(X_test_poly)\n",
"predictions = model.predict(X_test_scaled)\n",
"```\n",
"\n",
"### üìà M√©tricas de Evaluaci√≥n Importantes:\n",
"\n",
"#### **Para Regresi√≥n**:\n",
"- **R¬≤**: Proporci√≥n de varianza explicada (0-1, mayor mejor)\n",
"- **MSE**: Error cuadr√°tico medio (‚â•0, menor mejor)\n",
"- **RMSE**: Ra√≠z del MSE, en mismas unidades que y\n",
"- **MAE**: Error absoluto medio, robusto a outliers\n",
"\n",
"#### **Para Selecci√≥n de Modelo**:\n",
"- **AIC/BIC**: Criterios de informaci√≥n (menor mejor)\n",
"- **Cross-validation**: Estimaci√≥n robusta de generalizaci√≥n\n",
"- **Learning curves**: Detectar overfitting/underfitting\n",
"- **Validation curves**: Optimizar hiperpar√°metros\n",
"\n",
"### üö® Errores Comunes a Evitar:\n",
"\n",
"1. **Data Leakage**: Normalizar antes de dividir train/test\n",
"   ```python\n",
"   # ‚ùå INCORRECTO\n",
"   X_scaled = scaler.fit_transform(X)  # Usa info de test\n",
"   X_train, X_test = train_test_split(X_scaled)\n",
"   \n",
"   # ‚úÖ CORRECTO  \n",
"   X_train, X_test = train_test_split(X)\n",
"   X_train_scaled = scaler.fit_transform(X_train)\n",
"   X_test_scaled = scaler.transform(X_test)\n",
"   ```\n",
"\n",
"2. **Extrapolaci√≥n Peligrosa**: Polinomios se comportan mal fuera del rango\n",
"   ```python\n",
"   # Verificar rango de predicciones\n",
"   print(f\"Train range: [{X_train.min()}, {X_train.max()}]\")\n",
"   print(f\"Test range: [{X_test.min()}, {X_test.max()}]\")\n",
"   ```\n",
"\n",
"3. **Ignorar Multicolinealidad**: Features polinomiales est√°n correlacionadas\n",
"   ```python\n",
"   # Verificar correlaci√≥n entre features\n",
"   correlation_matrix = np.corrcoef(X_poly.T)\n",
"   # Considerar Ridge/Lasso si hay multicolinealidad alta\n",
"   ```\n",
"\n",
"4. **Optimizar en Test Set**: Solo usar test para evaluaci√≥n final\n",
"   ```python\n",
"   # ‚ùå INCORRECTO: Optimizar grado usando test\n",
"   best_degree = tune_degree_on_test()  # Sesgo optimista\n",
"   \n",
"   # ‚úÖ CORRECTO: Usar validation set o CV\n",
"   best_degree = cross_val_tune_degree()\n",
"   final_score = evaluate_on_test()  # Una sola vez\n",
"   ```\n",
"\n",
"### üéì Extensiones Avanzadas:\n",
"\n",
"#### **Regularizaci√≥n con Features Polinomiales**:\n",
"```python\n",
"from sklearn.linear_model import Ridge, Lasso\n",
"\n",
"# Ridge: Penaliza magnitud de coeficientes\n",
"ridge = Ridge(alpha=1.0)\n",
"\n",
"# Lasso: Puede eliminar features (selecci√≥n autom√°tica)\n",
"lasso = Lasso(alpha=1.0)\n",
"```\n",
"\n",
"#### **Splines para Flexibilidad Local**:\n",
"```python\n",
"from scipy.interpolate import UnivariateSpline\n",
"\n",
"# Splines: Polinomios por pedazos\n",
"spline = UnivariateSpline(x, y, s=100)  # s controla suavidad\n",
"```\n",
"\n",
"#### **Features Personalizadas Avanzadas**:\n",
"```python\n",
"# Features basadas en dominio espec√≠fico\n",
"def create_domain_features(X):\n",
"    return np.column_stack([\n",
"        X,                    # Original\n",
"        np.exp(-X**2),        # Gaussiana\n",
"        1 / (1 + X**2),       # Cauchy\n",
"        np.tanh(X),           # Tangente hiperb√≥lica\n",
"        np.sign(X) * X**2     # Cuadr√°tica con signo\n",
"    ])\n",
"```\n",
"\n",
"### üî¨ Casos de Estudio T√≠picos:\n",
"\n",
"#### **Caso 1: Precio de Casas**\n",
"- **Features**: Tama√±o, habitaciones, edad\n",
"- **Engineering**: Tama√±o¬≤, habitaciones√ópisos, log(edad+1)\n",
"- **Justificaci√≥n**: Efectos no lineales y de interacci√≥n conocidos\n",
"\n",
"#### **Caso 2: Ventas Estacionales**\n",
"- **Features**: Mes del a√±o\n",
"- **Engineering**: sin(2œÄt/12), cos(2œÄt/12) para capturar estacionalidad\n",
"- **Justificaci√≥n**: Patrones c√≠clicos evidentes\n",
"\n",
"#### **Caso 3: Crecimiento Biol√≥gico**\n",
"- **Features**: Tiempo\n",
"- **Engineering**: t, t¬≤, log(t+1), exp(-t) para diferentes fases\n",
"- **Justificaci√≥n**: Crecimiento exponencial inicial, saturaci√≥n posterior\n",
"\n",
"### üåü Pr√≥ximos Pasos Sugeridos:\n",
"\n",
"Despu√©s de dominar feature engineering b√°sico:\n",
"\n",
"1. **Regularizaci√≥n**: Ridge, Lasso, Elastic Net\n",
"2. **Selecci√≥n de Features**: M√©todos autom√°ticos\n",
"3. **Algoritmos No Lineales**: Random Forest, SVM, Neural Networks\n",
"4. **Feature Learning**: Autoencoders, PCA, t-SNE\n",
"5. **Domain-Specific Engineering**: Seg√∫n tu √°rea de aplicaci√≥n\n",
"\n",
"### üèÜ Conclusi√≥n Final:\n",
"\n",
"**Feature Engineering es tanto arte como ciencia:**\n",
"- üß† **Ciencia**: M√©todos sistem√°ticos, validaci√≥n estad√≠stica\n",
"- üé® **Arte**: Intuici√≥n del dominio, creatividad en transformaciones\n",
"- üîß **Pr√°ctica**: Experimentaci√≥n iterativa, evaluaci√≥n rigurosa\n",
"\n",
"**Has aprendido a:**\n",
"- Transformar features para capturar patrones no lineales\n",
"- Evaluar y seleccionar grados polinomiales apropiados\n",
"- Crear features personalizadas seg√∫n el problema\n",
"- Detectar y evitar overfitting\n",
"- Usar herramientas de validaci√≥n robustas\n",
"\n",
"**¬°Ahora puedes hacer que los modelos lineales capturen relaciones complejas!** üöÄ\n",
"\n",
"---\n",
"\n",
"*En el pr√≥ximo notebook veremos c√≥mo **Scikit-Learn** automatiza y optimiza todos estos procesos para uso profesional.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}