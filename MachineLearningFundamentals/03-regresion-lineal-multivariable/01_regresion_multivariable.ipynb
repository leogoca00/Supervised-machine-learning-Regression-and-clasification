{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresi√≥n Lineal con M√∫ltiples Variables\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "En este notebook aprender√°s:\n",
    "- Extender regresi√≥n lineal a m√∫ltiples features\n",
    "- Implementar vectorizaci√≥n con NumPy para eficiencia\n",
    "- Desarrollar gradient descent para m√∫ltiples variables\n",
    "- Aplicar el modelo a predicci√≥n de precios de casas\n",
    "\n",
    "## Problema de Negocio\n",
    "**Escenario**: Eres un agente inmobiliario que necesita estimar precios de casas bas√°ndose en m√∫ltiples caracter√≠sticas:\n",
    "- **Tama√±o** de la casa (pies cuadrados)\n",
    "- **N√∫mero de habitaciones**\n",
    "- **N√∫mero de pisos**\n",
    "- **Edad** de la casa (a√±os)\n",
    "\n",
    "**Objetivo**: Crear un modelo que prediga el precio usando todas estas caracter√≠sticas simult√°neamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "\n",
    "# Configuraci√≥n\n",
    "plt.style.use('default')\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"Librer√≠as importadas correctamente\")\n",
    "print(\"Listo para regresi√≥n multivariable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset: Precios de Casas\n",
    "\n",
    "### 1.1 Cargar y Explorar los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para cargar datos de casas\n",
    "def cargar_datos_casas():\n",
    "    \"\"\"\n",
    "    Carga dataset de precios de casas con m√∫ltiples features\n",
    "    \n",
    "    Returns:\n",
    "        X_train: Matriz de features (m, n)\n",
    "        y_train: Vector de precios (m,)\n",
    "        feature_names: Nombres de las caracter√≠sticas\n",
    "    \"\"\"\n",
    "    # Dataset real de casas\n",
    "    # Cada fila: [tama√±o_sqft, habitaciones, pisos, edad]\n",
    "    X_train = np.array([\n",
    "        [2104, 5, 1, 45],    # Casa 1\n",
    "        [1416, 3, 2, 40],    # Casa 2  \n",
    "        [852,  2, 1, 35],    # Casa 3\n",
    "        [1940, 4, 1, 10],    # Casa 4\n",
    "        [1539, 3, 2, 25],    # Casa 5\n",
    "        [3000, 4, 2, 8],     # Casa 6\n",
    "        [1230, 3, 1, 15],    # Casa 7\n",
    "        [2145, 4, 1, 12],    # Casa 8\n",
    "        [1736, 3, 2, 30],    # Casa 9\n",
    "        [1000, 2, 1, 50]     # Casa 10\n",
    "    ])\n",
    "    \n",
    "    # Precios correspondientes (en miles de d√≥lares)\n",
    "    y_train = np.array([460, 232, 178, 500, 315, 740, 285, 510, 390, 180])\n",
    "    \n",
    "    # Nombres de features\n",
    "    feature_names = ['Tama√±o (sqft)', 'Habitaciones', 'Pisos', 'Edad (a√±os)']\n",
    "    \n",
    "    return X_train, y_train, feature_names\n",
    "\n",
    "# Cargar datos\n",
    "X_train, y_train, feature_names = cargar_datos_casas()\n",
    "\n",
    "print(f\"Dataset de casas cargado:\")\n",
    "print(f\"  ‚Ä¢ N√∫mero de casas (m): {X_train.shape[0]}\")\n",
    "print(f\"  ‚Ä¢ N√∫mero de features (n): {X_train.shape[1]}\")\n",
    "print(f\"  ‚Ä¢ Shape de X_train: {X_train.shape}\")\n",
    "print(f\"  ‚Ä¢ Shape de y_train: {y_train.shape}\")\n",
    "\n",
    "print(f\"\\nPrimeras 5 casas:\")\n",
    "print(f\"{'':>5} {'Tama√±o':>8} {'Habitaciones':>12} {'Pisos':>6} {'Edad':>5} {'Precio':>8}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(5):\n",
    "    print(f\"Casa {i+1:1d}: {X_train[i,0]:>6.0f} {X_train[i,1]:>10.0f} {X_train[i,2]:>8.0f} {X_train[i,3]:>6.0f} ${y_train[i]:>6.0f}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualizaci√≥n del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar cada feature vs precio\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.scatter(X_train[:, i], y_train, alpha=0.7, s=50)\n",
    "    plt.xlabel(feature_names[i])\n",
    "    plt.ylabel('Precio (miles $)' if i == 0 else '')\n",
    "    plt.title(f'{feature_names[i]} vs Precio')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observaciones:\")\n",
    "print(\"  ‚Ä¢ Tama√±o: Relaci√≥n positiva clara - casas m√°s grandes cuestan m√°s\")\n",
    "print(\"  ‚Ä¢ Habitaciones: Tendencia positiva - m√°s habitaciones, mayor precio\")\n",
    "print(\"  ‚Ä¢ Pisos: Relaci√≥n menos clara, pero casas de 2 pisos tienden a costar m√°s\")\n",
    "print(\"  ‚Ä¢ Edad: Relaci√≥n negativa - casas m√°s nuevas cuestan m√°s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelo de Regresi√≥n Multivariable\n",
    "\n",
    "### 2.1 Extensi√≥n del Modelo Lineal\n",
    "\n",
    "**Modelo con una variable**: $f_{w,b}(x) = wx + b$\n",
    "\n",
    "**Modelo con m√∫ltiples variables**: \n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}) = w_0x_0 + w_1x_1 + w_2x_2 + w_3x_3 + b$$\n",
    "\n",
    "**En notaci√≥n vectorial**:\n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b$$\n",
    "\n",
    "Donde:\n",
    "- **x**: Vector de features [tama√±o, habitaciones, pisos, edad]\n",
    "- **w**: Vector de pesos [w‚ÇÄ, w‚ÇÅ, w‚ÇÇ, w‚ÇÉ]\n",
    "- **b**: T√©rmino de sesgo (bias)\n",
    "- **¬∑**: Producto punto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementaci√≥n de predicci√≥n multivariable\n",
    "\n",
    "def predecir_una_casa_loop(x, w, b):\n",
    "    \"\"\"\n",
    "    Predicci√≥n para una casa usando loop (versi√≥n educativa)\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Vector de features para una casa (n,)\n",
    "        w (ndarray): Vector de pesos (n,)\n",
    "        b (scalar): Bias\n",
    "    \n",
    "    Returns:\n",
    "        prediction (scalar): Precio predicho\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    prediccion = 0\n",
    "    \n",
    "    # Sumar cada w_i * x_i\n",
    "    for i in range(n):\n",
    "        prediccion += w[i] * x[i]\n",
    "    \n",
    "    # A√±adir bias\n",
    "    prediccion += b\n",
    "    \n",
    "    return prediccion\n",
    "\n",
    "def predecir_una_casa_vectorizada(x, w, b):\n",
    "    \"\"\"\n",
    "    Predicci√≥n vectorizada (versi√≥n eficiente)\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Vector de features (n,)\n",
    "        w (ndarray): Vector de pesos (n,)\n",
    "        b (scalar): Bias\n",
    "        \n",
    "    Returns:\n",
    "        prediction (scalar): Precio predicho\n",
    "    \"\"\"\n",
    "    return np.dot(x, w) + b\n",
    "\n",
    "# Probar ambas implementaciones\n",
    "# Par√°metros ejemplo (valores cercanos a √≥ptimos)\n",
    "w_inicial = np.array([0.39, 18.75, -53.36, -26.42])\n",
    "b_inicial = 785.18\n",
    "\n",
    "# Tomar primera casa como ejemplo\n",
    "x_ejemplo = X_train[0]  # [2104, 5, 1, 45]\n",
    "precio_real = y_train[0]  # 460\n",
    "\n",
    "print(f\"Casa ejemplo: {x_ejemplo}\")\n",
    "print(f\"Precio real: ${precio_real}k\")\n",
    "print(f\"\\nPredicciones:\")\n",
    "\n",
    "# Predicci√≥n con loop\n",
    "pred_loop = predecir_una_casa_loop(x_ejemplo, w_inicial, b_inicial)\n",
    "print(f\"  Con loop: ${pred_loop:.1f}k\")\n",
    "\n",
    "# Predicci√≥n vectorizada\n",
    "pred_vectorizada = predecir_una_casa_vectorizada(x_ejemplo, w_inicial, b_inicial)\n",
    "print(f\"  Vectorizada: ${pred_vectorizada:.1f}k\")\n",
    "print(f\"  ¬øSon iguales? {np.isclose(pred_loop, pred_vectorizada)}\")\n",
    "\n",
    "print(f\"\\nDesglose de la predicci√≥n vectorizada:\")\n",
    "for i, (feature, peso, valor) in enumerate(zip(feature_names, w_inicial, x_ejemplo)):\n",
    "    contribucion = peso * valor\n",
    "    print(f\"  {feature}: {peso:.2f} √ó {valor} = {contribucion:.1f}\")\n",
    "print(f\"  Bias: {b_inicial:.1f}\")\n",
    "print(f\"  Total: {pred_vectorizada:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Comparaci√≥n de Velocidad: Loop vs Vectorizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Crear dataset grande para comparar velocidad\n",
    "np.random.seed(42)\n",
    "n_casas_grandes = 100000\n",
    "X_grande = np.random.rand(n_casas_grandes, 4) * 1000  # Features aleatorias\n",
    "w_test = np.array([0.5, 10, -20, -15])\n",
    "b_test = 100\n",
    "\n",
    "print(f\"Comparaci√≥n de velocidad con {n_casas_grandes:,} casas:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# M√©todo con loop\n",
    "start_time = time.time()\n",
    "predicciones_loop = []\n",
    "for i in range(n_casas_grandes):\n",
    "    pred = predecir_una_casa_loop(X_grande[i], w_test, b_test)\n",
    "    predicciones_loop.append(pred)\n",
    "tiempo_loop = time.time() - start_time\n",
    "\n",
    "# M√©todo vectorizado\n",
    "start_time = time.time()\n",
    "predicciones_vectorizadas = X_grande @ w_test + b_test  # @ es equivalente a np.dot\n",
    "tiempo_vectorizado = time.time() - start_time\n",
    "\n",
    "# Verificar que son iguales\n",
    "predicciones_loop = np.array(predicciones_loop)\n",
    "son_iguales = np.allclose(predicciones_loop, predicciones_vectorizadas)\n",
    "\n",
    "print(f\"Tiempo con loop:      {tiempo_loop:.4f} segundos\")\n",
    "print(f\"Tiempo vectorizado:   {tiempo_vectorizado:.4f} segundos\")\n",
    "print(f\"Speedup:              {tiempo_loop/tiempo_vectorizado:.1f}x m√°s r√°pido\")\n",
    "print(f\"Resultados iguales:   {son_iguales}\")\n",
    "\n",
    "print(f\"\\nConclusi√≥n: La vectorizaci√≥n es {tiempo_loop/tiempo_vectorizado:.0f}x m√°s r√°pida\")\n",
    "print(\"Por eso siempre usaremos NumPy en machine learning\")\n",
    "\n",
    "# Limpiar memoria\n",
    "del X_grande, predicciones_loop, predicciones_vectorizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Funci√≥n de Costo para M√∫ltiples Variables\n",
    "\n",
    "La funci√≥n de costo se extiende naturalmente:\n",
    "\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Donde $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementaci√≥n de funci√≥n de costo multivariable\n",
    "\n",
    "def calcular_costo_multivariable(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Calcula la funci√≥n de costo para regresi√≥n lineal multivariable\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Matriz de features (m, n)\n",
    "        y (ndarray): Vector de targets (m,)\n",
    "        w (ndarray): Vector de pesos (n,)\n",
    "        b (scalar): Bias\n",
    "        \n",
    "    Returns:\n",
    "        costo (scalar): Valor de la funci√≥n de costo\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    costo = 0.0\n",
    "    \n",
    "    for i in range(m):\n",
    "        # Predicci√≥n para la casa i usando producto punto\n",
    "        f_wb_i = np.dot(X[i], w) + b\n",
    "        \n",
    "        # Error cuadr√°tico\n",
    "        costo += (f_wb_i - y[i]) ** 2\n",
    "    \n",
    "    costo = costo / (2 * m)\n",
    "    return costo\n",
    "\n",
    "# Probar funci√≥n de costo\n",
    "costo_inicial = calcular_costo_multivariable(X_train, y_train, w_inicial, b_inicial)\n",
    "print(f\"Costo con par√°metros iniciales: {costo_inicial:.3f}\")\n",
    "\n",
    "# Probar con diferentes par√°metros\n",
    "parametros_prueba = [\n",
    "    (np.zeros(4), 0, \"Ceros\"),\n",
    "    (np.ones(4), 0, \"Unos\"),\n",
    "    (w_inicial, b_inicial, \"Inicial (cerca del √≥ptimo)\"),\n",
    "    (np.array([1, 0, 0, 0]), 100, \"Solo tama√±o\")\n",
    "]\n",
    "\n",
    "print(f\"\\nComparaci√≥n de diferentes par√°metros:\")\n",
    "print(f\"{'Descripci√≥n':<25} {'Costo':<10}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for w_test, b_test, desc in parametros_prueba:\n",
    "    costo = calcular_costo_multivariable(X_train, y_train, w_test, b_test)\n",
    "    print(f\"{desc:<25} {costo:<10.3f}\")\n",
    "\n",
    "print(f\"\\nMenor costo indica mejor ajuste a los datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent para M√∫ltiples Variables\n",
    "\n",
    "### 4.1 Gradientes (Derivadas Parciales)\n",
    "\n",
    "Para m√∫ltiples variables, necesitamos calcular gradientes para cada par√°metro:\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m} \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$$\n",
    "\n",
    "**Algoritmo de actualizaci√≥n**:\n",
    "$$w_j = w_j - \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$$\n",
    "$$b = b - \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementaci√≥n de gradientes multivariables\n",
    "\n",
    "def calcular_gradientes_multivariable(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Calcula gradientes para regresi√≥n lineal multivariable\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Matriz de features (m, n)\n",
    "        y (ndarray): Vector de targets (m,)\n",
    "        w (ndarray): Vector de pesos (n,)\n",
    "        b (scalar): Bias\n",
    "        \n",
    "    Returns:\n",
    "        dj_dw (ndarray): Gradientes respecto a w (n,)\n",
    "        dj_db (scalar): Gradiente respecto a b\n",
    "    \"\"\"\n",
    "    m, n = X.shape  # m ejemplos, n features\n",
    "    \n",
    "    # Inicializar gradientes\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "    \n",
    "    for i in range(m):\n",
    "        # Error para el ejemplo i\n",
    "        error = (np.dot(X[i], w) + b) - y[i]\n",
    "        \n",
    "        # Gradientes para cada w_j\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += error * X[i, j]\n",
    "        \n",
    "        # Gradiente para b\n",
    "        dj_db += error\n",
    "    \n",
    "    # Promediar gradientes\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    \n",
    "    return dj_dw, dj_db\n",
    "\n",
    "# Probar c√°lculo de gradientes\n",
    "dj_dw_test, dj_db_test = calcular_gradientes_multivariable(X_train, y_train, w_inicial, b_inicial)\n",
    "\n",
    "print(f\"Gradientes con par√°metros iniciales:\")\n",
    "print(f\"dJ/db: {dj_db_test:.6f}\")\n",
    "print(f\"dJ/dw: {dj_dw_test}\")\n",
    "\n",
    "print(f\"\\nInterpretaci√≥n de gradientes:\")\n",
    "for i, (feature, grad) in enumerate(zip(feature_names, dj_dw_test)):\n",
    "    direccion = \"aumentar\" if grad < 0 else \"disminuir\"\n",
    "    print(f\"  {feature}: {grad:.6f} ‚Üí {direccion} w[{i}]\")\n",
    "\n",
    "direccion_b = \"aumentar\" if dj_db_test < 0 else \"disminuir\"\n",
    "print(f\"  Bias: {dj_db_test:.6f} ‚Üí {direccion_b} b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Implementaci√≥n Completa de Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent completo para m√∫ltiples variables\n",
    "\n",
    "def gradient_descent_multivariable(X, y, w_inicial, b_inicial, \n",
    "                                  calcular_costo_func, calcular_gradientes_func,\n",
    "                                  alpha, num_iteraciones):\n",
    "    \"\"\"\n",
    "    Implementa gradient descent para regresi√≥n lineal multivariable\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Matriz de features (m, n)\n",
    "        y (ndarray): Vector de targets (m,)\n",
    "        w_inicial (ndarray): Valores iniciales de w (n,)\n",
    "        b_inicial (scalar): Valor inicial de b\n",
    "        calcular_costo_func: Funci√≥n para calcular costo\n",
    "        calcular_gradientes_func: Funci√≥n para calcular gradientes\n",
    "        alpha (float): Learning rate\n",
    "        num_iteraciones (int): N√∫mero de iteraciones\n",
    "        \n",
    "    Returns:\n",
    "        w (ndarray): Par√°metros w optimizados\n",
    "        b (scalar): Par√°metro b optimizado\n",
    "        historial_costos (list): Historia del costo\n",
    "    \"\"\"\n",
    "    # Inicializar par√°metros (evitar modificar originales)\n",
    "    w = copy.deepcopy(w_inicial)\n",
    "    b = b_inicial\n",
    "    \n",
    "    # Historia para tracking\n",
    "    historial_costos = []\n",
    "    \n",
    "    for i in range(num_iteraciones):\n",
    "        # Calcular gradientes\n",
    "        dj_dw, dj_db = calcular_gradientes_func(X, y, w, b)\n",
    "        \n",
    "        # Actualizar par√°metros\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        # Guardar costo\n",
    "        if i < 100000:  # Evitar usar mucha memoria\n",
    "            costo = calcular_costo_func(X, y, w, b)\n",
    "            historial_costos.append(costo)\n",
    "        \n",
    "        # Imprimir progreso\n",
    "        if i % math.ceil(num_iteraciones / 10) == 0:\n",
    "            costo_actual = calcular_costo_func(X, y, w, b)\n",
    "            print(f\"Iteraci√≥n {i:4d}: Costo = {costo_actual:8.2f}\")\n",
    "    \n",
    "    return w, b, historial_costos\n",
    "\n",
    "# Ejecutar gradient descent\n",
    "print(\"Entrenando modelo de regresi√≥n multivariable...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Par√°metros de entrenamiento\n",
    "w_inicial = np.zeros(4)  # Empezar desde cero\n",
    "b_inicial = 0.\n",
    "learning_rate = 5.0e-7  # Learning rate peque√±o para datos no normalizados\n",
    "iteraciones = 1000\n",
    "\n",
    "# Entrenar modelo\n",
    "w_final, b_final, hist_costos = gradient_descent_multivariable(\n",
    "    X_train, y_train, w_inicial, b_inicial,\n",
    "    calcular_costo_multivariable, calcular_gradientes_multivariable,\n",
    "    learning_rate, iteraciones\n",
    ")\n",
    "\n",
    "print(f\"\\nEntrenamiento completado!\")\n",
    "print(f\"Par√°metros finales:\")\n",
    "for i, (feature, peso) in enumerate(zip(feature_names, w_final)):\n",
    "    print(f\"  w[{i}] ({feature}): {peso:.4f}\")\n",
    "print(f\"  b (bias): {b_final:.4f}\")\n",
    "print(f\"Costo final: {hist_costos[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualizaci√≥n del Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar proceso de entrenamiento\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Evoluci√≥n del costo\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist_costos, 'b-', linewidth=2)\n",
    "plt.title('Evoluci√≥n del Costo Durante el Entrenamiento')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo J(w,b)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Marcar costo final\n",
    "plt.plot(len(hist_costos)-1, hist_costos[-1], 'ro', markersize=8)\n",
    "plt.annotate(f'Final: {hist_costos[-1]:.1f}', \n",
    "             xy=(len(hist_costos)-1, hist_costos[-1]),\n",
    "             xytext=(len(hist_costos)*0.6, hist_costos[-1]*1.2),\n",
    "             arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "# Subplot 2: Par√°metros finales\n",
    "plt.subplot(1, 2, 2)\n",
    "colores = ['skyblue', 'lightgreen', 'lightcoral', 'lightsalmon']\n",
    "barras = plt.bar(range(len(feature_names)), w_final, color=colores, alpha=0.7, edgecolor='black')\n",
    "plt.title('Pesos Finales por Feature')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Peso w_j')\n",
    "plt.xticks(range(len(feature_names)), [f.replace(' ', '\\n').replace('(', '\\n(') for f in feature_names])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# A√±adir valores en las barras\n",
    "for bar, peso in zip(barras, w_final):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + (0.01 if height >= 0 else -0.03),\n",
    "             f'{peso:.3f}', ha='center', va='bottom' if height >= 0 else 'top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"An√°lisis del entrenamiento:\")\n",
    "print(f\"  ‚Ä¢ El costo disminuy√≥ de {hist_costos[0]:.1f} a {hist_costos[-1]:.1f}\")\n",
    "print(f\"  ‚Ä¢ Reducci√≥n: {((hist_costos[0] - hist_costos[-1])/hist_costos[0]*100):.1f}%\")\n",
    "\n",
    "print(f\"\\nInterpretaci√≥n de pesos:\")\n",
    "interpretaciones = [\n",
    "    \"Cada pie cuadrado adicional aumenta el precio\",\n",
    "    \"Cada habitaci√≥n adicional aumenta el precio\", \n",
    "    \"Pisos adicionales tienen impacto negativo (¬øsorprendente?)\",\n",
    "    \"Cada a√±o de edad reduce el precio (casas nuevas valen m√°s)\"\n",
    "]\n",
    "\n",
    "for feature, peso, interp in zip(feature_names, w_final, interpretaciones):\n",
    "    signo = \"‚ÜóÔ∏è\" if peso > 0 else \"‚ÜòÔ∏è\"\n",
    "    print(f\"  {signo} {feature}: {interp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluaci√≥n y Predicciones del Modelo\n",
    "\n",
    "### 5.1 Predicciones en el Conjunto de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer predicciones para todas las casas del entrenamiento\n",
    "print(f\"Predicciones vs Valores Reales:\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"{'Casa':<6} {'Tama√±o':<8} {'Habitac.':<9} {'Pisos':<6} {'Edad':<5} {'Pred.':<8} {'Real':<8} {'Error':<8}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "errores_absolutos = []\n",
    "for i in range(len(X_train)):\n",
    "    # Predicci√≥n\n",
    "    prediccion = np.dot(X_train[i], w_final) + b_final\n",
    "    error = prediccion - y_train[i]\n",
    "    error_abs = abs(error)\n",
    "    errores_absolutos.append(error_abs)\n",
    "    \n",
    "    print(f\"Casa {i+1:<2d} {X_train[i,0]:<8.0f} {X_train[i,1]:<9.0f} {X_train[i,2]:<6.0f} \"\n",
    "          f\"{X_train[i,3]:<5.0f} ${prediccion:<7.0f} ${y_train[i]:<7.0f} ${error:<+7.0f}\")\n",
    "\n",
    "# Estad√≠sticas de error\n",
    "mae = np.mean(errores_absolutos)  # Error Absoluto Medio\n",
    "rmse = np.sqrt(np.mean([e**2 for e in [pred - real for pred, real in \n",
    "                       zip([np.dot(X_train[i], w_final) + b_final for i in range(len(X_train))], y_train)]]))\n",
    "\n",
    "print(f\"\\nEstad√≠sticas de error:\")\n",
    "print(f\"  Error Absoluto Medio (MAE): ${mae:.1f}k\")\n",
    "print(f\"  Ra√≠z Error Cuadr√°tico Medio (RMSE): ${rmse:.1f}k\")\n",
    "print(f\"  Error m√°ximo: ${max(errores_absolutos):.1f}k\")\n",
    "\n",
    "# Calcular R¬≤\n",
    "predicciones_todas = [np.dot(X_train[i], w_final) + b_final for i in range(len(X_train))]\n",
    "ss_res = sum([(pred - real)**2 for pred, real in zip(predicciones_todas, y_train)])\n",
    "ss_tot = sum([(real - np.mean(y_train))**2 for real in y_train])\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"  Coeficiente R¬≤: {r2:.3f} ({r2*100:.1f}% de varianza explicada)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Predicciones para Casas Nuevas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones para casas hipot√©ticas\n",
    "casas_nuevas = [\n",
    "    ([1200, 3, 1, 15], \"Casa familiar t√≠pica\"),\n",
    "    ([2500, 4, 2, 5], \"Casa grande y nueva\"),\n",
    "    ([800, 2, 1, 30], \"Casa peque√±a, algo vieja\"),\n",
    "    ([3500, 5, 2, 2], \"Mansi√≥n nueva\"),\n",
    "    ([1800, 3, 2, 20], \"Casa mediana\"),\n",
    "]\n",
    "\n",
    "print(f\"Predicciones para Casas Nuevas:\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"{'Descripci√≥n':<20} {'Features':<20} {'Precio Predicho':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for features, descripcion in casas_nuevas:\n",
    "    x_nueva = np.array(features)\n",
    "    precio_predicho = np.dot(x_nueva, w_final) + b_final\n",
    "    \n",
    "    features_str = f\"{features[0]}sqft,{features[1]}bed,{features[2]}fl,{features[3]}yrs\"\n",
    "    print(f\"{descripcion:<20} {features_str:<20} ${precio_predicho:<14,.0f}\")\n",
    "\n",
    "print(f\"\\nAn√°lisis detallado de una predicci√≥n:\")\n",
    "x_ejemplo = np.array([1200, 3, 1, 15])\n",
    "precio = np.dot(x_ejemplo, w_final) + b_final\n",
    "\n",
    "print(f\"Casa: 1200 sqft, 3 habitaciones, 1 piso, 15 a√±os\")\n",
    "print(f\"Desglose del c√°lculo:\")\n",
    "total_features = 0\n",
    "for i, (feature, peso, valor) in enumerate(zip(feature_names, w_final, x_ejemplo)):\n",
    "    contribucion = peso * valor\n",
    "    total_features += contribucion\n",
    "    print(f\"  {feature}: {peso:.4f} √ó {valor} = ${contribucion:+.0f}\")\n",
    "    \n",
    "print(f\"  Bias: {b_final:+.0f}\")\n",
    "print(f\"  Total: ${precio:.0f}\")\n",
    "\n",
    "# An√°lisis de sensibilidad\n",
    "print(f\"\\nAn√°lisis de sensibilidad (cambio en precio por unidad):\")\n",
    "cambios_unidad = [1, 1, 1, 1]  # 1 sqft, 1 habitaci√≥n, 1 piso, 1 a√±o\n",
    "for feature, peso, cambio in zip(feature_names, w_final, cambios_unidad):\n",
    "    impacto = peso * cambio\n",
    "    print(f\"  +{cambio} {feature}: ${impacto:+.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualizaci√≥n de Predicciones vs Realidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaciones de evaluaci√≥n\n",
    "predicciones_entrenamiento = [np.dot(X_train[i], w_final) + b_final for i in range(len(X_train))]\n",
    "residuos = [pred - real for pred, real in zip(predicciones_entrenamiento, y_train)]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: Predicciones vs Valores Reales\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(predicciones_entrenamiento, y_train, alpha=0.7, s=50)\n",
    "\n",
    "# L√≠nea diagonal perfecta\n",
    "min_val = min(min(predicciones_entrenamiento), min(y_train))\n",
    "max_val = max(max(predicciones_entrenamiento), max(y_train))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, \n",
    "         label='Predicci√≥n perfecta')\n",
    "\n",
    "plt.title('Predicciones vs Valores Reales')\n",
    "plt.xlabel('Predicciones ($k)')\n",
    "plt.ylabel('Valores Reales ($k)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Residuos\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(predicciones_entrenamiento, residuos, alpha=0.7, s=50)\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Residuos vs Predicciones')\n",
    "plt.xlabel('Predicciones ($k)')\n",
    "plt.ylabel('Residuos ($k)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Histograma de residuos\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(residuos, bins=8, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Distribuci√≥n de Residuos')\n",
    "plt.xlabel('Residuos ($k)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"An√°lisis de residuos:\")\n",
    "print(f\"  Media de residuos: ${np.mean(residuos):.1f}k (deber√≠a ser ~0)\")\n",
    "print(f\"  Desviaci√≥n est√°ndar: ${np.std(residuos):.1f}k\")\n",
    "print(f\"  Residuo m√°ximo (absoluto): ${max([abs(r) for r in residuos]):.1f}k\")\n",
    "\n",
    "# An√°lisis de features m√°s importantes\n",
    "importancia_relativa = [abs(w) for w in w_final]\n",
    "feature_mas_importante = feature_names[np.argmax(importancia_relativa)]\n",
    "print(f\"\\nFeature m√°s importante: {feature_mas_importante}\")\n",
    "print(f\"Ranking de importancia (por magnitud de peso):\")\n",
    "indices_ordenados = np.argsort(importancia_relativa)[::-1]\n",
    "for i, idx in enumerate(indices_ordenados):\n",
    "    print(f\"  {i+1}. {feature_names[idx]}: |{w_final[idx]:.4f}|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: Analizar el Impacto de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 1: Analizar qu√© pasar√≠a si modificamos una feature\n",
    "\n",
    "def analizar_impacto_feature(casa_base, indice_feature, cambios, w, b):\n",
    "    \"\"\"\n",
    "    Analiza c√≥mo cambia el precio al modificar una feature espec√≠fica\n",
    "    \n",
    "    Args:\n",
    "        casa_base: Features de la casa base\n",
    "        indice_feature: √çndice de la feature a modificar\n",
    "        cambios: Lista de cambios a aplicar\n",
    "        w, b: Par√°metros del modelo\n",
    "    \"\"\"\n",
    "    casa_base = np.array(casa_base)\n",
    "    precio_base = np.dot(casa_base, w) + b\n",
    "    \n",
    "    print(f\"An√°lisis de impacto: {feature_names[indice_feature]}\")\n",
    "    print(f\"Casa base: {casa_base} ‚Üí ${precio_base:.0f}k\")\n",
    "    print(f\"\\n{'Cambio':<10} {'Nueva Feature':<15} {'Nuevo Precio':<12} {'Diferencia':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for cambio in cambios:\n",
    "        casa_modificada = casa_base.copy()\n",
    "        casa_modificada[indice_feature] += cambio\n",
    "        precio_nuevo = np.dot(casa_modificada, w) + b\n",
    "        diferencia = precio_nuevo - precio_base\n",
    "        \n",
    "        print(f\"{cambio:+<10} {casa_modificada[indice_feature]:<15.0f} \"\n",
    "              f\"${precio_nuevo:<11.0f} ${diferencia:+<11.0f}\")\n",
    "\n",
    "# Casa ejemplo: casa mediana\n",
    "casa_ejemplo = [1500, 3, 1, 20]  # 1500 sqft, 3 habitaciones, 1 piso, 20 a√±os\n",
    "\n",
    "print(\"Ejercicio 1: An√°lisis de Impacto de Features\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analizar impacto del tama√±o\n",
    "cambios_tama√±o = [-500, -200, 200, 500, 1000]\n",
    "analizar_impacto_feature(casa_ejemplo, 0, cambios_tama√±o, w_final, b_final)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Analizar impacto de habitaciones\n",
    "cambios_habitaciones = [-1, 1, 2]\n",
    "analizar_impacto_feature(casa_ejemplo, 1, cambios_habitaciones, w_final, b_final)\n",
    "\n",
    "print(f\"\\nConclusi√≥n: El tama√±o tiene el mayor impacto en el precio\")\n",
    "print(f\"Cada 100 sqft adicionales ‚Üí ${w_final[0]*100:+.0f}k en precio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Comparar Modelos con Diferentes Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 2: Entrenar modelos usando solo subconjuntos de features\n",
    "\n",
    "def entrenar_modelo_subset(X, y, indices_features, nombre_modelo):\n",
    "    \"\"\"\n",
    "    Entrena modelo usando solo un subconjunto de features\n",
    "    \"\"\"\n",
    "    X_subset = X[:, indices_features]\n",
    "    n_features = X_subset.shape[1]\n",
    "    \n",
    "    # Par√°metros iniciales\n",
    "    w_init = np.zeros(n_features)\n",
    "    b_init = 0.\n",
    "    \n",
    "    # Entrenar (versi√≥n simplificada, menos iteraciones)\n",
    "    w, b, _ = gradient_descent_multivariable(\n",
    "        X_subset, y, w_init, b_init,\n",
    "        calcular_costo_multivariable, calcular_gradientes_multivariable,\n",
    "        5.0e-7, 300  # Menos iteraciones para el ejercicio\n",
    "    )\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    predicciones = [np.dot(X_subset[i], w) + b for i in range(len(X_subset))]\n",
    "    mse = np.mean([(pred - real)**2 for pred, real in zip(predicciones, y)])\n",
    "    mae = np.mean([abs(pred - real) for pred, real in zip(predicciones, y)])\n",
    "    \n",
    "    return {\n",
    "        'nombre': nombre_modelo,\n",
    "        'features_usadas': [feature_names[i] for i in indices_features],\n",
    "        'w': w,\n",
    "        'b': b,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'predicciones': predicciones\n",
    "    }\n",
    "\n",
    "print(\"Ejercicio 2: Comparaci√≥n de Modelos con Diferentes Features\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Definir diferentes combinaciones de features\n",
    "modelos_a_probar = [\n",
    "    ([0], \"Solo Tama√±o\"),\n",
    "    ([0, 1], \"Tama√±o + Habitaciones\"),\n",
    "    ([0, 3], \"Tama√±o + Edad\"),\n",
    "    ([0, 1, 3], \"Tama√±o + Habitaciones + Edad\"),\n",
    "    ([0, 1, 2, 3], \"Todas las Features\")\n",
    "]\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for indices, nombre in modelos_a_probar:\n",
    "    print(f\"\\nEntrenando: {nombre}\")\n",
    "    resultado = entrenar_modelo_subset(X_train, y_train, indices, nombre)\n",
    "    resultados.append(resultado)\n",
    "\n",
    "# Comparar resultados\n",
    "print(f\"\\n\\nComparaci√≥n de Modelos:\")\n",
    "print(f\"{'Modelo':<25} {'MSE':<8} {'MAE':<8} {'Features':<30}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for resultado in resultados:\n",
    "    features_str = \", \".join([f.split()[0] for f in resultado['features_usadas']])\n",
    "    print(f\"{resultado['nombre']:<25} {resultado['mse']:<8.1f} {resultado['mae']:<8.1f} {features_str:<30}\")\n",
    "\n",
    "# Encontrar el mejor modelo\n",
    "mejor_modelo = min(resultados, key=lambda x: x['mse'])\n",
    "print(f\"\\nMejor modelo por MSE: {mejor_modelo['nombre']}\")\n",
    "print(f\"Features: {', '.join(mejor_modelo['features_usadas'])}\")\n",
    "\n",
    "# An√°lisis de complejidad vs rendimiento\n",
    "print(f\"\\nAn√°lisis:\")\n",
    "print(f\"  ‚Ä¢ Usar solo tama√±o da resultados razonables\")\n",
    "print(f\"  ‚Ä¢ A√±adir m√°s features mejora el modelo hasta cierto punto\")\n",
    "print(f\"  ‚Ä¢ El modelo completo tiene el mejor rendimiento\")\n",
    "print(f\"  ‚Ä¢ Cada feature adicional aporta informaci√≥n valiosa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen y Conceptos Clave\n",
    "\n",
    "### ‚úÖ Lo que has aprendido:\n",
    "\n",
    "#### 1. **Extensi√≥n a M√∫ltiples Variables**:\n",
    "   - **Modelo**: $f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b$\n",
    "   - **Vectorizaci√≥n**: Uso eficiente de NumPy para operaciones matriciales\n",
    "   - **M√∫ltiples features**: Tama√±o, habitaciones, pisos, edad\n",
    "\n",
    "#### 2. **Implementaci√≥n Vectorizada**:\n",
    "   - **Predicci√≥n**: `np.dot(x, w) + b`\n",
    "   - **Speedup**: 100x-1000x m√°s r√°pido que loops\n",
    "   - **Escalabilidad**: Maneja datasets grandes eficientemente\n",
    "\n",
    "#### 3. **Funci√≥n de Costo Multivariable**:\n",
    "   - **F√≥rmula**: Misma estructura, pero con vectores\n",
    "   - **Implementaci√≥n**: Loop sobre ejemplos, producto punto por predicci√≥n\n",
    "   - **Interpretaci√≥n**: Mide ajuste global del modelo\n",
    "\n",
    "#### 4. **Gradient Descent Multivariable**:\n",
    "   - **Gradientes**: Un gradiente por cada par√°metro w_j\n",
    "   - **Actualizaci√≥n simult√°nea**: Todos los par√°metros se actualizan juntos\n",
    "   - **Convergencia**: Encuentra m√≠nimo global en funci√≥n convexa\n",
    "\n",
    "#### 5. **An√°lisis de Features**:\n",
    "   - **Importancia relativa**: Magnitud de pesos indica importancia\n",
    "   - **Interpretaci√≥n**: Cada peso representa impacto marginal\n",
    "   - **Sensibilidad**: An√°lisis de cambios en features individuales\n",
    "\n",
    "### üè† Aplicaci√≥n Pr√°ctica - Precios de Casas:\n",
    "- **Modelo final**: 4 features ‚Üí predicci√≥n de precio\n",
    "- **Rendimiento**: R¬≤ = {r2:.3f} (explicaci√≥n de varianza)\n",
    "- **Feature m√°s importante**: Tama√±o de la casa\n",
    "- **Aplicaci√≥n**: Valoraci√≥n autom√°tica de propiedades\n",
    "\n",
    "### üöÄ Pr√≥ximos pasos:\n",
    "- **Feature Scaling**: Normalizaci√≥n para mejor convergencia\n",
    "- **Learning Rate**: Optimizaci√≥n de hiperpar√°metros\n",
    "- **Regularizaci√≥n**: Control de overfitting\n",
    "- **Feature Engineering**: Creaci√≥n de features m√°s informativas\n",
    "\n",
    "### üí° Puntos clave para recordar:\n",
    "1. **Vectorizaci√≥n es esencial** para eficiencia en ML\n",
    "2. **M√°s features ‚â† siempre mejor modelo** (curse of dimensionality)\n",
    "3. **Interpretabilidad**: Los pesos tienen significado business\n",
    "4. **Evaluaci√≥n m√∫ltiple**: Usar varias m√©tricas (MSE, MAE, R¬≤)\n",
    "5. **An√°lisis de residuos**: Validar supuestos del modelo\n",
    "\n",
    "### ‚ö†Ô∏è Limitaciones observadas:\n",
    "- **Learning rate peque√±o**: Datos no normalizados requieren Œ± muy bajo\n",
    "- **Convergencia lenta**: Diferentes escalas de features afectan velocidad\n",
    "- **Interpretaci√≥n cuidadosa**: Peso negativo de \"pisos\" requiere an√°lisis\n",
    "\n",
    "**Siguiente paso**: Aprenderemos t√©cnicas de feature scaling para resolver estos problemas y mejorar significativamente el entrenamiento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}