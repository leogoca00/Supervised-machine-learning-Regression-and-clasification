{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal con Múltiples Variables\n",
    "\n",
    "## Objetivos de Aprendizaje\n",
    "En este notebook aprenderás:\n",
    "- Extender regresión lineal a múltiples features\n",
    "- Implementar vectorización con NumPy para eficiencia\n",
    "- Desarrollar gradient descent para múltiples variables\n",
    "- Aplicar el modelo a predicción de precios de casas\n",
    "\n",
    "## Problema de Negocio\n",
    "**Escenario**: Eres un agente inmobiliario que necesita estimar precios de casas basándose en múltiples características:\n",
    "- **Tamaño** de la casa (pies cuadrados)\n",
    "- **Número de habitaciones**\n",
    "- **Número de pisos**\n",
    "- **Edad** de la casa (años)\n",
    "\n",
    "**Objetivo**: Crear un modelo que prediga el precio usando todas estas características simultáneamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "\n",
    "# Configuración\n",
    "plt.style.use('default')\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(\"Librerías importadas correctamente\")\n",
    "print(\"Listo para regresión multivariable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset: Precios de Casas\n",
    "\n",
    "### 1.1 Cargar y Explorar los Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cargar datos de casas\n",
    "def cargar_datos_casas():\n",
    "    \"\"\"\n",
    "    Carga dataset de precios de casas con múltiples features\n",
    "    \n",
    "    Returns:\n",
    "        X_train: Matriz de features (m, n)\n",
    "        y_train: Vector de precios (m,)\n",
    "        feature_names: Nombres de las características\n",
    "    \"\"\"\n",
    "    # Dataset real de casas\n",
    "    # Cada fila: [tamaño_sqft, habitaciones, pisos, edad]\n",
    "    X_train = np.array([\n",
    "        [2104, 5, 1, 45],    # Casa 1\n",
    "        [1416, 3, 2, 40],    # Casa 2  \n",
    "        [852,  2, 1, 35],    # Casa 3\n",
    "        [1940, 4, 1, 10],    # Casa 4\n",
    "        [1539, 3, 2, 25],    # Casa 5\n",
    "        [3000, 4, 2, 8],     # Casa 6\n",
    "        [1230, 3, 1, 15],    # Casa 7\n",
    "        [2145, 4, 1, 12],    # Casa 8\n",
    "        [1736, 3, 2, 30],    # Casa 9\n",
    "        [1000, 2, 1, 50]     # Casa 10\n",
    "    ])\n",
    "    \n",
    "    # Precios correspondientes (en miles de dólares)\n",
    "    y_train = np.array([460, 232, 178, 500, 315, 740, 285, 510, 390, 180])\n",
    "    \n",
    "    # Nombres de features\n",
    "    feature_names = ['Tamaño (sqft)', 'Habitaciones', 'Pisos', 'Edad (años)']\n",
    "    \n",
    "    return X_train, y_train, feature_names\n",
    "\n",
    "# Cargar datos\n",
    "X_train, y_train, feature_names = cargar_datos_casas()\n",
    "\n",
    "print(f\"Dataset de casas cargado:\")\n",
    "print(f\"  • Número de casas (m): {X_train.shape[0]}\")\n",
    "print(f\"  • Número de features (n): {X_train.shape[1]}\")\n",
    "print(f\"  • Shape de X_train: {X_train.shape}\")\n",
    "print(f\"  • Shape de y_train: {y_train.shape}\")\n",
    "\n",
    "print(f\"\\nPrimeras 5 casas:\")\n",
    "print(f\"{'':>5} {'Tamaño':>8} {'Habitaciones':>12} {'Pisos':>6} {'Edad':>5} {'Precio':>8}\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(5):\n",
    "    print(f\"Casa {i+1:1d}: {X_train[i,0]:>6.0f} {X_train[i,1]:>10.0f} {X_train[i,2]:>8.0f} {X_train[i,3]:>6.0f} ${y_train[i]:>6.0f}k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualización del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar cada feature vs precio\n",
    "plt.figure(figsize=(16, 4))\n",
    "\n",
    "for i in range(len(feature_names)):\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    plt.scatter(X_train[:, i], y_train, alpha=0.7, s=50)\n",
    "    plt.xlabel(feature_names[i])\n",
    "    plt.ylabel('Precio (miles $)' if i == 0 else '')\n",
    "    plt.title(f'{feature_names[i]} vs Precio')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Observaciones:\")\n",
    "print(\"  • Tamaño: Relación positiva clara - casas más grandes cuestan más\")\n",
    "print(\"  • Habitaciones: Tendencia positiva - más habitaciones, mayor precio\")\n",
    "print(\"  • Pisos: Relación menos clara, pero casas de 2 pisos tienden a costar más\")\n",
    "print(\"  • Edad: Relación negativa - casas más nuevas cuestan más\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modelo de Regresión Multivariable\n",
    "\n",
    "### 2.1 Extensión del Modelo Lineal\n",
    "\n",
    "**Modelo con una variable**: $f_{w,b}(x) = wx + b$\n",
    "\n",
    "**Modelo con múltiples variables**: \n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}) = w_0x_0 + w_1x_1 + w_2x_2 + w_3x_3 + b$$\n",
    "\n",
    "**En notación vectorial**:\n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b$$\n",
    "\n",
    "Donde:\n",
    "- **x**: Vector de features [tamaño, habitaciones, pisos, edad]\n",
    "- **w**: Vector de pesos [w₀, w₁, w₂, w₃]\n",
    "- **b**: Término de sesgo (bias)\n",
    "- **·**: Producto punto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de predicción multivariable\n",
    "\n",
    "def predecir_una_casa_loop(x, w, b):\n",
    "    \"\"\"\n",
    "    Predicción para una casa usando loop (versión educativa)\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Vector de features para una casa (n,)\n",
    "        w (ndarray): Vector de pesos (n,)\n",
    "        b (scalar): Bias\n",
    "    \n",
    "    Returns:\n",
    "        prediction (scalar): Precio predicho\n",
    "    \"\"\"\n",
    "    n = x.shape[0]\n",
    "    prediccion = 0\n",
    "    \n",
    "    # Sumar cada w_i * x_i\n",
    "    for i in range(n):\n",
    "        prediccion += w[i] * x[i]\n",
    "    \n",
    "    # Añadir bias\n",
    "    prediccion += b\n",
    "    \n",
    "    return prediccion\n",
    "\n",
    "def predecir_una_casa_vectorizada(x, w, b):\n",
    "    \"\"\"\n",
    "    Predicción vectorizada (versión eficiente)\n",
    "    \n",
    "    Args:\n",
    "        x (ndarray): Vector de features (n,)\n",
    "        w (ndarray): Vector de pesos (n,)\n",
    "        b (scalar): Bias\n",
    "        \n",
    "    Returns:\n",
    "        prediction (scalar): Precio predicho\n",
    "    \"\"\"\n",
    "    return np.dot(x, w) + b\n",
    "\n",
    "# Probar ambas implementaciones\n",
    "# Parámetros ejemplo (valores cercanos a óptimos)\n",
    "w_inicial = np.array([0.39, 18.75, -53.36, -26.42])\n",
    "b_inicial = 785.18\n",
    "\n",
    "# Tomar primera casa como ejemplo\n",
    "x_ejemplo = X_train[0]  # [2104, 5, 1, 45]\n",
    "precio_real = y_train[0]  # 460\n",
    "\n",
    "print(f\"Casa ejemplo: {x_ejemplo}\")\n",
    "print(f\"Precio real: ${precio_real}k\")\n",
    "print(f\"\\nPredicciones:\")\n",
    "\n",
    "# Predicción con loop\n",
    "pred_loop = predecir_una_casa_loop(x_ejemplo, w_inicial, b_inicial)\n",
    "print(f\"  Con loop: ${pred_loop:.1f}k\")\n",
    "\n",
    "# Predicción vectorizada\n",
    "pred_vectorizada = predecir_una_casa_vectorizada(x_ejemplo, w_inicial, b_inicial)\n",
    "print(f\"  Vectorizada: ${pred_vectorizada:.1f}k\")\n",
    "print(f\"  ¿Son iguales? {np.isclose(pred_loop, pred_vectorizada)}\")\n",
    "\n",
    "print(f\"\\nDesglose de la predicción vectorizada:\")\n",
    "for i, (feature, peso, valor) in enumerate(zip(feature_names, w_inicial, x_ejemplo)):\n",
    "    contribucion = peso * valor\n",
    "    print(f\"  {feature}: {peso:.2f} × {valor} = {contribucion:.1f}\")\n",
    "print(f\"  Bias: {b_inicial:.1f}\")\n",
    "print(f\"  Total: {pred_vectorizada:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Comparación de Velocidad: Loop vs Vectorización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Crear dataset grande para comparar velocidad\n",
    "np.random.seed(42)\n",
    "n_casas_grandes = 100000\n",
    "X_grande = np.random.rand(n_casas_grandes, 4) * 1000  # Features aleatorias\n",
    "w_test = np.array([0.5, 10, -20, -15])\n",
    "b_test = 100\n",
    "\n",
    "print(f\"Comparación de velocidad con {n_casas_grandes:,} casas:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Método con loop\n",
    "start_time = time.time()\n",
    "predicciones_loop = []\n",
    "for i in range(n_casas_grandes):\n",
    "    pred = predecir_una_casa_loop(X_grande[i], w_test, b_test)\n",
    "    predicciones_loop.append(pred)\n",
    "tiempo_loop = time.time() - start_time\n",
    "\n",
    "# Método vectorizado\n",
    "start_time = time.time()\n",
    "predicciones_vectorizadas = X_grande @ w_test + b_test  # @ es equivalente a np.dot\n",
    "tiempo_vectorizado = time.time() - start_time\n",
    "\n",
    "# Verificar que son iguales\n",
    "predicciones_loop = np.array(predicciones_loop)\n",
    "son_iguales = np.allclose(predicciones_loop, predicciones_vectorizadas)\n",
    "\n",
    "print(f\"Tiempo con loop:      {tiempo_loop:.4f} segundos\")\n",
    "print(f\"Tiempo vectorizado:   {tiempo_vectorizado:.4f} segundos\")\n",
    "print(f\"Speedup:              {tiempo_loop/tiempo_vectorizado:.1f}x más rápido\")\n",
    "print(f\"Resultados iguales:   {son_iguales}\")\n",
    "\n",
    "print(f\"\\nConclusión: La vectorización es {tiempo_loop/tiempo_vectorizado:.0f}x más rápida\")\n",
    "print(\"Por eso siempre usaremos NumPy en machine learning\")\n",
    "\n",
    "# Limpiar memoria\n",
    "del X_grande, predicciones_loop, predicciones_vectorizadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Función de Costo para Múltiples Variables\n",
    "\n",
    "La función de costo se extiende naturalmente:\n",
    "\n",
    "$$J(\\mathbf{w},b) = \\frac{1}{2m} \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Donde $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = \\mathbf{w} \\cdot \\mathbf{x}^{(i)} + b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de función de costo multivariable\n",
    "\n",
    "def calcular_costo_multivariable(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Calcula la función de costo para regresión lineal multivariable\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Matriz de features (m, n)\n",
    "        y (ndarray): Vector de targets (m,)\n",
    "        w (ndarray): Vector de pesos (n,)\n",
    "        b (scalar): Bias\n",
    "        \n",
    "    Returns:\n",
    "        costo (scalar): Valor de la función de costo\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    costo = 0.0\n",
    "    \n",
    "    for i in range(m):\n",
    "        # Predicción para la casa i usando producto punto\n",
    "        f_wb_i = np.dot(X[i], w) + b\n",
    "        \n",
    "        # Error cuadrático\n",
    "        costo += (f_wb_i - y[i]) ** 2\n",
    "    \n",
    "    costo = costo / (2 * m)\n",
    "    return costo\n",
    "\n",
    "# Probar función de costo\n",
    "costo_inicial = calcular_costo_multivariable(X_train, y_train, w_inicial, b_inicial)\n",
    "print(f\"Costo con parámetros iniciales: {costo_inicial:.3f}\")\n",
    "\n",
    "# Probar con diferentes parámetros\n",
    "parametros_prueba = [\n",
    "    (np.zeros(4), 0, \"Ceros\"),\n",
    "    (np.ones(4), 0, \"Unos\"),\n",
    "    (w_inicial, b_inicial, \"Inicial (cerca del óptimo)\"),\n",
    "    (np.array([1, 0, 0, 0]), 100, \"Solo tamaño\")\n",
    "]\n",
    "\n",
    "print(f\"\\nComparación de diferentes parámetros:\")\n",
    "print(f\"{'Descripción':<25} {'Costo':<10}\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "for w_test, b_test, desc in parametros_prueba:\n",
    "    costo = calcular_costo_multivariable(X_train, y_train, w_test, b_test)\n",
    "    print(f\"{desc:<25} {costo:<10.3f}\")\n",
    "\n",
    "print(f\"\\nMenor costo indica mejor ajuste a los datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Gradient Descent para Múltiples Variables\n",
    "\n",
    "### 4.1 Gradientes (Derivadas Parciales)\n",
    "\n",
    "Para múltiples variables, necesitamos calcular gradientes para cada parámetro:\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b} = \\frac{1}{m} \\sum_{i=0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - y^{(i)})$$\n",
    "\n",
    "**Algoritmo de actualización**:\n",
    "$$w_j = w_j - \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}$$\n",
    "$$b = b - \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de gradientes multivariables\n",
    "\n",
    "def calcular_gradientes_multivariable(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Calcula gradientes para regresión lineal multivariable\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Matriz de features (m, n)\n",
    "        y (ndarray): Vector de targets (m,)\n",
    "        w (ndarray): Vector de pesos (n,)\n",
    "        b (scalar): Bias\n",
    "        \n",
    "    Returns:\n",
    "        dj_dw (ndarray): Gradientes respecto a w (n,)\n",
    "        dj_db (scalar): Gradiente respecto a b\n",
    "    \"\"\"\n",
    "    m, n = X.shape  # m ejemplos, n features\n",
    "    \n",
    "    # Inicializar gradientes\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "    \n",
    "    for i in range(m):\n",
    "        # Error para el ejemplo i\n",
    "        error = (np.dot(X[i], w) + b) - y[i]\n",
    "        \n",
    "        # Gradientes para cada w_j\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += error * X[i, j]\n",
    "        \n",
    "        # Gradiente para b\n",
    "        dj_db += error\n",
    "    \n",
    "    # Promediar gradientes\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    \n",
    "    return dj_dw, dj_db\n",
    "\n",
    "# Probar cálculo de gradientes\n",
    "dj_dw_test, dj_db_test = calcular_gradientes_multivariable(X_train, y_train, w_inicial, b_inicial)\n",
    "\n",
    "print(f\"Gradientes con parámetros iniciales:\")\n",
    "print(f\"dJ/db: {dj_db_test:.6f}\")\n",
    "print(f\"dJ/dw: {dj_dw_test}\")\n",
    "\n",
    "print(f\"\\nInterpretación de gradientes:\")\n",
    "for i, (feature, grad) in enumerate(zip(feature_names, dj_dw_test)):\n",
    "    direccion = \"aumentar\" if grad < 0 else \"disminuir\"\n",
    "    print(f\"  {feature}: {grad:.6f} → {direccion} w[{i}]\")\n",
    "\n",
    "direccion_b = \"aumentar\" if dj_db_test < 0 else \"disminuir\"\n",
    "print(f\"  Bias: {dj_db_test:.6f} → {direccion_b} b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Implementación Completa de Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent completo para múltiples variables\n",
    "\n",
    "def gradient_descent_multivariable(X, y, w_inicial, b_inicial, \n",
    "                                  calcular_costo_func, calcular_gradientes_func,\n",
    "                                  alpha, num_iteraciones):\n",
    "    \"\"\"\n",
    "    Implementa gradient descent para regresión lineal multivariable\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Matriz de features (m, n)\n",
    "        y (ndarray): Vector de targets (m,)\n",
    "        w_inicial (ndarray): Valores iniciales de w (n,)\n",
    "        b_inicial (scalar): Valor inicial de b\n",
    "        calcular_costo_func: Función para calcular costo\n",
    "        calcular_gradientes_func: Función para calcular gradientes\n",
    "        alpha (float): Learning rate\n",
    "        num_iteraciones (int): Número de iteraciones\n",
    "        \n",
    "    Returns:\n",
    "        w (ndarray): Parámetros w optimizados\n",
    "        b (scalar): Parámetro b optimizado\n",
    "        historial_costos (list): Historia del costo\n",
    "    \"\"\"\n",
    "    # Inicializar parámetros (evitar modificar originales)\n",
    "    w = copy.deepcopy(w_inicial)\n",
    "    b = b_inicial\n",
    "    \n",
    "    # Historia para tracking\n",
    "    historial_costos = []\n",
    "    \n",
    "    for i in range(num_iteraciones):\n",
    "        # Calcular gradientes\n",
    "        dj_dw, dj_db = calcular_gradientes_func(X, y, w, b)\n",
    "        \n",
    "        # Actualizar parámetros\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        # Guardar costo\n",
    "        if i < 100000:  # Evitar usar mucha memoria\n",
    "            costo = calcular_costo_func(X, y, w, b)\n",
    "            historial_costos.append(costo)\n",
    "        \n",
    "        # Imprimir progreso\n",
    "        if i % math.ceil(num_iteraciones / 10) == 0:\n",
    "            costo_actual = calcular_costo_func(X, y, w, b)\n",
    "            print(f\"Iteración {i:4d}: Costo = {costo_actual:8.2f}\")\n",
    "    \n",
    "    return w, b, historial_costos\n",
    "\n",
    "# Ejecutar gradient descent\n",
    "print(\"Entrenando modelo de regresión multivariable...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Parámetros de entrenamiento\n",
    "w_inicial = np.zeros(4)  # Empezar desde cero\n",
    "b_inicial = 0.\n",
    "learning_rate = 5.0e-7  # Learning rate pequeño para datos no normalizados\n",
    "iteraciones = 1000\n",
    "\n",
    "# Entrenar modelo\n",
    "w_final, b_final, hist_costos = gradient_descent_multivariable(\n",
    "    X_train, y_train, w_inicial, b_inicial,\n",
    "    calcular_costo_multivariable, calcular_gradientes_multivariable,\n",
    "    learning_rate, iteraciones\n",
    ")\n",
    "\n",
    "print(f\"\\nEntrenamiento completado!\")\n",
    "print(f\"Parámetros finales:\")\n",
    "for i, (feature, peso) in enumerate(zip(feature_names, w_final)):\n",
    "    print(f\"  w[{i}] ({feature}): {peso:.4f}\")\n",
    "print(f\"  b (bias): {b_final:.4f}\")\n",
    "print(f\"Costo final: {hist_costos[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualización del Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar proceso de entrenamiento\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Subplot 1: Evolución del costo\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist_costos, 'b-', linewidth=2)\n",
    "plt.title('Evolución del Costo Durante el Entrenamiento')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo J(w,b)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Marcar costo final\n",
    "plt.plot(len(hist_costos)-1, hist_costos[-1], 'ro', markersize=8)\n",
    "plt.annotate(f'Final: {hist_costos[-1]:.1f}', \n",
    "             xy=(len(hist_costos)-1, hist_costos[-1]),\n",
    "             xytext=(len(hist_costos)*0.6, hist_costos[-1]*1.2),\n",
    "             arrowprops=dict(arrowstyle='->', color='red'))\n",
    "\n",
    "# Subplot 2: Parámetros finales\n",
    "plt.subplot(1, 2, 2)\n",
    "colores = ['skyblue', 'lightgreen', 'lightcoral', 'lightsalmon']\n",
    "barras = plt.bar(range(len(feature_names)), w_final, color=colores, alpha=0.7, edgecolor='black')\n",
    "plt.title('Pesos Finales por Feature')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Peso w_j')\n",
    "plt.xticks(range(len(feature_names)), [f.replace(' ', '\\n').replace('(', '\\n(') for f in feature_names])\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for bar, peso in zip(barras, w_final):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + (0.01 if height >= 0 else -0.03),\n",
    "             f'{peso:.3f}', ha='center', va='bottom' if height >= 0 else 'top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Análisis del entrenamiento:\")\n",
    "print(f\"  • El costo disminuyó de {hist_costos[0]:.1f} a {hist_costos[-1]:.1f}\")\n",
    "print(f\"  • Reducción: {((hist_costos[0] - hist_costos[-1])/hist_costos[0]*100):.1f}%\")\n",
    "\n",
    "print(f\"\\nInterpretación de pesos:\")\n",
    "interpretaciones = [\n",
    "    \"Cada pie cuadrado adicional aumenta el precio\",\n",
    "    \"Cada habitación adicional aumenta el precio\", \n",
    "    \"Pisos adicionales tienen impacto negativo (¿sorprendente?)\",\n",
    "    \"Cada año de edad reduce el precio (casas nuevas valen más)\"\n",
    "]\n",
    "\n",
    "for feature, peso, interp in zip(feature_names, w_final, interpretaciones):\n",
    "    signo = \"↗️\" if peso > 0 else \"↘️\"\n",
    "    print(f\"  {signo} {feature}: {interp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluación y Predicciones del Modelo\n",
    "\n",
    "### 5.1 Predicciones en el Conjunto de Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer predicciones para todas las casas del entrenamiento\n",
    "print(f\"Predicciones vs Valores Reales:\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"{'Casa':<6} {'Tamaño':<8} {'Habitac.':<9} {'Pisos':<6} {'Edad':<5} {'Pred.':<8} {'Real':<8} {'Error':<8}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "errores_absolutos = []\n",
    "for i in range(len(X_train)):\n",
    "    # Predicción\n",
    "    prediccion = np.dot(X_train[i], w_final) + b_final\n",
    "    error = prediccion - y_train[i]\n",
    "    error_abs = abs(error)\n",
    "    errores_absolutos.append(error_abs)\n",
    "    \n",
    "    print(f\"Casa {i+1:<2d} {X_train[i,0]:<8.0f} {X_train[i,1]:<9.0f} {X_train[i,2]:<6.0f} \"\n",
    "          f\"{X_train[i,3]:<5.0f} ${prediccion:<7.0f} ${y_train[i]:<7.0f} ${error:<+7.0f}\")\n",
    "\n",
    "# Estadísticas de error\n",
    "mae = np.mean(errores_absolutos)  # Error Absoluto Medio\n",
    "rmse = np.sqrt(np.mean([e**2 for e in [pred - real for pred, real in \n",
    "                       zip([np.dot(X_train[i], w_final) + b_final for i in range(len(X_train))], y_train)]]))\n",
    "\n",
    "print(f\"\\nEstadísticas de error:\")\n",
    "print(f\"  Error Absoluto Medio (MAE): ${mae:.1f}k\")\n",
    "print(f\"  Raíz Error Cuadrático Medio (RMSE): ${rmse:.1f}k\")\n",
    "print(f\"  Error máximo: ${max(errores_absolutos):.1f}k\")\n",
    "\n",
    "# Calcular R²\n",
    "predicciones_todas = [np.dot(X_train[i], w_final) + b_final for i in range(len(X_train))]\n",
    "ss_res = sum([(pred - real)**2 for pred, real in zip(predicciones_todas, y_train)])\n",
    "ss_tot = sum([(real - np.mean(y_train))**2 for real in y_train])\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"  Coeficiente R²: {r2:.3f} ({r2*100:.1f}% de varianza explicada)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Predicciones para Casas Nuevas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones para casas hipotéticas\n",
    "casas_nuevas = [\n",
    "    ([1200, 3, 1, 15], \"Casa familiar típica\"),\n",
    "    ([2500, 4, 2, 5], \"Casa grande y nueva\"),\n",
    "    ([800, 2, 1, 30], \"Casa pequeña, algo vieja\"),\n",
    "    ([3500, 5, 2, 2], \"Mansión nueva\"),\n",
    "    ([1800, 3, 2, 20], \"Casa mediana\"),\n",
    "]\n",
    "\n",
    "print(f\"Predicciones para Casas Nuevas:\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"{'Descripción':<20} {'Features':<20} {'Precio Predicho':<15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for features, descripcion in casas_nuevas:\n",
    "    x_nueva = np.array(features)\n",
    "    precio_predicho = np.dot(x_nueva, w_final) + b_final\n",
    "    \n",
    "    features_str = f\"{features[0]}sqft,{features[1]}bed,{features[2]}fl,{features[3]}yrs\"\n",
    "    print(f\"{descripcion:<20} {features_str:<20} ${precio_predicho:<14,.0f}\")\n",
    "\n",
    "print(f\"\\nAnálisis detallado de una predicción:\")\n",
    "x_ejemplo = np.array([1200, 3, 1, 15])\n",
    "precio = np.dot(x_ejemplo, w_final) + b_final\n",
    "\n",
    "print(f\"Casa: 1200 sqft, 3 habitaciones, 1 piso, 15 años\")\n",
    "print(f\"Desglose del cálculo:\")\n",
    "total_features = 0\n",
    "for i, (feature, peso, valor) in enumerate(zip(feature_names, w_final, x_ejemplo)):\n",
    "    contribucion = peso * valor\n",
    "    total_features += contribucion\n",
    "    print(f\"  {feature}: {peso:.4f} × {valor} = ${contribucion:+.0f}\")\n",
    "    \n",
    "print(f\"  Bias: {b_final:+.0f}\")\n",
    "print(f\"  Total: ${precio:.0f}\")\n",
    "\n",
    "# Análisis de sensibilidad\n",
    "print(f\"\\nAnálisis de sensibilidad (cambio en precio por unidad):\")\n",
    "cambios_unidad = [1, 1, 1, 1]  # 1 sqft, 1 habitación, 1 piso, 1 año\n",
    "for feature, peso, cambio in zip(feature_names, w_final, cambios_unidad):\n",
    "    impacto = peso * cambio\n",
    "    print(f\"  +{cambio} {feature}: ${impacto:+.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Visualización de Predicciones vs Realidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear visualizaciones de evaluación\n",
    "predicciones_entrenamiento = [np.dot(X_train[i], w_final) + b_final for i in range(len(X_train))]\n",
    "residuos = [pred - real for pred, real in zip(predicciones_entrenamiento, y_train)]\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Subplot 1: Predicciones vs Valores Reales\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(predicciones_entrenamiento, y_train, alpha=0.7, s=50)\n",
    "\n",
    "# Línea diagonal perfecta\n",
    "min_val = min(min(predicciones_entrenamiento), min(y_train))\n",
    "max_val = max(max(predicciones_entrenamiento), max(y_train))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, \n",
    "         label='Predicción perfecta')\n",
    "\n",
    "plt.title('Predicciones vs Valores Reales')\n",
    "plt.xlabel('Predicciones ($k)')\n",
    "plt.ylabel('Valores Reales ($k)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Residuos\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(predicciones_entrenamiento, residuos, alpha=0.7, s=50)\n",
    "plt.axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Residuos vs Predicciones')\n",
    "plt.xlabel('Predicciones ($k)')\n",
    "plt.ylabel('Residuos ($k)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Histograma de residuos\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(residuos, bins=8, alpha=0.7, edgecolor='black')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Distribución de Residuos')\n",
    "plt.xlabel('Residuos ($k)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Análisis de residuos:\")\n",
    "print(f\"  Media de residuos: ${np.mean(residuos):.1f}k (debería ser ~0)\")\n",
    "print(f\"  Desviación estándar: ${np.std(residuos):.1f}k\")\n",
    "print(f\"  Residuo máximo (absoluto): ${max([abs(r) for r in residuos]):.1f}k\")\n",
    "\n",
    "# Análisis de features más importantes\n",
    "importancia_relativa = [abs(w) for w in w_final]\n",
    "feature_mas_importante = feature_names[np.argmax(importancia_relativa)]\n",
    "print(f\"\\nFeature más importante: {feature_mas_importante}\")\n",
    "print(f\"Ranking de importancia (por magnitud de peso):\")\n",
    "indices_ordenados = np.argsort(importancia_relativa)[::-1]\n",
    "for i, idx in enumerate(indices_ordenados):\n",
    "    print(f\"  {i+1}. {feature_names[idx]}: |{w_final[idx]:.4f}|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Analizar el Impacto de Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 1: Analizar qué pasaría si modificamos una feature\n",
    "\n",
    "def analizar_impacto_feature(casa_base, indice_feature, cambios, w, b):\n",
    "    \"\"\"\n",
    "    Analiza cómo cambia el precio al modificar una feature específica\n",
    "    \n",
    "    Args:\n",
    "        casa_base: Features de la casa base\n",
    "        indice_feature: Índice de la feature a modificar\n",
    "        cambios: Lista de cambios a aplicar\n",
    "        w, b: Parámetros del modelo\n",
    "    \"\"\"\n",
    "    casa_base = np.array(casa_base)\n",
    "    precio_base = np.dot(casa_base, w) + b\n",
    "    \n",
    "    print(f\"Análisis de impacto: {feature_names[indice_feature]}\")\n",
    "    print(f\"Casa base: {casa_base} → ${precio_base:.0f}k\")\n",
    "    print(f\"\\n{'Cambio':<10} {'Nueva Feature':<15} {'Nuevo Precio':<12} {'Diferencia':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for cambio in cambios:\n",
    "        casa_modificada = casa_base.copy()\n",
    "        casa_modificada[indice_feature] += cambio\n",
    "        precio_nuevo = np.dot(casa_modificada, w) + b\n",
    "        diferencia = precio_nuevo - precio_base\n",
    "        \n",
    "        print(f\"{cambio:+<10} {casa_modificada[indice_feature]:<15.0f} \"\n",
    "              f\"${precio_nuevo:<11.0f} ${diferencia:+<11.0f}\")\n",
    "\n",
    "# Casa ejemplo: casa mediana\n",
    "casa_ejemplo = [1500, 3, 1, 20]  # 1500 sqft, 3 habitaciones, 1 piso, 20 años\n",
    "\n",
    "print(\"Ejercicio 1: Análisis de Impacto de Features\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Analizar impacto del tamaño\n",
    "cambios_tamaño = [-500, -200, 200, 500, 1000]\n",
    "analizar_impacto_feature(casa_ejemplo, 0, cambios_tamaño, w_final, b_final)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Analizar impacto de habitaciones\n",
    "cambios_habitaciones = [-1, 1, 2]\n",
    "analizar_impacto_feature(casa_ejemplo, 1, cambios_habitaciones, w_final, b_final)\n",
    "\n",
    "print(f\"\\nConclusión: El tamaño tiene el mayor impacto en el precio\")\n",
    "print(f\"Cada 100 sqft adicionales → ${w_final[0]*100:+.0f}k en precio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Comparar Modelos con Diferentes Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 2: Entrenar modelos usando solo subconjuntos de features\n",
    "\n",
    "def entrenar_modelo_subset(X, y, indices_features, nombre_modelo):\n",
    "    \"\"\"\n",
    "    Entrena modelo usando solo un subconjunto de features\n",
    "    \"\"\"\n",
    "    X_subset = X[:, indices_features]\n",
    "    n_features = X_subset.shape[1]\n",
    "    \n",
    "    # Parámetros iniciales\n",
    "    w_init = np.zeros(n_features)\n",
    "    b_init = 0.\n",
    "    \n",
    "    # Entrenar (versión simplificada, menos iteraciones)\n",
    "    w, b, _ = gradient_descent_multivariable(\n",
    "        X_subset, y, w_init, b_init,\n",
    "        calcular_costo_multivariable, calcular_gradientes_multivariable,\n",
    "        5.0e-7, 300  # Menos iteraciones para el ejercicio\n",
    "    )\n",
    "    \n",
    "    # Calcular métricas\n",
    "    predicciones = [np.dot(X_subset[i], w) + b for i in range(len(X_subset))]\n",
    "    mse = np.mean([(pred - real)**2 for pred, real in zip(predicciones, y)])\n",
    "    mae = np.mean([abs(pred - real) for pred, real in zip(predicciones, y)])\n",
    "    \n",
    "    return {\n",
    "        'nombre': nombre_modelo,\n",
    "        'features_usadas': [feature_names[i] for i in indices_features],\n",
    "        'w': w,\n",
    "        'b': b,\n",
    "        'mse': mse,\n",
    "        'mae': mae,\n",
    "        'predicciones': predicciones\n",
    "    }\n",
    "\n",
    "print(\"Ejercicio 2: Comparación de Modelos con Diferentes Features\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Definir diferentes combinaciones de features\n",
    "modelos_a_probar = [\n",
    "    ([0], \"Solo Tamaño\"),\n",
    "    ([0, 1], \"Tamaño + Habitaciones\"),\n",
    "    ([0, 3], \"Tamaño + Edad\"),\n",
    "    ([0, 1, 3], \"Tamaño + Habitaciones + Edad\"),\n",
    "    ([0, 1, 2, 3], \"Todas las Features\")\n",
    "]\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for indices, nombre in modelos_a_probar:\n",
    "    print(f\"\\nEntrenando: {nombre}\")\n",
    "    resultado = entrenar_modelo_subset(X_train, y_train, indices, nombre)\n",
    "    resultados.append(resultado)\n",
    "\n",
    "# Comparar resultados\n",
    "print(f\"\\n\\nComparación de Modelos:\")\n",
    "print(f\"{'Modelo':<25} {'MSE':<8} {'MAE':<8} {'Features':<30}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for resultado in resultados:\n",
    "    features_str = \", \".join([f.split()[0] for f in resultado['features_usadas']])\n",
    "    print(f\"{resultado['nombre']:<25} {resultado['mse']:<8.1f} {resultado['mae']:<8.1f} {features_str:<30}\")\n",
    "\n",
    "# Encontrar el mejor modelo\n",
    "mejor_modelo = min(resultados, key=lambda x: x['mse'])\n",
    "print(f\"\\nMejor modelo por MSE: {mejor_modelo['nombre']}\")\n",
    "print(f\"Features: {', '.join(mejor_modelo['features_usadas'])}\")\n",
    "\n",
    "# Análisis de complejidad vs rendimiento\n",
    "print(f\"\\nAnálisis:\")\n",
    "print(f\"  • Usar solo tamaño da resultados razonables\")\n",
    "print(f\"  • Añadir más features mejora el modelo hasta cierto punto\")\n",
    "print(f\"  • El modelo completo tiene el mejor rendimiento\")\n",
    "print(f\"  • Cada feature adicional aporta información valiosa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen y Conceptos Clave\n",
    "\n",
    "### ✅ Lo que has aprendido:\n",
    "\n",
    "#### 1. **Extensión a Múltiples Variables**:\n",
    "   - **Modelo**: $f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b$\n",
    "   - **Vectorización**: Uso eficiente de NumPy para operaciones matriciales\n",
    "   - **Múltiples features**: Tamaño, habitaciones, pisos, edad\n",
    "\n",
    "#### 2. **Implementación Vectorizada**:\n",
    "   - **Predicción**: `np.dot(x, w) + b`\n",
    "   - **Speedup**: 100x-1000x más rápido que loops\n",
    "   - **Escalabilidad**: Maneja datasets grandes eficientemente\n",
    "\n",
    "#### 3. **Función de Costo Multivariable**:\n",
    "   - **Fórmula**: Misma estructura, pero con vectores\n",
    "   - **Implementación**: Loop sobre ejemplos, producto punto por predicción\n",
    "   - **Interpretación**: Mide ajuste global del modelo\n",
    "\n",
    "#### 4. **Gradient Descent Multivariable**:\n",
    "   - **Gradientes**: Un gradiente por cada parámetro w_j\n",
    "   - **Actualización simultánea**: Todos los parámetros se actualizan juntos\n",
    "   - **Convergencia**: Encuentra mínimo global en función convexa\n",
    "\n",
    "#### 5. **Análisis de Features**:\n",
    "   - **Importancia relativa**: Magnitud de pesos indica importancia\n",
    "   - **Interpretación**: Cada peso representa impacto marginal\n",
    "   - **Sensibilidad**: Análisis de cambios en features individuales\n",
    "\n",
    "### 🏠 Aplicación Práctica - Precios de Casas:\n",
    "- **Modelo final**: 4 features → predicción de precio\n",
    "- **Rendimiento**: R² = {r2:.3f} (explicación de varianza)\n",
    "- **Feature más importante**: Tamaño de la casa\n",
    "- **Aplicación**: Valoración automática de propiedades\n",
    "\n",
    "### 🚀 Próximos pasos:\n",
    "- **Feature Scaling**: Normalización para mejor convergencia\n",
    "- **Learning Rate**: Optimización de hiperparámetros\n",
    "- **Regularización**: Control de overfitting\n",
    "- **Feature Engineering**: Creación de features más informativas\n",
    "\n",
    "### 💡 Puntos clave para recordar:\n",
    "1. **Vectorización es esencial** para eficiencia en ML\n",
    "2. **Más features ≠ siempre mejor modelo** (curse of dimensionality)\n",
    "3. **Interpretabilidad**: Los pesos tienen significado business\n",
    "4. **Evaluación múltiple**: Usar varias métricas (MSE, MAE, R²)\n",
    "5. **Análisis de residuos**: Validar supuestos del modelo\n",
    "\n",
    "### ⚠️ Limitaciones observadas:\n",
    "- **Learning rate pequeño**: Datos no normalizados requieren α muy bajo\n",
    "- **Convergencia lenta**: Diferentes escalas de features afectan velocidad\n",
    "- **Interpretación cuidadosa**: Peso negativo de \"pisos\" requiere análisis\n",
    "\n",
    "**Siguiente paso**: Aprenderemos técnicas de feature scaling para resolver estos problemas y mejorar significativamente el entrenamiento."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}