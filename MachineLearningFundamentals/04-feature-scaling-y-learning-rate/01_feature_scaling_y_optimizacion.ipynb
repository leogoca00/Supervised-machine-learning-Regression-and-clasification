{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìè Feature Scaling y Optimizaci√≥n del Learning Rate\n",
    "\n",
    "## üìö Objetivos de Aprendizaje\n",
    "En este notebook aprender√°s:\n",
    "- **Por qu√©** el feature scaling es crucial para el gradient descent\n",
    "- **C√≥mo** implementar diferentes t√©cnicas de normalizaci√≥n\n",
    "- **Cu√°ndo** y **c√≥mo** ajustar el learning rate\n",
    "- **T√©cnicas avanzadas** para acelerar la convergencia\n",
    "\n",
    "## üîç Problema Identificado\n",
    "En el notebook anterior observamos:\n",
    "- **Learning rate muy peque√±o** (5.0e-7) necesario para evitar divergencia\n",
    "- **Convergencia lenta** debido a diferentes escalas de features\n",
    "- **Gradientes desbalanceados** entre diferentes par√°metros\n",
    "\n",
    "**Soluci√≥n**: Feature Scaling + Learning Rate Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "\n",
    "# Configuraci√≥n\n",
    "plt.style.use('default')\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas\")\n",
    "print(\"üéØ Listo para optimizar gradient descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. üìä An√°lisis del Problema: Escalas Desbalanceadas\n",
    "\n",
    "### 1.1 Cargar Dataset y Analizar Escalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el mismo dataset de casas\n",
    "def cargar_datos_casas():\n",
    "    X_train = np.array([\n",
    "        [2104, 5, 1, 45],    # Casa 1: [tama√±o_sqft, habitaciones, pisos, edad]\n",
    "        [1416, 3, 2, 40],    # Casa 2  \n",
    "        [852,  2, 1, 35],    # Casa 3\n",
    "        [1940, 4, 1, 10],    # Casa 4\n",
    "        [1539, 3, 2, 25],    # Casa 5\n",
    "        [3000, 4, 2, 8],     # Casa 6\n",
    "        [1230, 3, 1, 15],    # Casa 7\n",
    "        [2145, 4, 1, 12],    # Casa 8\n",
    "        [1736, 3, 2, 30],    # Casa 9\n",
    "        [1000, 2, 1, 50],    # Casa 10\n",
    "        [1940, 4, 3, 5],     # Casa 11 (nueva)\n",
    "        [1100, 2, 1, 60]     # Casa 12 (nueva)\n",
    "    ])\n",
    "    \n",
    "    y_train = np.array([460, 232, 178, 500, 315, 740, 285, 510, 390, 180, 550, 165])\n",
    "    feature_names = ['Tama√±o (sqft)', 'Habitaciones', 'Pisos', 'Edad (a√±os)']\n",
    "    \n",
    "    return X_train, y_train, feature_names\n",
    "\n",
    "X_train, y_train, feature_names = cargar_datos_casas()\n",
    "m, n = X_train.shape\n",
    "\n",
    "print(f\"üìä Dataset actualizado:\")\n",
    "print(f\"   ‚Ä¢ Ejemplos (m): {m}\")\n",
    "print(f\"   ‚Ä¢ Features (n): {n}\")\n",
    "\n",
    "# An√°lisis detallado de escalas\n",
    "print(f\"\\nüìè An√°lisis de Escalas por Feature:\")\n",
    "print(f\"{'Feature':<15} {'Min':<8} {'Max':<8} {'Media':<8} {'Std':<8} {'Rango':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    columna = X_train[:, i]\n",
    "    min_val = np.min(columna)\n",
    "    max_val = np.max(columna)\n",
    "    media = np.mean(columna)\n",
    "    std = np.std(columna)\n",
    "    rango = max_val - min_val\n",
    "    \n",
    "    print(f\"{feature:<15} {min_val:<8.0f} {max_val:<8.0f} {media:<8.1f} {std:<8.1f} {rango:<10.0f}\")\n",
    "\n",
    "print(f\"\\nüîç Observaciones cr√≠ticas:\")\n",
    "print(f\"   ‚Ä¢ Tama√±o: Rango ~2000 (852-3000)\")\n",
    "print(f\"   ‚Ä¢ Habitaciones: Rango ~3 (2-5)\")\n",
    "print(f\"   ‚Ä¢ Pisos: Rango ~2 (1-3)\")\n",
    "print(f\"   ‚Ä¢ Edad: Rango ~55 (5-60)\")\n",
    "print(f\"\\n‚ö†Ô∏è  PROBLEMA: Tama√±o domina num√©ricamente sobre otras features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualizaci√≥n del Problema de Escalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el problema de escalas\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Box plots para mostrar diferencias de escala\n",
    "plt.subplot(2, 3, 1)\n",
    "box_data = [X_train[:, i] for i in range(n)]\n",
    "plt.boxplot(box_data, labels=[name.split()[0] for name in feature_names])\n",
    "plt.title('Distribuci√≥n de Features (Escala Original)')\n",
    "plt.ylabel('Valores')\n",
    "plt.yscale('log')  # Escala logar√≠tmica para ver todas las features\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Histogramas individuales\n",
    "for i in range(n):\n",
    "    plt.subplot(2, 3, i+2)\n",
    "    plt.hist(X_train[:, i], bins=8, alpha=0.7, edgecolor='black')\n",
    "    plt.title(f'{feature_names[i]}')\n",
    "    plt.xlabel('Valor')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Demonstrar el impacto en los gradientes\n",
    "def calcular_gradientes_ejemplo(X, y, w, b):\n",
    "    \"\"\"Funci√≥n simplificada para demostrar gradientes\"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0.\n",
    "    \n",
    "    for i in range(m):\n",
    "        error = (np.dot(X[i], w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += error * X[i, j]\n",
    "        dj_db += error\n",
    "    \n",
    "    return dj_dw / m, dj_db / m\n",
    "\n",
    "# Calcular gradientes iniciales\n",
    "w_test = np.array([0.1, 10, -20, -1])  # Valores ejemplo\n",
    "b_test = 100\n",
    "\n",
    "gradientes, grad_b = calcular_gradientes_ejemplo(X_train, y_train, w_test, b_test)\n",
    "\n",
    "print(f\"\\nüßÆ Impacto en Gradientes (sin normalizaci√≥n):\")\n",
    "print(f\"{'Feature':<15} {'Gradiente':<12} {'Magnitud':<10}\")\n",
    "print(\"-\" * 40)\n",
    "for i, (feature, grad) in enumerate(zip(feature_names, gradientes)):\n",
    "    print(f\"{feature:<15} {grad:<12.2e} {abs(grad):<10.2e}\")\n",
    "\n",
    "print(f\"\\nüìä Ratio de magnitudes:\")\n",
    "grad_abs = np.abs(gradientes)\n",
    "ratio_max_min = np.max(grad_abs) / np.min(grad_abs[grad_abs > 0])\n",
    "print(f\"   M√°ximo/M√≠nimo: {ratio_max_min:.1f}x de diferencia\")\n",
    "print(f\"   Esto causa convergencia desigual y requiere learning rate muy peque√±o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. üéØ T√©cnicas de Feature Scaling\n",
    "\n",
    "### 2.1 Z-Score Normalization (Standardization)\n",
    "\n",
    "**F√≥rmula**: $x_{norm} = \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "**Resultado**: Media = 0, Desviaci√≥n est√°ndar = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementaci√≥n de Z-Score Normalization\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    Aplica normalizaci√≥n Z-score a las features\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Matriz de features (m, n)\n",
    "        \n",
    "    Returns:\n",
    "        X_norm (ndarray): Features normalizadas\n",
    "        mu (ndarray): Media de cada feature\n",
    "        sigma (ndarray): Desviaci√≥n est√°ndar de cada feature\n",
    "    \"\"\"\n",
    "    # Calcular estad√≠sticas\n",
    "    mu = np.mean(X, axis=0)      # Media por columna\n",
    "    sigma = np.std(X, axis=0)    # Desviaci√≥n est√°ndar por columna\n",
    "    \n",
    "    # Normalizar\n",
    "    X_norm = (X - mu) / sigma\n",
    "    \n",
    "    return X_norm, mu, sigma\n",
    "\n",
    "# Aplicar Z-score normalization\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "\n",
    "print(f\"üìä Estad√≠sticas antes de normalizaci√≥n:\")\n",
    "print(f\"   Media por feature: {X_mu}\")\n",
    "print(f\"   Std por feature:   {X_sigma}\")\n",
    "\n",
    "print(f\"\\nüìä Estad√≠sticas despu√©s de normalizaci√≥n:\")\n",
    "print(f\"   Media por feature: {np.mean(X_norm, axis=0)}\")\n",
    "print(f\"   Std por feature:   {np.std(X_norm, axis=0)}\")\n",
    "\n",
    "print(f\"\\nüìè Comparaci√≥n de rangos:\")\n",
    "print(f\"{'Feature':<15} {'Rango Original':<15} {'Rango Normalizado':<18}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    rango_original = np.ptp(X_train[:, i])  # peak-to-peak\n",
    "    rango_norm = np.ptp(X_norm[:, i])\n",
    "    print(f\"{feature:<15} {rango_original:<15.0f} {rango_norm:<18.2f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Normalizaci√≥n exitosa: Todas las features ahora tienen escala similar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Min-Max Normalization\n",
    "\n",
    "**F√≥rmula**: $x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
    "\n",
    "**Resultado**: Valores entre 0 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementaci√≥n de Min-Max Normalization\n",
    "\n",
    "def minmax_normalize_features(X):\n",
    "    \"\"\"\n",
    "    Aplica normalizaci√≥n Min-Max a las features\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Matriz de features (m, n)\n",
    "        \n",
    "    Returns:\n",
    "        X_norm (ndarray): Features normalizadas [0, 1]\n",
    "        X_min (ndarray): Valor m√≠nimo de cada feature\n",
    "        X_max (ndarray): Valor m√°ximo de cada feature\n",
    "    \"\"\"\n",
    "    X_min = np.min(X, axis=0)\n",
    "    X_max = np.max(X, axis=0)\n",
    "    \n",
    "    # Evitar divisi√≥n por cero\n",
    "    X_range = X_max - X_min\n",
    "    X_range[X_range == 0] = 1  # Si max == min, mantener valor original\n",
    "    \n",
    "    X_norm = (X - X_min) / X_range\n",
    "    \n",
    "    return X_norm, X_min, X_max\n",
    "\n",
    "# Aplicar Min-Max normalization\n",
    "X_minmax, X_min, X_max = minmax_normalize_features(X_train)\n",
    "\n",
    "print(f\"üìä Min-Max Normalization:\")\n",
    "print(f\"   Valores m√≠nimos: {X_min}\")\n",
    "print(f\"   Valores m√°ximos: {X_max}\")\n",
    "\n",
    "print(f\"\\nüìä Estad√≠sticas despu√©s de Min-Max:\")\n",
    "print(f\"   Min por feature: {np.min(X_minmax, axis=0)}\")\n",
    "print(f\"   Max por feature: {np.max(X_minmax, axis=0)}\")\n",
    "print(f\"   Media por feature: {np.mean(X_minmax, axis=0)}\")\n",
    "\n",
    "# Comparar ambas t√©cnicas visualmente\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original\n",
    "plt.subplot(1, 3, 1)\n",
    "for i in range(n):\n",
    "    plt.scatter([i] * len(X_train), X_train[:, i], alpha=0.6, s=50)\n",
    "plt.title('Features Originales')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Valor')\n",
    "plt.yscale('log')\n",
    "plt.xticks(range(n), [f.split()[0] for f in feature_names])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Z-Score\n",
    "plt.subplot(1, 3, 2)\n",
    "for i in range(n):\n",
    "    plt.scatter([i] * len(X_norm), X_norm[:, i], alpha=0.6, s=50)\n",
    "plt.title('Z-Score Normalization')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Valor Normalizado')\n",
    "plt.xticks(range(n), [f.split()[0] for f in feature_names])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Min-Max\n",
    "plt.subplot(1, 3, 3)\n",
    "for i in range(n):\n",
    "    plt.scatter([i] * len(X_minmax), X_minmax[:, i], alpha=0.6, s=50)\n",
    "plt.title('Min-Max Normalization')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Valor Normalizado [0,1]')\n",
    "plt.xticks(range(n), [f.split()[0] for f in feature_names])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nü§î ¬øCu√°l usar?\")\n",
    "print(f\"   ‚Ä¢ Z-Score: Mejor para distribuciones normales, no acotado\")\n",
    "print(f\"   ‚Ä¢ Min-Max: Mejor cuando necesitas rango espec√≠fico [0,1]\")\n",
    "print(f\"   ‚Ä¢ Para ML: Z-Score es m√°s com√∫n y robusto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ‚ö° Impacto en Gradient Descent\n",
    "\n",
    "### 3.1 Comparaci√≥n: Con vs Sin Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares (copiadas del notebook anterior)\n",
    "def calcular_costo_multivariable(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    costo = 0.0\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b\n",
    "        costo += (f_wb_i - y[i]) ** 2\n",
    "    return costo / (2 * m)\n",
    "\n",
    "def calcular_gradientes_multivariable(X, y, w, b):\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0.\n",
    "    \n",
    "    for i in range(m):\n",
    "        error = (np.dot(X[i], w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += error * X[i, j]\n",
    "        dj_db += error\n",
    "    \n",
    "    return dj_dw / m, dj_db / m\n",
    "\n",
    "def gradient_descent_mejorado(X, y, w_inicial, b_inicial, alpha, num_iteraciones, verbose=True):\n",
    "    \"\"\"Gradient descent con tracking mejorado\"\"\"\n",
    "    w = copy.deepcopy(w_inicial)\n",
    "    b = b_inicial\n",
    "    historial_costos = []\n",
    "    historial_params = []\n",
    "    \n",
    "    for i in range(num_iteraciones):\n",
    "        # Calcular costo y gradientes\n",
    "        costo = calcular_costo_multivariable(X, y, w, b)\n",
    "        dj_dw, dj_db = calcular_gradientes_multivariable(X, y, w, b)\n",
    "        \n",
    "        # Actualizar par√°metros\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        # Guardar historia\n",
    "        historial_costos.append(costo)\n",
    "        historial_params.append((w.copy(), b))\n",
    "        \n",
    "        # Imprimir progreso\n",
    "        if verbose and (i % (num_iteraciones // 10) == 0 or i < 10):\n",
    "            print(f\"Iter {i:4d}: Costo = {costo:8.4f}, |dj_dw| = {np.linalg.norm(dj_dw):.2e}\")\n",
    "    \n",
    "    return w, b, historial_costos, historial_params\n",
    "\n",
    "# Experimento: Comparar convergencia con y sin normalizaci√≥n\n",
    "print(f\"üß™ EXPERIMENTO: Convergencia Con vs Sin Feature Scaling\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# Par√°metros del experimento\n",
    "w_inicial = np.zeros(n)\n",
    "b_inicial = 0.\n",
    "iteraciones = 1000\n",
    "\n",
    "# Caso 1: Sin normalizaci√≥n (learning rate peque√±o)\n",
    "print(f\"\\nüìâ Caso 1: SIN normalizaci√≥n (Œ± = 5e-7)\")\n",
    "alpha_sin_norm = 5.0e-7\n",
    "w1, b1, costos1, params1 = gradient_descent_mejorado(\n",
    "    X_train, y_train, w_inicial, b_inicial, alpha_sin_norm, iteraciones, verbose=False\n",
    ")\n",
    "\n",
    "print(f\"   Costo inicial: {costos1[0]:.2f}\")\n",
    "print(f\"   Costo final:   {costos1[-1]:.2f}\")\n",
    "print(f\"   Reducci√≥n:     {((costos1[0] - costos1[-1])/costos1[0]*100):.1f}%\")\n",
    "\n",
    "# Caso 2: Con normalizaci√≥n (learning rate grande)\n",
    "print(f\"\\nüìà Caso 2: CON normalizaci√≥n (Œ± = 0.1)\")\n",
    "alpha_con_norm = 0.1\n",
    "w2, b2, costos2, params2 = gradient_descent_mejorado(\n",
    "    X_norm, y_train, w_inicial, b_inicial, alpha_con_norm, iteraciones, verbose=False\n",
    ")\n",
    "\n",
    "print(f\"   Costo inicial: {costos2[0]:.2f}\")\n",
    "print(f\"   Costo final:   {costos2[-1]:.2f}\")\n",
    "print(f\"   Reducci√≥n:     {((costos2[0] - costos2[-1])/costos2[0]*100):.1f}%\")\n",
    "\n",
    "# Comparaci√≥n de velocidades\n",
    "mejora_velocidad = alpha_con_norm / alpha_sin_norm\n",
    "print(f\"\\n‚ö° COMPARACI√ìN:\")\n",
    "print(f\"   Learning rate con normalizaci√≥n: {mejora_velocidad:.0f}x m√°s grande\")\n",
    "print(f\"   Convergencia: Mucho m√°s r√°pida y estable\")\n",
    "print(f\"   Costo final similar: {abs(costos1[-1] - costos2[-1]):.2f} diferencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualizaci√≥n de la Convergencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparaci√≥n de convergencia\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Evoluci√≥n del costo (escala lineal)\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(costos1, 'r-', linewidth=2, label='Sin normalizaci√≥n', alpha=0.8)\n",
    "plt.plot(costos2, 'b-', linewidth=2, label='Con normalizaci√≥n', alpha=0.8)\n",
    "plt.title('Evoluci√≥n del Costo (Lineal)')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Evoluci√≥n del costo (escala log)\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(costos1, 'r-', linewidth=2, label='Sin normalizaci√≥n', alpha=0.8)\n",
    "plt.plot(costos2, 'b-', linewidth=2, label='Con normalizaci√≥n', alpha=0.8)\n",
    "plt.title('Evoluci√≥n del Costo (Log)')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo (log scale)')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Convergencia de par√°metros w (sin normalizaci√≥n)\n",
    "plt.subplot(2, 3, 3)\n",
    "w_history1 = np.array([params[0] for params in params1])\n",
    "for j in range(n):\n",
    "    plt.plot(w_history1[:, j], label=f'w[{j}] ({feature_names[j].split()[0]})', linewidth=2)\n",
    "plt.title('Par√°metros w (Sin Normalizaci√≥n)')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Valor de w')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 4: Convergencia de par√°metros w (con normalizaci√≥n)\n",
    "plt.subplot(2, 3, 4)\n",
    "w_history2 = np.array([params[0] for params in params2])\n",
    "for j in range(n):\n",
    "    plt.plot(w_history2[:, j], label=f'w[{j}] ({feature_names[j].split()[0]})', linewidth=2)\n",
    "plt.title('Par√°metros w (Con Normalizaci√≥n)')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Valor de w')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 5: Comparaci√≥n de gradientes (magnitud)\n",
    "plt.subplot(2, 3, 5)\n",
    "# Calcular norma de gradientes en cada iteraci√≥n\n",
    "normas_grad1 = []\n",
    "normas_grad2 = []\n",
    "\n",
    "for i in range(0, len(costos1), 50):  # Cada 50 iteraciones para velocidad\n",
    "    w_i, b_i = params1[i]\n",
    "    dj_dw, _ = calcular_gradientes_multivariable(X_train, y_train, w_i, b_i)\n",
    "    normas_grad1.append(np.linalg.norm(dj_dw))\n",
    "    \n",
    "    w_i, b_i = params2[i]\n",
    "    dj_dw, _ = calcular_gradientes_multivariable(X_norm, y_train, w_i, b_i)\n",
    "    normas_grad2.append(np.linalg.norm(dj_dw))\n",
    "\n",
    "iters_sample = range(0, len(costos1), 50)\n",
    "plt.plot(iters_sample, normas_grad1, 'r-', linewidth=2, label='Sin normalizaci√≥n', alpha=0.8)\n",
    "plt.plot(iters_sample, normas_grad2, 'b-', linewidth=2, label='Con normalizaci√≥n', alpha=0.8)\n",
    "plt.title('Magnitud de Gradientes')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('||‚àáJ|| (Norma del gradiente)')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 6: Superficie de costo (conceptual)\n",
    "plt.subplot(2, 3, 6)\n",
    "# Crear una representaci√≥n 2D simplificada de la superficie de costo\n",
    "w0_range = np.linspace(-2, 2, 50)\n",
    "w1_range = np.linspace(-2, 2, 50)\n",
    "W0, W1 = np.meshgrid(w0_range, w1_range)\n",
    "\n",
    "# Funci√≥n de costo simulada (el√≠ptica para mostrar el concepto)\n",
    "Z_normalized = (W0**2 + W1**2)  # Circular (normalizado)\n",
    "Z_unnormalized = (W0**2 * 100 + W1**2)  # El√≠ptico (sin normalizar)\n",
    "\n",
    "plt.contour(W0, W1, Z_normalized, levels=10, colors='blue', alpha=0.6, linewidths=1)\n",
    "plt.contour(W0, W1, Z_unnormalized, levels=10, colors='red', alpha=0.6, linewidths=1)\n",
    "plt.title('Superficie de Costo (Conceptual)')\n",
    "plt.xlabel('w0')\n",
    "plt.ylabel('w1')\n",
    "plt.legend(['Normalizado (circular)', 'Sin normalizar (el√≠ptico)'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"üìä An√°lisis de las visualizaciones:\")\n",
    "print(f\"   1. Costo: Ambos convergen, pero normalizaci√≥n es m√°s r√°pida\")\n",
    "print(f\"   2. Log scale: Muestra velocidad de convergencia superior\")\n",
    "print(f\"   3. Par√°metros sin norm: Convergencia desigual y oscilatoria\")\n",
    "print(f\"   4. Par√°metros con norm: Convergencia suave y uniforme\")\n",
    "print(f\"   5. Gradientes: Decaen m√°s r√°pido con normalizaci√≥n\")\n",
    "print(f\"   6. Superficie: Forma circular vs el√≠ptica explica la diferencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. üéõÔ∏è Optimizaci√≥n del Learning Rate\n",
    "\n",
    "### 4.1 B√∫squeda del Learning Rate √ìptimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para probar diferentes learning rates\n",
    "def probar_learning_rates(X, y, learning_rates, iteraciones=500):\n",
    "    \"\"\"\n",
    "    Prueba diferentes learning rates y retorna resultados\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    \n",
    "    for alpha in learning_rates:\n",
    "        try:\n",
    "            w, b, costos, _ = gradient_descent_mejorado(\n",
    "                X, y, np.zeros(X.shape[1]), 0., alpha, iteraciones, verbose=False\n",
    "            )\n",
    "            \n",
    "            # Verificar si divergi√≥\n",
    "            if np.isnan(costos[-1]) or np.isinf(costos[-1]) or costos[-1] > costos[0] * 10:\n",
    "                status = \"Diverge\"\n",
    "                costo_final = float('inf')\n",
    "            else:\n",
    "                status = \"Converge\"\n",
    "                costo_final = costos[-1]\n",
    "            \n",
    "            resultados.append({\n",
    "                'alpha': alpha,\n",
    "                'costo_final': costo_final,\n",
    "                'status': status,\n",
    "                'costos': costos,\n",
    "                'reduccion': ((costos[0] - costo_final) / costos[0] * 100) if costo_final != float('inf') else 0\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            resultados.append({\n",
    "                'alpha': alpha,\n",
    "                'costo_final': float('inf'),\n",
    "                'status': 'Error',\n",
    "                'costos': [],\n",
    "                'reduccion': 0\n",
    "            })\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Probar rango amplio de learning rates\n",
    "learning_rates_test = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0]\n",
    "\n",
    "print(f\"üß™ EXPERIMENTO: B√∫squeda de Learning Rate √ìptimo\")\n",
    "print(f\"=\" * 55)\n",
    "print(f\"Probando {len(learning_rates_test)} valores de Œ± con datos normalizados...\")\n",
    "\n",
    "resultados_lr = probar_learning_rates(X_norm, y_train, learning_rates_test)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"\\n{'Œ± (Learning Rate)':<15} {'Status':<10} {'Costo Final':<12} {'Reducci√≥n %':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for resultado in resultados_lr:\n",
    "    if resultado['costo_final'] == float('inf'):\n",
    "        costo_str = \"‚àû (Diverge)\"\n",
    "        reduccion_str = \"0.0%\"\n",
    "    else:\n",
    "        costo_str = f\"{resultado['costo_final']:.2f}\"\n",
    "        reduccion_str = f\"{resultado['reduccion']:.1f}%\"\n",
    "    \n",
    "    print(f\"{resultado['alpha']:<15} {resultado['status']:<10} {costo_str:<12} {reduccion_str:<12}\")\n",
    "\n",
    "# Encontrar el mejor learning rate\n",
    "resultados_validos = [r for r in resultados_lr if r['status'] == 'Converge']\n",
    "if resultados_validos:\n",
    "    mejor_resultado = min(resultados_validos, key=lambda x: x['costo_final'])\n",
    "    print(f\"\\nüèÜ Mejor learning rate: Œ± = {mejor_resultado['alpha']}\")\n",
    "    print(f\"   Costo final: {mejor_resultado['costo_final']:.4f}\")\n",
    "    print(f\"   Reducci√≥n: {mejor_resultado['reduccion']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Visualizaci√≥n de Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comportamiento de diferentes learning rates\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Colores para cada learning rate\n",
    "colores = plt.cm.viridis(np.linspace(0, 1, len(learning_rates_test)))\n",
    "\n",
    "# Subplot 1: Evoluci√≥n del costo para diferentes Œ±\n",
    "plt.subplot(2, 2, 1)\n",
    "for i, (resultado, color) in enumerate(zip(resultados_lr, colores)):\n",
    "    if resultado['costos'] and len(resultado['costos']) > 0:\n",
    "        # Limitar costos para visualizaci√≥n\n",
    "        costos_plot = np.array(resultado['costos'])\n",
    "        costos_plot = np.clip(costos_plot, 0, 10000)  # Limitar valores extremos\n",
    "        \n",
    "        plt.plot(costos_plot, color=color, linewidth=2, \n",
    "                label=f\"Œ±={resultado['alpha']:.3f}\" + \n",
    "                      (\" (Diverge)\" if resultado['status'] != 'Converge' else \"\"))\n",
    "\n",
    "plt.title('Evoluci√≥n del Costo vs Learning Rate')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Subplot 2: Costo final vs learning rate\n",
    "plt.subplot(2, 2, 2)\n",
    "alphas_plot = []\n",
    "costos_finales = []\n",
    "colores_status = []\n",
    "\n",
    "for resultado in resultados_lr:\n",
    "    alphas_plot.append(resultado['alpha'])\n",
    "    if resultado['costo_final'] == float('inf'):\n",
    "        costos_finales.append(1000)  # Valor alto para visualizaci√≥n\n",
    "        colores_status.append('red')\n",
    "    else:\n",
    "        costos_finales.append(resultado['costo_final'])\n",
    "        colores_status.append('green')\n",
    "\n",
    "plt.scatter(alphas_plot, costos_finales, c=colores_status, s=100, alpha=0.7)\n",
    "plt.title('Costo Final vs Learning Rate')\n",
    "plt.xlabel('Learning Rate (Œ±)')\n",
    "plt.ylabel('Costo Final')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# A√±adir l√≠nea vertical en el mejor Œ±\n",
    "if resultados_validos:\n",
    "    plt.axvline(x=mejor_resultado['alpha'], color='blue', linestyle='--', \n",
    "                linewidth=2, label=f'Mejor: Œ±={mejor_resultado[\"alpha\"]}')\n",
    "    plt.legend()\n",
    "\n",
    "# Subplot 3: Comparaci√≥n espec√≠fica de algunos Œ± buenos\n",
    "plt.subplot(2, 2, 3)\n",
    "alphas_comparar = [0.01, 0.1, 1.0]  # Seleccionar algunos para comparaci√≥n detallada\n",
    "\n",
    "for alpha in alphas_comparar:\n",
    "    # Encontrar resultado correspondiente\n",
    "    resultado = next((r for r in resultados_lr if r['alpha'] == alpha), None)\n",
    "    if resultado and resultado['costos']:\n",
    "        plt.plot(resultado['costos'][:200], linewidth=2, label=f'Œ± = {alpha}')\n",
    "\n",
    "plt.title('Comparaci√≥n Detallada (Primeras 200 iteraciones)')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 4: Velocidad de convergencia\n",
    "plt.subplot(2, 2, 4)\n",
    "# Calcular iteraciones para alcanzar 95% de convergencia\n",
    "iteraciones_conv = []\n",
    "alphas_conv = []\n",
    "\n",
    "for resultado in resultados_validos:\n",
    "    costos = np.array(resultado['costos'])\n",
    "    if len(costos) > 50:\n",
    "        costo_inicial = costos[0]\n",
    "        costo_final = costos[-1]\n",
    "        umbral_95 = costo_inicial - 0.95 * (costo_inicial - costo_final)\n",
    "        \n",
    "        # Encontrar cu√°ndo alcanza 95% de convergencia\n",
    "        idx_conv = np.where(costos <= umbral_95)[0]\n",
    "        if len(idx_conv) > 0:\n",
    "            iteraciones_conv.append(idx_conv[0])\n",
    "            alphas_conv.append(resultado['alpha'])\n",
    "\n",
    "if iteraciones_conv:\n",
    "    plt.scatter(alphas_conv, iteraciones_conv, s=100, alpha=0.7)\n",
    "    plt.title('Velocidad de Convergencia\\n(Iteraciones para 95% convergencia)')\n",
    "    plt.xlabel('Learning Rate (Œ±)')\n",
    "    plt.ylabel('Iteraciones')\n",
    "    plt.xscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä An√°lisis de Learning Rate:\")\n",
    "print(f\"   üü¢ Œ± muy peque√±o (< 0.01): Converge lento\")\n",
    "print(f\"   üü° Œ± moderado (0.01-0.3): Convergencia √≥ptima\")\n",
    "print(f\"   üî¥ Œ± muy grande (> 1.0): Puede divergir\")\n",
    "print(f\"   üéØ Rango recomendado: 0.01 - 0.3 para datos normalizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. üöÄ Modelo Optimizado Final\n",
    "\n",
    "### 5.1 Entrenar el Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo final con configuraci√≥n √≥ptima\n",
    "print(f\"üöÄ ENTRENAMIENTO FINAL: Modelo Optimizado\")\n",
    "print(f\"=\" * 50)\n",
    "\n",
    "# Configuraci√≥n √≥ptima\n",
    "alpha_optimo = mejor_resultado['alpha'] if 'mejor_resultado' in locals() else 0.1\n",
    "iteraciones_final = 1000\n",
    "\n",
    "print(f\"Configuraci√≥n:\")\n",
    "print(f\"   ‚Ä¢ Feature scaling: Z-score normalization\")\n",
    "print(f\"   ‚Ä¢ Learning rate: Œ± = {alpha_optimo}\")\n",
    "print(f\"   ‚Ä¢ Iteraciones: {iteraciones_final}\")\n",
    "\n",
    "# Entrenar modelo final\n",
    "w_final, b_final, costos_final, params_final = gradient_descent_mejorado(\n",
    "    X_norm, y_train, np.zeros(n), 0., alpha_optimo, iteraciones_final, verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüéâ RESULTADOS FINALES:\")\n",
    "print(f\"   Costo inicial: {costos_final[0]:.4f}\")\n",
    "print(f\"   Costo final:   {costos_final[-1]:.4f}\")\n",
    "print(f\"   Reducci√≥n:     {((costos_final[0] - costos_final[-1])/costos_final[0]*100):.1f}%\")\n",
    "\n",
    "print(f\"\\nüìä Par√°metros del modelo normalizado:\")\n",
    "for i, (feature, peso) in enumerate(zip(feature_names, w_final)):\n",
    "    print(f\"   w[{i}] ({feature}): {peso:8.4f}\")\n",
    "print(f\"   b (bias): {b_final:8.4f}\")\n",
    "\n",
    "# Evaluar modelo\n",
    "predicciones_norm = [np.dot(X_norm[i], w_final) + b_final for i in range(len(X_norm))]\n",
    "residuos = [pred - real for pred, real in zip(predicciones_norm, y_train)]\n",
    "\n",
    "# M√©tricas\n",
    "mae = np.mean([abs(r) for r in residuos])\n",
    "rmse = np.sqrt(np.mean([r**2 for r in residuos]))\n",
    "ss_res = sum([r**2 for r in residuos])\n",
    "ss_tot = sum([(real - np.mean(y_train))**2 for real in y_train])\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"\\nüìà M√©tricas de rendimiento:\")\n",
    "print(f\"   MAE (Error Absoluto Medio):  ${mae:.1f}k\")\n",
    "print(f\"   RMSE (Ra√≠z Error Cuadr√°tico): ${rmse:.1f}k\")\n",
    "print(f\"   R¬≤ (Coef. Determinaci√≥n):     {r2:.4f} ({r2*100:.1f}% varianza explicada)\")\n",
    "\n",
    "if r2 > 0.9:\n",
    "    print(f\"   ‚úÖ Excelente ajuste (R¬≤ > 0.9)\")\n",
    "elif r2 > 0.8:\n",
    "    print(f\"   ‚úÖ Muy buen ajuste (R¬≤ > 0.8)\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Ajuste aceptable pero mejorable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Funci√≥n de Predicci√≥n para Nuevos Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear funci√≥n de predicci√≥n completa que maneje normalizaci√≥n\n",
    "def predecir_precio_casa(tama√±o, habitaciones, pisos, edad, w, b, mu, sigma):\n",
    "    \"\"\"\n",
    "    Predice el precio de una casa usando el modelo entrenado\n",
    "    \n",
    "    Args:\n",
    "        tama√±o: Tama√±o en pies cuadrados\n",
    "        habitaciones: N√∫mero de habitaciones\n",
    "        pisos: N√∫mero de pisos\n",
    "        edad: Edad en a√±os\n",
    "        w, b: Par√°metros del modelo normalizado\n",
    "        mu, sigma: Estad√≠sticas de normalizaci√≥n\n",
    "    \n",
    "    Returns:\n",
    "        precio: Precio predicho en miles de d√≥lares\n",
    "    \"\"\"\n",
    "    # 1. Crear vector de features\n",
    "    x_casa = np.array([tama√±o, habitaciones, pisos, edad])\n",
    "    \n",
    "    # 2. Normalizar usando estad√≠sticas del entrenamiento\n",
    "    x_norm = (x_casa - mu) / sigma\n",
    "    \n",
    "    # 3. Hacer predicci√≥n\n",
    "    precio = np.dot(x_norm, w) + b\n",
    "    \n",
    "    return precio\n",
    "\n",
    "# Funci√≥n para an√°lisis detallado\n",
    "def analisis_prediccion_detallado(tama√±o, habitaciones, pisos, edad, w, b, mu, sigma):\n",
    "    \"\"\"\n",
    "    An√°lisis detallado de una predicci√≥n mostrando cada paso\n",
    "    \"\"\"\n",
    "    print(f\"üîç AN√ÅLISIS DETALLADO DE PREDICCI√ìN\")\n",
    "    print(f\"=\" * 40)\n",
    "    print(f\"Casa: {tama√±o} sqft, {habitaciones} hab, {pisos} pisos, {edad} a√±os\")\n",
    "    \n",
    "    # Paso 1: Features originales\n",
    "    x_original = np.array([tama√±o, habitaciones, pisos, edad])\n",
    "    print(f\"\\n1Ô∏è‚É£ Features originales: {x_original}\")\n",
    "    \n",
    "    # Paso 2: Normalizaci√≥n\n",
    "    x_norm = (x_original - mu) / sigma\n",
    "    print(f\"\\n2Ô∏è‚É£ Normalizaci√≥n (z-score):\")\n",
    "    for i, (feature, orig, norm, m, s) in enumerate(zip(feature_names, x_original, x_norm, mu, sigma)):\n",
    "        print(f\"   {feature}: ({orig:.0f} - {m:.1f}) / {s:.1f} = {norm:.3f}\")\n",
    "    \n",
    "    # Paso 3: C√°lculo de predicci√≥n\n",
    "    print(f\"\\n3Ô∏è‚É£ C√°lculo de predicci√≥n:\")\n",
    "    contribuciones = w * x_norm\n",
    "    for i, (feature, peso, x_n, contrib) in enumerate(zip(feature_names, w, x_norm, contribuciones)):\n",
    "        print(f\"   {feature}: {peso:.4f} √ó {x_n:.3f} = {contrib:.3f}\")\n",
    "    \n",
    "    suma_contrib = np.sum(contribuciones)\n",
    "    precio_final = suma_contrib + b\n",
    "    \n",
    "    print(f\"   Suma de contribuciones: {suma_contrib:.3f}\")\n",
    "    print(f\"   + Bias: {b:.3f}\")\n",
    "    print(f\"   = Precio final: ${precio_final:.0f}k\")\n",
    "    \n",
    "    return precio_final\n",
    "\n",
    "# Ejemplos de predicci√≥n\n",
    "ejemplos_casas = [\n",
    "    (1200, 3, 1, 15, \"Casa familiar t√≠pica\"),\n",
    "    (2500, 4, 2, 5, \"Casa grande y nueva\"),\n",
    "    (800, 2, 1, 40, \"Casa peque√±a, antigua\"),\n",
    "    (3200, 5, 2, 2, \"Mansi√≥n nueva\"),\n",
    "]\n",
    "\n",
    "print(f\"\\nüè† PREDICCIONES PARA CASAS NUEVAS\")\n",
    "print(f\"=\" * 45)\n",
    "print(f\"{'Descripci√≥n':<20} {'Features':<20} {'Precio':<12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for tama√±o, hab, pisos, edad, desc in ejemplos_casas:\n",
    "    precio = predecir_precio_casa(tama√±o, hab, pisos, edad, w_final, b_final, X_mu, X_sigma)\n",
    "    features_str = f\"{tama√±o}sq,{hab}b,{pisos}f,{edad}y\"\n",
    "    print(f\"{desc:<20} {features_str:<20} ${precio:<11.0f}\")\n",
    "\n",
    "# An√°lisis detallado de un ejemplo\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "ejemplo_detalle = ejemplos_casas[0]  # Casa familiar t√≠pica\n",
    "analisis_prediccion_detallado(ejemplo_detalle[0], ejemplo_detalle[1], \n",
    "                             ejemplo_detalle[2], ejemplo_detalle[3], \n",
    "                             w_final, b_final, X_mu, X_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Comparaci√≥n Final: Antes vs Despu√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaci√≥n final completa\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Datos para comparaci√≥n (recrear el entrenamiento sin normalizaci√≥n para comparar)\n",
    "w_sin_norm, b_sin_norm, costos_sin_norm, _ = gradient_descent_mejorado(\n",
    "    X_train, y_train, np.zeros(n), 0., 5e-7, 1000, verbose=False\n",
    ")\n",
    "\n",
    "# Subplot 1: Comparaci√≥n de convergencia\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(costos_sin_norm, 'r-', linewidth=2, label='Sin normalizaci√≥n', alpha=0.8)\n",
    "plt.plot(costos_final, 'g-', linewidth=2, label='Con normalizaci√≥n optimizada', alpha=0.8)\n",
    "plt.title('Comparaci√≥n de Convergencia')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Subplot 2: Predicciones vs valores reales (modelo optimizado)\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.scatter(predicciones_norm, y_train, alpha=0.7, s=60, color='green')\n",
    "min_val = min(min(predicciones_norm), min(y_train))\n",
    "max_val = max(max(predicciones_norm), max(y_train))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfecto')\n",
    "plt.title('Predicciones vs Reales (Optimizado)')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Reales')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Distribuci√≥n de residuos\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(residuos, bins=10, alpha=0.7, edgecolor='black', color='green')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Distribuci√≥n de Residuos')\n",
    "plt.xlabel('Residuos ($k)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 4: Importancia de features (pesos normalizados)\n",
    "plt.subplot(2, 3, 4)\n",
    "importancia = np.abs(w_final)\n",
    "indices_ordenados = np.argsort(importancia)[::-1]\n",
    "colores_bars = ['gold', 'silver', 'chocolate', 'lightblue']\n",
    "\n",
    "bars = plt.bar(range(n), importancia[indices_ordenados], \n",
    "               color=[colores_bars[i] for i in indices_ordenados], \n",
    "               alpha=0.7, edgecolor='black')\n",
    "plt.title('Importancia de Features\\n(Magnitud de Pesos Normalizados)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('|Peso|')\n",
    "plt.xticks(range(n), [feature_names[i].split()[0] for i in indices_ordenados], rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# A√±adir valores en las barras\n",
    "for bar, valor in zip(bars, importancia[indices_ordenados]):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "             f'{valor:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Subplot 5: Comparaci√≥n de m√©tricas\n",
    "plt.subplot(2, 3, 5)\n",
    "# Calcular m√©tricas del modelo sin normalizar\n",
    "pred_sin_norm = [np.dot(X_train[i], w_sin_norm) + b_sin_norm for i in range(len(X_train))]\n",
    "residuos_sin_norm = [pred - real for pred, real in zip(pred_sin_norm, y_train)]\n",
    "mae_sin_norm = np.mean([abs(r) for r in residuos_sin_norm])\n",
    "rmse_sin_norm = np.sqrt(np.mean([r**2 for r in residuos_sin_norm]))\n",
    "\n",
    "metricas_nombres = ['MAE ($k)', 'RMSE ($k)']\n",
    "sin_norm_vals = [mae_sin_norm, rmse_sin_norm]\n",
    "con_norm_vals = [mae, rmse]\n",
    "\n",
    "x_pos = np.arange(len(metricas_nombres))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x_pos - width/2, sin_norm_vals, width, label='Sin normalizaci√≥n', \n",
    "        color='red', alpha=0.7, edgecolor='black')\n",
    "plt.bar(x_pos + width/2, con_norm_vals, width, label='Con normalizaci√≥n', \n",
    "        color='green', alpha=0.7, edgecolor='black')\n",
    "\n",
    "plt.title('Comparaci√≥n de M√©tricas')\n",
    "plt.xlabel('M√©trica')\n",
    "plt.ylabel('Valor')\n",
    "plt.xticks(x_pos, metricas_nombres)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Subplot 6: Resumen de mejoras\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.text(0.1, 0.9, 'üéØ MEJORAS LOGRADAS', fontsize=14, fontweight='bold', \n",
    "         transform=plt.gca().transAxes)\n",
    "\n",
    "mejoras_texto = f\"\"\"\n",
    "‚ö° Learning Rate:\n",
    "   ‚Ä¢ Antes: 5.0e-7\n",
    "   ‚Ä¢ Despu√©s: {alpha_optimo}\n",
    "   ‚Ä¢ Mejora: {alpha_optimo/5e-7:.0f}x m√°s r√°pido\n",
    "\n",
    "üìä Convergencia:\n",
    "   ‚Ä¢ M√°s estable y predecible\n",
    "   ‚Ä¢ Sin oscilaciones\n",
    "   ‚Ä¢ Gradientes balanceados\n",
    "\n",
    "üéØ Rendimiento:\n",
    "   ‚Ä¢ MAE: {mae_sin_norm:.1f} ‚Üí {mae:.1f}\n",
    "   ‚Ä¢ RMSE: {rmse_sin_norm:.1f} ‚Üí {rmse:.1f}\n",
    "   ‚Ä¢ R¬≤: {r2:.3f} ({r2*100:.1f}%)\n",
    "\n",
    "‚úÖ Resultado:\n",
    "   Modelo robusto y eficiente\n",
    "   para producci√≥n\n",
    "\"\"\"\n",
    "\n",
    "plt.text(0.05, 0.8, mejoras_texto, fontsize=10, \n",
    "         transform=plt.gca().transAxes, verticalalignment='top')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüéâ RESUMEN FINAL\")\n",
    "print(f\"=\" * 30)\n",
    "print(f\"‚úÖ Feature scaling implementado exitosamente\")\n",
    "print(f\"‚úÖ Learning rate optimizado: {alpha_optimo/5e-7:.0f}x m√°s r√°pido\")\n",
    "print(f\"‚úÖ Modelo con R¬≤ = {r2:.3f} ({r2*100:.1f}% de varianza explicada)\")\n",
    "print(f\"‚úÖ Convergencia estable y predecible\")\n",
    "print(f\"‚úÖ Listo para hacer predicciones en nuevos datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. üß™ Ejercicios Pr√°cticos\n",
    "\n",
    "### Ejercicio 1: Implementar Diferentes T√©cnicas de Normalizaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 1: Comparar t√©cnicas de normalizaci√≥n\n",
    "print(f\"üß™ EJERCICIO 1: Comparaci√≥n de T√©cnicas de Normalizaci√≥n\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "def robust_normalize_features(X):\n",
    "    \"\"\"\n",
    "    Normalizaci√≥n robusta usando mediana y rango intercuartil\n",
    "    Menos sensible a outliers que Z-score\n",
    "    \"\"\"\n",
    "    median = np.median(X, axis=0)\n",
    "    q75 = np.percentile(X, 75, axis=0)\n",
    "    q25 = np.percentile(X, 25, axis=0)\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    # Evitar divisi√≥n por cero\n",
    "    iqr[iqr == 0] = 1\n",
    "    \n",
    "    X_robust = (X - median) / iqr\n",
    "    return X_robust, median, iqr\n",
    "\n",
    "def unit_vector_normalize(X):\n",
    "    \"\"\"\n",
    "    Normalizaci√≥n de vector unitario (L2 normalization)\n",
    "    Cada ejemplo tiene norma = 1\n",
    "    \"\"\"\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1  # Evitar divisi√≥n por cero\n",
    "    return X / norms\n",
    "\n",
    "# Aplicar todas las t√©cnicas\n",
    "tecnicas = {\n",
    "    'Original': (X_train, None, None),\n",
    "    'Z-Score': zscore_normalize_features(X_train),\n",
    "    'Min-Max': minmax_normalize_features(X_train),\n",
    "    'Robust': robust_normalize_features(X_train),\n",
    "    'Unit Vector': (unit_vector_normalize(X_train), None, None)\n",
    "}\n",
    "\n",
    "# Entrenar modelo con cada t√©cnica\n",
    "resultados_tecnicas = {}\n",
    "\n",
    "for nombre, (X_norm_tech, _, _) in tecnicas.items():\n",
    "    if nombre == 'Original':\n",
    "        alpha_tech = 5e-7  # Learning rate peque√±o para datos originales\n",
    "    else:\n",
    "        alpha_tech = 0.1   # Learning rate grande para datos normalizados\n",
    "    \n",
    "    try:\n",
    "        w_tech, b_tech, costos_tech, _ = gradient_descent_mejorado(\n",
    "            X_norm_tech, y_train, np.zeros(n), 0., alpha_tech, 500, verbose=False\n",
    "        )\n",
    "        \n",
    "        # M√©tricas\n",
    "        pred_tech = [np.dot(X_norm_tech[i], w_tech) + b_tech for i in range(len(X_norm_tech))]\n",
    "        mae_tech = np.mean([abs(pred - real) for pred, real in zip(pred_tech, y_train)])\n",
    "        \n",
    "        resultados_tecnicas[nombre] = {\n",
    "            'costo_final': costos_tech[-1],\n",
    "            'mae': mae_tech,\n",
    "            'convergencia': len([c for c in costos_tech if not np.isnan(c)]),\n",
    "            'alpha': alpha_tech\n",
    "        }\n",
    "    except:\n",
    "        resultados_tecnicas[nombre] = {\n",
    "            'costo_final': float('inf'),\n",
    "            'mae': float('inf'),\n",
    "            'convergencia': 0,\n",
    "            'alpha': alpha_tech\n",
    "        }\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"{'T√©cnica':<15} {'Œ±':<10} {'Costo Final':<12} {'MAE':<8} {'Convergencia':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for nombre, resultado in resultados_tecnicas.items():\n",
    "    costo_str = f\"{resultado['costo_final']:.2f}\" if resultado['costo_final'] != float('inf') else \"Fall√≥\"\n",
    "    mae_str = f\"{resultado['mae']:.1f}\" if resultado['mae'] != float('inf') else \"N/A\"\n",
    "    \n",
    "    print(f\"{nombre:<15} {resultado['alpha']:<10.0e} {costo_str:<12} {mae_str:<8} {'‚úÖ' if resultado['convergencia'] > 400 else '‚ùå':<12}\")\n",
    "\n",
    "# Encontrar la mejor t√©cnica\n",
    "tecnicas_validas = {k: v for k, v in resultados_tecnicas.items() if v['mae'] != float('inf')}\n",
    "if tecnicas_validas:\n",
    "    mejor_tecnica = min(tecnicas_validas, key=lambda x: tecnicas_validas[x]['mae'])\n",
    "    print(f\"\\nüèÜ Mejor t√©cnica: {mejor_tecnica}\")\n",
    "    print(f\"   MAE: {tecnicas_validas[mejor_tecnica]['mae']:.1f}\")\n",
    "    print(f\"   Permite Œ± = {tecnicas_validas[mejor_tecnica]['alpha']:.0e}\")\n",
    "\n",
    "print(f\"\\nüí° Conclusiones:\")\n",
    "print(f\"   ‚Ä¢ Z-Score y Min-Max suelen ser las mejores opciones\")\n",
    "print(f\"   ‚Ä¢ Robust normalization es √∫til con outliers\")\n",
    "print(f\"   ‚Ä¢ Unit vector es espec√≠fica para ciertos casos\")\n",
    "print(f\"   ‚Ä¢ Normalizaci√≥n permite learning rates 10,000x m√°s grandes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: An√°lisis de Sensibilidad del Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 2: B√∫squeda sistem√°tica de learning rate √≥ptimo\n",
    "print(f\"üß™ EJERCICIO 2: B√∫squeda Sistem√°tica de Learning Rate\")\n",
    "print(f\"=\" * 55)\n",
    "\n",
    "def busqueda_learning_rate_detallada(X, y, alpha_min=1e-4, alpha_max=10, num_puntos=20):\n",
    "    \"\"\"\n",
    "    B√∫squeda sistem√°tica de learning rate con an√°lisis detallado\n",
    "    \"\"\"\n",
    "    # Generar rango logar√≠tmico de learning rates\n",
    "    alphas = np.logspace(np.log10(alpha_min), np.log10(alpha_max), num_puntos)\n",
    "    \n",
    "    resultados_detallados = []\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        try:\n",
    "            # Entrenamiento corto para evaluaci√≥n r√°pida\n",
    "            w, b, costos, _ = gradient_descent_mejorado(\n",
    "                X, y, np.zeros(X.shape[1]), 0., alpha, 200, verbose=False\n",
    "            )\n",
    "            \n",
    "            # An√°lisis de convergencia\n",
    "            if len(costos) < 50:\n",
    "                status = \"Error temprano\"\n",
    "                velocidad_conv = 0\n",
    "                estabilidad = 0\n",
    "            elif np.isnan(costos[-1]) or np.isinf(costos[-1]):\n",
    "                status = \"Divergencia (NaN/Inf)\"\n",
    "                velocidad_conv = 0\n",
    "                estabilidad = 0\n",
    "            elif costos[-1] > costos[0] * 2:\n",
    "                status = \"Divergencia (Costo aumenta)\"\n",
    "                velocidad_conv = 0\n",
    "                estabilidad = 0\n",
    "            else:\n",
    "                status = \"Convergencia\"\n",
    "                # Velocidad: qu√© tan r√°pido baja el costo en las primeras iteraciones\n",
    "                if len(costos) >= 50:\n",
    "                    velocidad_conv = (costos[0] - costos[49]) / costos[0]\n",
    "                else:\n",
    "                    velocidad_conv = (costos[0] - costos[-1]) / costos[0]\n",
    "                \n",
    "                # Estabilidad: variaci√≥n en las √∫ltimas iteraciones\n",
    "                ultimos_costos = costos[-20:] if len(costos) >= 20 else costos\n",
    "                estabilidad = 1 / (1 + np.std(ultimos_costos))\n",
    "            \n",
    "            resultados_detallados.append({\n",
    "                'alpha': alpha,\n",
    "                'status': status,\n",
    "                'costo_final': costos[-1] if costos and not np.isnan(costos[-1]) and not np.isinf(costos[-1]) else float('inf'),\n",
    "                'velocidad_convergencia': velocidad_conv,\n",
    "                'estabilidad': estabilidad,\n",
    "                'score_total': velocidad_conv * estabilidad  # M√©trica combinada\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            resultados_detallados.append({\n",
    "                'alpha': alpha,\n",
    "                'status': f\"Error: {str(e)[:20]}\",\n",
    "                'costo_final': float('inf'),\n",
    "                'velocidad_convergencia': 0,\n",
    "                'estabilidad': 0,\n",
    "                'score_total': 0\n",
    "            })\n",
    "    \n",
    "    return resultados_detallados\n",
    "\n",
    "# Realizar b√∫squeda detallada\n",
    "print(\"Realizando b√∫squeda sistem√°tica...\")\n",
    "resultados_lr_detallados = busqueda_learning_rate_detallada(X_norm, y_train)\n",
    "\n",
    "# Mostrar resultados tabulados\n",
    "print(f\"\\n{'Œ±':<10} {'Status':<15} {'Costo Final':<12} {'Velocidad':<10} {'Estabilidad':<12} {'Score':<8}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for resultado in resultados_lr_detallados[:10]:  # Mostrar primeros 10\n",
    "    costo_str = f\"{resultado['costo_final']:.2f}\" if resultado['costo_final'] != float('inf') else \"‚àû\"\n",
    "    print(f\"{resultado['alpha']:<10.1e} {resultado['status']:<15} {costo_str:<12} \"\n",
    "          f\"{resultado['velocidad_convergencia']:<10.3f} {resultado['estabilidad']:<12.3f} \"\n",
    "          f\"{resultado['score_total']:<8.3f}\")\n",
    "\n",
    "# Encontrar el mejor seg√∫n score combinado\n",
    "resultados_validos_det = [r for r in resultados_lr_detallados if r['status'] == 'Convergencia']\n",
    "if resultados_validos_det:\n",
    "    mejor_lr_detallado = max(resultados_validos_det, key=lambda x: x['score_total'])\n",
    "    print(f\"\\nüéØ Learning rate √≥ptimo (an√°lisis detallado):\")\n",
    "    print(f\"   Œ± = {mejor_lr_detallado['alpha']:.3f}\")\n",
    "    print(f\"   Score total: {mejor_lr_detallado['score_total']:.3f}\")\n",
    "    print(f\"   Velocidad de convergencia: {mejor_lr_detallado['velocidad_convergencia']:.3f}\")\n",
    "    print(f\"   Estabilidad: {mejor_lr_detallado['estabilidad']:.3f}\")\n",
    "\n",
    "# Visualizaci√≥n de la b√∫squeda\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "alphas_plot = [r['alpha'] for r in resultados_lr_detallados]\n",
    "scores_plot = [r['score_total'] for r in resultados_lr_detallados]\n",
    "colores_status = ['green' if r['status'] == 'Convergencia' else 'red' for r in resultados_lr_detallados]\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(alphas_plot, scores_plot, c=colores_status, alpha=0.7, s=50)\n",
    "plt.xscale('log')\n",
    "plt.title('Score de Learning Rate vs Œ±')\n",
    "plt.xlabel('Learning Rate (Œ±)')\n",
    "plt.ylabel('Score (Velocidad √ó Estabilidad)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Marcar el mejor\n",
    "if resultados_validos_det:\n",
    "    plt.scatter([mejor_lr_detallado['alpha']], [mejor_lr_detallado['score_total']], \n",
    "               c='blue', s=200, marker='*', edgecolor='black', linewidth=2,\n",
    "               label=f\"√ìptimo: Œ±={mejor_lr_detallado['alpha']:.3f}\")\n",
    "    plt.legend()\n",
    "\n",
    "# Heatmap de regiones\n",
    "plt.subplot(1, 2, 2)\n",
    "velocidades = [r['velocidad_convergencia'] for r in resultados_lr_detallados]\n",
    "estabilidades = [r['estabilidad'] for r in resultados_lr_detallados]\n",
    "\n",
    "plt.scatter(velocidades, estabilidades, c=scores_plot, s=100, alpha=0.7, cmap='viridis')\n",
    "plt.colorbar(label='Score Total')\n",
    "plt.title('Velocidad vs Estabilidad')\n",
    "plt.xlabel('Velocidad de Convergencia')\n",
    "plt.ylabel('Estabilidad')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Conclusiones del an√°lisis sistem√°tico:\")\n",
    "print(f\"   ‚Ä¢ Zona √≥ptima: Œ± entre 0.03 y 0.3\")\n",
    "print(f\"   ‚Ä¢ Balance ideal entre velocidad y estabilidad\")\n",
    "print(f\"   ‚Ä¢ Evitar Œ± < 0.001 (muy lento) y Œ± > 1.0 (inestable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Resumen y Conceptos Clave\n",
    "\n",
    "### ‚úÖ Lo que has aprendido:\n",
    "\n",
    "#### 1. **Problem Diagnosis**:\n",
    "   - **Features con escalas muy diferentes** causan convergencia lenta\n",
    "   - **Gradientes desbalanceados** requieren learning rates muy peque√±os\n",
    "   - **Superficies de costo el√≠pticas** vs **circulares**\n",
    "\n",
    "#### 2. **Feature Scaling Techniques**:\n",
    "   - **Z-Score**: $(x - \\mu) / \\sigma$ ‚Üí Media=0, Std=1\n",
    "   - **Min-Max**: $(x - x_{min}) / (x_{max} - x_{min})$ ‚Üí Rango [0,1]\n",
    "   - **Robust**: $(x - median) / IQR$ ‚Üí Resistente a outliers\n",
    "   - **Unit Vector**: $x / ||x||$ ‚Üí Norma = 1\n",
    "\n",
    "#### 3. **Impact on Gradient Descent**:\n",
    "   - **Learning rate**: 10,000x m√°s grande con normalizaci√≥n\n",
    "   - **Convergencia**: M√°s r√°pida, estable y predecible\n",
    "   - **Gradientes**: Magnitudes similares entre features\n",
    "\n",
    "#### 4. **Learning Rate Optimization**:\n",
    "   - **Systematic search**: Explorar rango logar√≠tmico\n",
    "   - **Balance**: Velocidad vs estabilidad\n",
    "   - **Zona √≥ptima**: 0.01 - 0.3 para datos normalizados\n",
    "\n",
    "#### 5. **Production Ready Model**:\n",
    "   - **Normalizaci√≥n**: Aplicar a nuevos datos usando estad√≠sticas del entrenamiento\n",
    "   - **Pipeline completo**: Normalizar ‚Üí Predecir ‚Üí Desnormalizar (si necesario)\n",
    "   - **M√©tricas**: R¬≤ = {r2:.3f} ({r2*100:.1f}% varianza explicada)\n",
    "\n",
    "### üöÄ Mejoras Logradas:\n",
    "- **Velocidad**: {alpha_optimo/5e-7:.0f}x m√°s r√°pido\n",
    "- **Estabilidad**: Sin oscilaciones ni divergencia\n",
    "- **Rendimiento**: MAE reducido, R¬≤ mejorado\n",
    "- **Robustez**: Modelo listo para producci√≥n\n",
    "\n",
    "### üéØ Pr√≥ximos pasos:\n",
    "- **Feature Engineering**: Crear features polinomiales y transformaciones\n",
    "- **Regularizaci√≥n**: L1/L2 para prevenir overfitting\n",
    "- **Advanced Optimization**: Adam, RMSprop, momentum\n",
    "- **Cross-validation**: Validaci√≥n robusta del modelo\n",
    "\n",
    "### üí° Best Practices:\n",
    "1. **Siempre normalizar** cuando features tienen escalas diferentes\n",
    "2. **Guardar estad√≠sticas** (Œº, œÉ) del conjunto de entrenamiento\n",
    "3. **Aplicar misma normalizaci√≥n** a datos nuevos\n",
    "4. **Experimentar con learning rates** en rango logar√≠tmico\n",
    "5. **Monitorear convergencia** con m√∫ltiples m√©tricas\n",
    "6. **Validar en datos de prueba** antes de producci√≥n\n",
    "\n",
    "### ‚ö†Ô∏è Cuidados importantes:\n",
    "- **Nunca normalizar** usando estad√≠sticas de datos de prueba\n",
    "- **Feature scaling no siempre mejora** todos los algoritmos\n",
    "- **Learning rate muy alto** puede causar divergencia\n",
    "- **Interpretar pesos** de modelo normalizado requiere cuidado\n",
    "\n",
    "**Este notebook te ha dado las herramientas fundamentales para optimizar gradient descent y crear modelos de ML robustos y eficientes.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}