{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📏 Feature Scaling y Optimización del Learning Rate\n",
    "\n",
    "## 📚 Objetivos de Aprendizaje\n",
    "En este notebook aprenderás:\n",
    "- **Por qué** el feature scaling es crucial para el gradient descent\n",
    "- **Cómo** implementar diferentes técnicas de normalización\n",
    "- **Cuándo** y **cómo** ajustar el learning rate\n",
    "- **Técnicas avanzadas** para acelerar la convergencia\n",
    "\n",
    "## 🔍 Problema Identificado\n",
    "En el notebook anterior observamos:\n",
    "- **Learning rate muy pequeño** (5.0e-7) necesario para evitar divergencia\n",
    "- **Convergencia lenta** debido a diferentes escalas de features\n",
    "- **Gradientes desbalanceados** entre diferentes parámetros\n",
    "\n",
    "**Solución**: Feature Scaling + Learning Rate Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "\n",
    "# Configuración\n",
    "plt.style.use('default')\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "\n",
    "print(\"✅ Librerías importadas\")\n",
    "print(\"🎯 Listo para optimizar gradient descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 📊 Análisis del Problema: Escalas Desbalanceadas\n",
    "\n",
    "### 1.1 Cargar Dataset y Analizar Escalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el mismo dataset de casas\n",
    "def cargar_datos_casas():\n",
    "    X_train = np.array([\n",
    "        [2104, 5, 1, 45],    # Casa 1: [tamaño_sqft, habitaciones, pisos, edad]\n",
    "        [1416, 3, 2, 40],    # Casa 2  \n",
    "        [852,  2, 1, 35],    # Casa 3\n",
    "        [1940, 4, 1, 10],    # Casa 4\n",
    "        [1539, 3, 2, 25],    # Casa 5\n",
    "        [3000, 4, 2, 8],     # Casa 6\n",
    "        [1230, 3, 1, 15],    # Casa 7\n",
    "        [2145, 4, 1, 12],    # Casa 8\n",
    "        [1736, 3, 2, 30],    # Casa 9\n",
    "        [1000, 2, 1, 50],    # Casa 10\n",
    "        [1940, 4, 3, 5],     # Casa 11 (nueva)\n",
    "        [1100, 2, 1, 60]     # Casa 12 (nueva)\n",
    "    ])\n",
    "    \n",
    "    y_train = np.array([460, 232, 178, 500, 315, 740, 285, 510, 390, 180, 550, 165])\n",
    "    feature_names = ['Tamaño (sqft)', 'Habitaciones', 'Pisos', 'Edad (años)']\n",
    "    \n",
    "    return X_train, y_train, feature_names\n",
    "\n",
    "X_train, y_train, feature_names = cargar_datos_casas()\n",
    "m, n = X_train.shape\n",
    "\n",
    "print(f\"📊 Dataset actualizado:\")\n",
    "print(f\"   • Ejemplos (m): {m}\")\n",
    "print(f\"   • Features (n): {n}\")\n",
    "\n",
    "# Análisis detallado de escalas\n",
    "print(f\"\\n📏 Análisis de Escalas por Feature:\")\n",
    "print(f\"{'Feature':<15} {'Min':<8} {'Max':<8} {'Media':<8} {'Std':<8} {'Rango':<10}\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    columna = X_train[:, i]\n",
    "    min_val = np.min(columna)\n",
    "    max_val = np.max(columna)\n",
    "    media = np.mean(columna)\n",
    "    std = np.std(columna)\n",
    "    rango = max_val - min_val\n",
    "    \n",
    "    print(f\"{feature:<15} {min_val:<8.0f} {max_val:<8.0f} {media:<8.1f} {std:<8.1f} {rango:<10.0f}\")\n",
    "\n",
    "print(f\"\\n🔍 Observaciones críticas:\")\n",
    "print(f\"   • Tamaño: Rango ~2000 (852-3000)\")\n",
    "print(f\"   • Habitaciones: Rango ~3 (2-5)\")\n",
    "print(f\"   • Pisos: Rango ~2 (1-3)\")\n",
    "print(f\"   • Edad: Rango ~55 (5-60)\")\n",
    "print(f\"\\n⚠️  PROBLEMA: Tamaño domina numéricamente sobre otras features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Visualización del Problema de Escalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar el problema de escalas\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Box plots para mostrar diferencias de escala\n",
    "plt.subplot(2, 3, 1)\n",
    "box_data = [X_train[:, i] for i in range(n)]\n",
    "plt.boxplot(box_data, labels=[name.split()[0] for name in feature_names])\n",
    "plt.title('Distribución de Features (Escala Original)')\n",
    "plt.ylabel('Valores')\n",
    "plt.yscale('log')  # Escala logarítmica para ver todas las features\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Histogramas individuales\n",
    "for i in range(n):\n",
    "    plt.subplot(2, 3, i+2)\n",
    "    plt.hist(X_train[:, i], bins=8, alpha=0.7, edgecolor='black')\n",
    "    plt.title(f'{feature_names[i]}')\n",
    "    plt.xlabel('Valor')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Demonstrar el impacto en los gradientes\n",
    "def calcular_gradientes_ejemplo(X, y, w, b):\n",
    "    \"\"\"Función simplificada para demostrar gradientes\"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0.\n",
    "    \n",
    "    for i in range(m):\n",
    "        error = (np.dot(X[i], w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += error * X[i, j]\n",
    "        dj_db += error\n",
    "    \n",
    "    return dj_dw / m, dj_db / m\n",
    "\n",
    "# Calcular gradientes iniciales\n",
    "w_test = np.array([0.1, 10, -20, -1])  # Valores ejemplo\n",
    "b_test = 100\n",
    "\n",
    "gradientes, grad_b = calcular_gradientes_ejemplo(X_train, y_train, w_test, b_test)\n",
    "\n",
    "print(f\"\\n🧮 Impacto en Gradientes (sin normalización):\")\n",
    "print(f\"{'Feature':<15} {'Gradiente':<12} {'Magnitud':<10}\")\n",
    "print(\"-\" * 40)\n",
    "for i, (feature, grad) in enumerate(zip(feature_names, gradientes)):\n",
    "    print(f\"{feature:<15} {grad:<12.2e} {abs(grad):<10.2e}\")\n",
    "\n",
    "print(f\"\\n📊 Ratio de magnitudes:\")\n",
    "grad_abs = np.abs(gradientes)\n",
    "ratio_max_min = np.max(grad_abs) / np.min(grad_abs[grad_abs > 0])\n",
    "print(f\"   Máximo/Mínimo: {ratio_max_min:.1f}x de diferencia\")\n",
    "print(f\"   Esto causa convergencia desigual y requiere learning rate muy pequeño\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 🎯 Técnicas de Feature Scaling\n",
    "\n",
    "### 2.1 Z-Score Normalization (Standardization)\n",
    "\n",
    "**Fórmula**: $x_{norm} = \\frac{x - \\mu}{\\sigma}$\n",
    "\n",
    "**Resultado**: Media = 0, Desviación estándar = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de Z-Score Normalization\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    Aplica normalización Z-score a las features\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Matriz de features (m, n)\n",
    "        \n",
    "    Returns:\n",
    "        X_norm (ndarray): Features normalizadas\n",
    "        mu (ndarray): Media de cada feature\n",
    "        sigma (ndarray): Desviación estándar de cada feature\n",
    "    \"\"\"\n",
    "    # Calcular estadísticas\n",
    "    mu = np.mean(X, axis=0)      # Media por columna\n",
    "    sigma = np.std(X, axis=0)    # Desviación estándar por columna\n",
    "    \n",
    "    # Normalizar\n",
    "    X_norm = (X - mu) / sigma\n",
    "    \n",
    "    return X_norm, mu, sigma\n",
    "\n",
    "# Aplicar Z-score normalization\n",
    "X_norm, X_mu, X_sigma = zscore_normalize_features(X_train)\n",
    "\n",
    "print(f\"📊 Estadísticas antes de normalización:\")\n",
    "print(f\"   Media por feature: {X_mu}\")\n",
    "print(f\"   Std por feature:   {X_sigma}\")\n",
    "\n",
    "print(f\"\\n📊 Estadísticas después de normalización:\")\n",
    "print(f\"   Media por feature: {np.mean(X_norm, axis=0)}\")\n",
    "print(f\"   Std por feature:   {np.std(X_norm, axis=0)}\")\n",
    "\n",
    "print(f\"\\n📏 Comparación de rangos:\")\n",
    "print(f\"{'Feature':<15} {'Rango Original':<15} {'Rango Normalizado':<18}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, feature in enumerate(feature_names):\n",
    "    rango_original = np.ptp(X_train[:, i])  # peak-to-peak\n",
    "    rango_norm = np.ptp(X_norm[:, i])\n",
    "    print(f\"{feature:<15} {rango_original:<15.0f} {rango_norm:<18.2f}\")\n",
    "\n",
    "print(f\"\\n✅ Normalización exitosa: Todas las features ahora tienen escala similar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Min-Max Normalization\n",
    "\n",
    "**Fórmula**: $x_{norm} = \\frac{x - x_{min}}{x_{max} - x_{min}}$\n",
    "\n",
    "**Resultado**: Valores entre 0 y 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de Min-Max Normalization\n",
    "\n",
    "def minmax_normalize_features(X):\n",
    "    \"\"\"\n",
    "    Aplica normalización Min-Max a las features\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray): Matriz de features (m, n)\n",
    "        \n",
    "    Returns:\n",
    "        X_norm (ndarray): Features normalizadas [0, 1]\n",
    "        X_min (ndarray): Valor mínimo de cada feature\n",
    "        X_max (ndarray): Valor máximo de cada feature\n",
    "    \"\"\"\n",
    "    X_min = np.min(X, axis=0)\n",
    "    X_max = np.max(X, axis=0)\n",
    "    \n",
    "    # Evitar división por cero\n",
    "    X_range = X_max - X_min\n",
    "    X_range[X_range == 0] = 1  # Si max == min, mantener valor original\n",
    "    \n",
    "    X_norm = (X - X_min) / X_range\n",
    "    \n",
    "    return X_norm, X_min, X_max\n",
    "\n",
    "# Aplicar Min-Max normalization\n",
    "X_minmax, X_min, X_max = minmax_normalize_features(X_train)\n",
    "\n",
    "print(f\"📊 Min-Max Normalization:\")\n",
    "print(f\"   Valores mínimos: {X_min}\")\n",
    "print(f\"   Valores máximos: {X_max}\")\n",
    "\n",
    "print(f\"\\n📊 Estadísticas después de Min-Max:\")\n",
    "print(f\"   Min por feature: {np.min(X_minmax, axis=0)}\")\n",
    "print(f\"   Max por feature: {np.max(X_minmax, axis=0)}\")\n",
    "print(f\"   Media por feature: {np.mean(X_minmax, axis=0)}\")\n",
    "\n",
    "# Comparar ambas técnicas visualmente\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original\n",
    "plt.subplot(1, 3, 1)\n",
    "for i in range(n):\n",
    "    plt.scatter([i] * len(X_train), X_train[:, i], alpha=0.6, s=50)\n",
    "plt.title('Features Originales')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Valor')\n",
    "plt.yscale('log')\n",
    "plt.xticks(range(n), [f.split()[0] for f in feature_names])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Z-Score\n",
    "plt.subplot(1, 3, 2)\n",
    "for i in range(n):\n",
    "    plt.scatter([i] * len(X_norm), X_norm[:, i], alpha=0.6, s=50)\n",
    "plt.title('Z-Score Normalization')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Valor Normalizado')\n",
    "plt.xticks(range(n), [f.split()[0] for f in feature_names])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Min-Max\n",
    "plt.subplot(1, 3, 3)\n",
    "for i in range(n):\n",
    "    plt.scatter([i] * len(X_minmax), X_minmax[:, i], alpha=0.6, s=50)\n",
    "plt.title('Min-Max Normalization')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Valor Normalizado [0,1]')\n",
    "plt.xticks(range(n), [f.split()[0] for f in feature_names])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🤔 ¿Cuál usar?\")\n",
    "print(f\"   • Z-Score: Mejor para distribuciones normales, no acotado\")\n",
    "print(f\"   • Min-Max: Mejor cuando necesitas rango específico [0,1]\")\n",
    "print(f\"   • Para ML: Z-Score es más común y robusto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ⚡ Impacto en Gradient Descent\n",
    "\n",
    "### 3.1 Comparación: Con vs Sin Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares (copiadas del notebook anterior)\n",
    "def calcular_costo_multivariable(X, y, w, b):\n",
    "    m = X.shape[0]\n",
    "    costo = 0.0\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b\n",
    "        costo += (f_wb_i - y[i]) ** 2\n",
    "    return costo / (2 * m)\n",
    "\n",
    "def calcular_gradientes_multivariable(X, y, w, b):\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(n)\n",
    "    dj_db = 0.\n",
    "    \n",
    "    for i in range(m):\n",
    "        error = (np.dot(X[i], w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] += error * X[i, j]\n",
    "        dj_db += error\n",
    "    \n",
    "    return dj_dw / m, dj_db / m\n",
    "\n",
    "def gradient_descent_mejorado(X, y, w_inicial, b_inicial, alpha, num_iteraciones, verbose=True):\n",
    "    \"\"\"Gradient descent con tracking mejorado\"\"\"\n",
    "    w = copy.deepcopy(w_inicial)\n",
    "    b = b_inicial\n",
    "    historial_costos = []\n",
    "    historial_params = []\n",
    "    \n",
    "    for i in range(num_iteraciones):\n",
    "        # Calcular costo y gradientes\n",
    "        costo = calcular_costo_multivariable(X, y, w, b)\n",
    "        dj_dw, dj_db = calcular_gradientes_multivariable(X, y, w, b)\n",
    "        \n",
    "        # Actualizar parámetros\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        \n",
    "        # Guardar historia\n",
    "        historial_costos.append(costo)\n",
    "        historial_params.append((w.copy(), b))\n",
    "        \n",
    "        # Imprimir progreso\n",
    "        if verbose and (i % (num_iteraciones // 10) == 0 or i < 10):\n",
    "            print(f\"Iter {i:4d}: Costo = {costo:8.4f}, |dj_dw| = {np.linalg.norm(dj_dw):.2e}\")\n",
    "    \n",
    "    return w, b, historial_costos, historial_params\n",
    "\n",
    "# Experimento: Comparar convergencia con y sin normalización\n",
    "print(f\"🧪 EXPERIMENTO: Convergencia Con vs Sin Feature Scaling\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# Parámetros del experimento\n",
    "w_inicial = np.zeros(n)\n",
    "b_inicial = 0.\n",
    "iteraciones = 1000\n",
    "\n",
    "# Caso 1: Sin normalización (learning rate pequeño)\n",
    "print(f\"\\n📉 Caso 1: SIN normalización (α = 5e-7)\")\n",
    "alpha_sin_norm = 5.0e-7\n",
    "w1, b1, costos1, params1 = gradient_descent_mejorado(\n",
    "    X_train, y_train, w_inicial, b_inicial, alpha_sin_norm, iteraciones, verbose=False\n",
    ")\n",
    "\n",
    "print(f\"   Costo inicial: {costos1[0]:.2f}\")\n",
    "print(f\"   Costo final:   {costos1[-1]:.2f}\")\n",
    "print(f\"   Reducción:     {((costos1[0] - costos1[-1])/costos1[0]*100):.1f}%\")\n",
    "\n",
    "# Caso 2: Con normalización (learning rate grande)\n",
    "print(f\"\\n📈 Caso 2: CON normalización (α = 0.1)\")\n",
    "alpha_con_norm = 0.1\n",
    "w2, b2, costos2, params2 = gradient_descent_mejorado(\n",
    "    X_norm, y_train, w_inicial, b_inicial, alpha_con_norm, iteraciones, verbose=False\n",
    ")\n",
    "\n",
    "print(f\"   Costo inicial: {costos2[0]:.2f}\")\n",
    "print(f\"   Costo final:   {costos2[-1]:.2f}\")\n",
    "print(f\"   Reducción:     {((costos2[0] - costos2[-1])/costos2[0]*100):.1f}%\")\n",
    "\n",
    "# Comparación de velocidades\n",
    "mejora_velocidad = alpha_con_norm / alpha_sin_norm\n",
    "print(f\"\\n⚡ COMPARACIÓN:\")\n",
    "print(f\"   Learning rate con normalización: {mejora_velocidad:.0f}x más grande\")\n",
    "print(f\"   Convergencia: Mucho más rápida y estable\")\n",
    "print(f\"   Costo final similar: {abs(costos1[-1] - costos2[-1]):.2f} diferencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Visualización de la Convergencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparación de convergencia\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Evolución del costo (escala lineal)\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(costos1, 'r-', linewidth=2, label='Sin normalización', alpha=0.8)\n",
    "plt.plot(costos2, 'b-', linewidth=2, label='Con normalización', alpha=0.8)\n",
    "plt.title('Evolución del Costo (Lineal)')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Evolución del costo (escala log)\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(costos1, 'r-', linewidth=2, label='Sin normalización', alpha=0.8)\n",
    "plt.plot(costos2, 'b-', linewidth=2, label='Con normalización', alpha=0.8)\n",
    "plt.title('Evolución del Costo (Log)')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo (log scale)')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Convergencia de parámetros w (sin normalización)\n",
    "plt.subplot(2, 3, 3)\n",
    "w_history1 = np.array([params[0] for params in params1])\n",
    "for j in range(n):\n",
    "    plt.plot(w_history1[:, j], label=f'w[{j}] ({feature_names[j].split()[0]})', linewidth=2)\n",
    "plt.title('Parámetros w (Sin Normalización)')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Valor de w')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 4: Convergencia de parámetros w (con normalización)\n",
    "plt.subplot(2, 3, 4)\n",
    "w_history2 = np.array([params[0] for params in params2])\n",
    "for j in range(n):\n",
    "    plt.plot(w_history2[:, j], label=f'w[{j}] ({feature_names[j].split()[0]})', linewidth=2)\n",
    "plt.title('Parámetros w (Con Normalización)')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Valor de w')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 5: Comparación de gradientes (magnitud)\n",
    "plt.subplot(2, 3, 5)\n",
    "# Calcular norma de gradientes en cada iteración\n",
    "normas_grad1 = []\n",
    "normas_grad2 = []\n",
    "\n",
    "for i in range(0, len(costos1), 50):  # Cada 50 iteraciones para velocidad\n",
    "    w_i, b_i = params1[i]\n",
    "    dj_dw, _ = calcular_gradientes_multivariable(X_train, y_train, w_i, b_i)\n",
    "    normas_grad1.append(np.linalg.norm(dj_dw))\n",
    "    \n",
    "    w_i, b_i = params2[i]\n",
    "    dj_dw, _ = calcular_gradientes_multivariable(X_norm, y_train, w_i, b_i)\n",
    "    normas_grad2.append(np.linalg.norm(dj_dw))\n",
    "\n",
    "iters_sample = range(0, len(costos1), 50)\n",
    "plt.plot(iters_sample, normas_grad1, 'r-', linewidth=2, label='Sin normalización', alpha=0.8)\n",
    "plt.plot(iters_sample, normas_grad2, 'b-', linewidth=2, label='Con normalización', alpha=0.8)\n",
    "plt.title('Magnitud de Gradientes')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('||∇J|| (Norma del gradiente)')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 6: Superficie de costo (conceptual)\n",
    "plt.subplot(2, 3, 6)\n",
    "# Crear una representación 2D simplificada de la superficie de costo\n",
    "w0_range = np.linspace(-2, 2, 50)\n",
    "w1_range = np.linspace(-2, 2, 50)\n",
    "W0, W1 = np.meshgrid(w0_range, w1_range)\n",
    "\n",
    "# Función de costo simulada (elíptica para mostrar el concepto)\n",
    "Z_normalized = (W0**2 + W1**2)  # Circular (normalizado)\n",
    "Z_unnormalized = (W0**2 * 100 + W1**2)  # Elíptico (sin normalizar)\n",
    "\n",
    "plt.contour(W0, W1, Z_normalized, levels=10, colors='blue', alpha=0.6, linewidths=1)\n",
    "plt.contour(W0, W1, Z_unnormalized, levels=10, colors='red', alpha=0.6, linewidths=1)\n",
    "plt.title('Superficie de Costo (Conceptual)')\n",
    "plt.xlabel('w0')\n",
    "plt.ylabel('w1')\n",
    "plt.legend(['Normalizado (circular)', 'Sin normalizar (elíptico)'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"📊 Análisis de las visualizaciones:\")\n",
    "print(f\"   1. Costo: Ambos convergen, pero normalización es más rápida\")\n",
    "print(f\"   2. Log scale: Muestra velocidad de convergencia superior\")\n",
    "print(f\"   3. Parámetros sin norm: Convergencia desigual y oscilatoria\")\n",
    "print(f\"   4. Parámetros con norm: Convergencia suave y uniforme\")\n",
    "print(f\"   5. Gradientes: Decaen más rápido con normalización\")\n",
    "print(f\"   6. Superficie: Forma circular vs elíptica explica la diferencia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 🎛️ Optimización del Learning Rate\n",
    "\n",
    "### 4.1 Búsqueda del Learning Rate Óptimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para probar diferentes learning rates\n",
    "def probar_learning_rates(X, y, learning_rates, iteraciones=500):\n",
    "    \"\"\"\n",
    "    Prueba diferentes learning rates y retorna resultados\n",
    "    \"\"\"\n",
    "    resultados = []\n",
    "    \n",
    "    for alpha in learning_rates:\n",
    "        try:\n",
    "            w, b, costos, _ = gradient_descent_mejorado(\n",
    "                X, y, np.zeros(X.shape[1]), 0., alpha, iteraciones, verbose=False\n",
    "            )\n",
    "            \n",
    "            # Verificar si divergió\n",
    "            if np.isnan(costos[-1]) or np.isinf(costos[-1]) or costos[-1] > costos[0] * 10:\n",
    "                status = \"Diverge\"\n",
    "                costo_final = float('inf')\n",
    "            else:\n",
    "                status = \"Converge\"\n",
    "                costo_final = costos[-1]\n",
    "            \n",
    "            resultados.append({\n",
    "                'alpha': alpha,\n",
    "                'costo_final': costo_final,\n",
    "                'status': status,\n",
    "                'costos': costos,\n",
    "                'reduccion': ((costos[0] - costo_final) / costos[0] * 100) if costo_final != float('inf') else 0\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            resultados.append({\n",
    "                'alpha': alpha,\n",
    "                'costo_final': float('inf'),\n",
    "                'status': 'Error',\n",
    "                'costos': [],\n",
    "                'reduccion': 0\n",
    "            })\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "# Probar rango amplio de learning rates\n",
    "learning_rates_test = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0, 3.0]\n",
    "\n",
    "print(f\"🧪 EXPERIMENTO: Búsqueda de Learning Rate Óptimo\")\n",
    "print(f\"=\" * 55)\n",
    "print(f\"Probando {len(learning_rates_test)} valores de α con datos normalizados...\")\n",
    "\n",
    "resultados_lr = probar_learning_rates(X_norm, y_train, learning_rates_test)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"\\n{'α (Learning Rate)':<15} {'Status':<10} {'Costo Final':<12} {'Reducción %':<12}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for resultado in resultados_lr:\n",
    "    if resultado['costo_final'] == float('inf'):\n",
    "        costo_str = \"∞ (Diverge)\"\n",
    "        reduccion_str = \"0.0%\"\n",
    "    else:\n",
    "        costo_str = f\"{resultado['costo_final']:.2f}\"\n",
    "        reduccion_str = f\"{resultado['reduccion']:.1f}%\"\n",
    "    \n",
    "    print(f\"{resultado['alpha']:<15} {resultado['status']:<10} {costo_str:<12} {reduccion_str:<12}\")\n",
    "\n",
    "# Encontrar el mejor learning rate\n",
    "resultados_validos = [r for r in resultados_lr if r['status'] == 'Converge']\n",
    "if resultados_validos:\n",
    "    mejor_resultado = min(resultados_validos, key=lambda x: x['costo_final'])\n",
    "    print(f\"\\n🏆 Mejor learning rate: α = {mejor_resultado['alpha']}\")\n",
    "    print(f\"   Costo final: {mejor_resultado['costo_final']:.4f}\")\n",
    "    print(f\"   Reducción: {mejor_resultado['reduccion']:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Visualización de Learning Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comportamiento de diferentes learning rates\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Colores para cada learning rate\n",
    "colores = plt.cm.viridis(np.linspace(0, 1, len(learning_rates_test)))\n",
    "\n",
    "# Subplot 1: Evolución del costo para diferentes α\n",
    "plt.subplot(2, 2, 1)\n",
    "for i, (resultado, color) in enumerate(zip(resultados_lr, colores)):\n",
    "    if resultado['costos'] and len(resultado['costos']) > 0:\n",
    "        # Limitar costos para visualización\n",
    "        costos_plot = np.array(resultado['costos'])\n",
    "        costos_plot = np.clip(costos_plot, 0, 10000)  # Limitar valores extremos\n",
    "        \n",
    "        plt.plot(costos_plot, color=color, linewidth=2, \n",
    "                label=f\"α={resultado['alpha']:.3f}\" + \n",
    "                      (\" (Diverge)\" if resultado['status'] != 'Converge' else \"\"))\n",
    "\n",
    "plt.title('Evolución del Costo vs Learning Rate')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Subplot 2: Costo final vs learning rate\n",
    "plt.subplot(2, 2, 2)\n",
    "alphas_plot = []\n",
    "costos_finales = []\n",
    "colores_status = []\n",
    "\n",
    "for resultado in resultados_lr:\n",
    "    alphas_plot.append(resultado['alpha'])\n",
    "    if resultado['costo_final'] == float('inf'):\n",
    "        costos_finales.append(1000)  # Valor alto para visualización\n",
    "        colores_status.append('red')\n",
    "    else:\n",
    "        costos_finales.append(resultado['costo_final'])\n",
    "        colores_status.append('green')\n",
    "\n",
    "plt.scatter(alphas_plot, costos_finales, c=colores_status, s=100, alpha=0.7)\n",
    "plt.title('Costo Final vs Learning Rate')\n",
    "plt.xlabel('Learning Rate (α)')\n",
    "plt.ylabel('Costo Final')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Añadir línea vertical en el mejor α\n",
    "if resultados_validos:\n",
    "    plt.axvline(x=mejor_resultado['alpha'], color='blue', linestyle='--', \n",
    "                linewidth=2, label=f'Mejor: α={mejor_resultado[\"alpha\"]}')\n",
    "    plt.legend()\n",
    "\n",
    "# Subplot 3: Comparación específica de algunos α buenos\n",
    "plt.subplot(2, 2, 3)\n",
    "alphas_comparar = [0.01, 0.1, 1.0]  # Seleccionar algunos para comparación detallada\n",
    "\n",
    "for alpha in alphas_comparar:\n",
    "    # Encontrar resultado correspondiente\n",
    "    resultado = next((r for r in resultados_lr if r['alpha'] == alpha), None)\n",
    "    if resultado and resultado['costos']:\n",
    "        plt.plot(resultado['costos'][:200], linewidth=2, label=f'α = {alpha}')\n",
    "\n",
    "plt.title('Comparación Detallada (Primeras 200 iteraciones)')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 4: Velocidad de convergencia\n",
    "plt.subplot(2, 2, 4)\n",
    "# Calcular iteraciones para alcanzar 95% de convergencia\n",
    "iteraciones_conv = []\n",
    "alphas_conv = []\n",
    "\n",
    "for resultado in resultados_validos:\n",
    "    costos = np.array(resultado['costos'])\n",
    "    if len(costos) > 50:\n",
    "        costo_inicial = costos[0]\n",
    "        costo_final = costos[-1]\n",
    "        umbral_95 = costo_inicial - 0.95 * (costo_inicial - costo_final)\n",
    "        \n",
    "        # Encontrar cuándo alcanza 95% de convergencia\n",
    "        idx_conv = np.where(costos <= umbral_95)[0]\n",
    "        if len(idx_conv) > 0:\n",
    "            iteraciones_conv.append(idx_conv[0])\n",
    "            alphas_conv.append(resultado['alpha'])\n",
    "\n",
    "if iteraciones_conv:\n",
    "    plt.scatter(alphas_conv, iteraciones_conv, s=100, alpha=0.7)\n",
    "    plt.title('Velocidad de Convergencia\\n(Iteraciones para 95% convergencia)')\n",
    "    plt.xlabel('Learning Rate (α)')\n",
    "    plt.ylabel('Iteraciones')\n",
    "    plt.xscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 Análisis de Learning Rate:\")\n",
    "print(f\"   🟢 α muy pequeño (< 0.01): Converge lento\")\n",
    "print(f\"   🟡 α moderado (0.01-0.3): Convergencia óptima\")\n",
    "print(f\"   🔴 α muy grande (> 1.0): Puede divergir\")\n",
    "print(f\"   🎯 Rango recomendado: 0.01 - 0.3 para datos normalizados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 🚀 Modelo Optimizado Final\n",
    "\n",
    "### 5.1 Entrenar el Mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar modelo final con configuración óptima\n",
    "print(f\"🚀 ENTRENAMIENTO FINAL: Modelo Optimizado\")\n",
    "print(f\"=\" * 50)\n",
    "\n",
    "# Configuración óptima\n",
    "alpha_optimo = mejor_resultado['alpha'] if 'mejor_resultado' in locals() else 0.1\n",
    "iteraciones_final = 1000\n",
    "\n",
    "print(f\"Configuración:\")\n",
    "print(f\"   • Feature scaling: Z-score normalization\")\n",
    "print(f\"   • Learning rate: α = {alpha_optimo}\")\n",
    "print(f\"   • Iteraciones: {iteraciones_final}\")\n",
    "\n",
    "# Entrenar modelo final\n",
    "w_final, b_final, costos_final, params_final = gradient_descent_mejorado(\n",
    "    X_norm, y_train, np.zeros(n), 0., alpha_optimo, iteraciones_final, verbose=True\n",
    ")\n",
    "\n",
    "print(f\"\\n🎉 RESULTADOS FINALES:\")\n",
    "print(f\"   Costo inicial: {costos_final[0]:.4f}\")\n",
    "print(f\"   Costo final:   {costos_final[-1]:.4f}\")\n",
    "print(f\"   Reducción:     {((costos_final[0] - costos_final[-1])/costos_final[0]*100):.1f}%\")\n",
    "\n",
    "print(f\"\\n📊 Parámetros del modelo normalizado:\")\n",
    "for i, (feature, peso) in enumerate(zip(feature_names, w_final)):\n",
    "    print(f\"   w[{i}] ({feature}): {peso:8.4f}\")\n",
    "print(f\"   b (bias): {b_final:8.4f}\")\n",
    "\n",
    "# Evaluar modelo\n",
    "predicciones_norm = [np.dot(X_norm[i], w_final) + b_final for i in range(len(X_norm))]\n",
    "residuos = [pred - real for pred, real in zip(predicciones_norm, y_train)]\n",
    "\n",
    "# Métricas\n",
    "mae = np.mean([abs(r) for r in residuos])\n",
    "rmse = np.sqrt(np.mean([r**2 for r in residuos]))\n",
    "ss_res = sum([r**2 for r in residuos])\n",
    "ss_tot = sum([(real - np.mean(y_train))**2 for real in y_train])\n",
    "r2 = 1 - (ss_res / ss_tot)\n",
    "\n",
    "print(f\"\\n📈 Métricas de rendimiento:\")\n",
    "print(f\"   MAE (Error Absoluto Medio):  ${mae:.1f}k\")\n",
    "print(f\"   RMSE (Raíz Error Cuadrático): ${rmse:.1f}k\")\n",
    "print(f\"   R² (Coef. Determinación):     {r2:.4f} ({r2*100:.1f}% varianza explicada)\")\n",
    "\n",
    "if r2 > 0.9:\n",
    "    print(f\"   ✅ Excelente ajuste (R² > 0.9)\")\n",
    "elif r2 > 0.8:\n",
    "    print(f\"   ✅ Muy buen ajuste (R² > 0.8)\")\n",
    "else:\n",
    "    print(f\"   ⚠️  Ajuste aceptable pero mejorable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Función de Predicción para Nuevos Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear función de predicción completa que maneje normalización\n",
    "def predecir_precio_casa(tamaño, habitaciones, pisos, edad, w, b, mu, sigma):\n",
    "    \"\"\"\n",
    "    Predice el precio de una casa usando el modelo entrenado\n",
    "    \n",
    "    Args:\n",
    "        tamaño: Tamaño en pies cuadrados\n",
    "        habitaciones: Número de habitaciones\n",
    "        pisos: Número de pisos\n",
    "        edad: Edad en años\n",
    "        w, b: Parámetros del modelo normalizado\n",
    "        mu, sigma: Estadísticas de normalización\n",
    "    \n",
    "    Returns:\n",
    "        precio: Precio predicho en miles de dólares\n",
    "    \"\"\"\n",
    "    # 1. Crear vector de features\n",
    "    x_casa = np.array([tamaño, habitaciones, pisos, edad])\n",
    "    \n",
    "    # 2. Normalizar usando estadísticas del entrenamiento\n",
    "    x_norm = (x_casa - mu) / sigma\n",
    "    \n",
    "    # 3. Hacer predicción\n",
    "    precio = np.dot(x_norm, w) + b\n",
    "    \n",
    "    return precio\n",
    "\n",
    "# Función para análisis detallado\n",
    "def analisis_prediccion_detallado(tamaño, habitaciones, pisos, edad, w, b, mu, sigma):\n",
    "    \"\"\"\n",
    "    Análisis detallado de una predicción mostrando cada paso\n",
    "    \"\"\"\n",
    "    print(f\"🔍 ANÁLISIS DETALLADO DE PREDICCIÓN\")\n",
    "    print(f\"=\" * 40)\n",
    "    print(f\"Casa: {tamaño} sqft, {habitaciones} hab, {pisos} pisos, {edad} años\")\n",
    "    \n",
    "    # Paso 1: Features originales\n",
    "    x_original = np.array([tamaño, habitaciones, pisos, edad])\n",
    "    print(f\"\\n1️⃣ Features originales: {x_original}\")\n",
    "    \n",
    "    # Paso 2: Normalización\n",
    "    x_norm = (x_original - mu) / sigma\n",
    "    print(f\"\\n2️⃣ Normalización (z-score):\")\n",
    "    for i, (feature, orig, norm, m, s) in enumerate(zip(feature_names, x_original, x_norm, mu, sigma)):\n",
    "        print(f\"   {feature}: ({orig:.0f} - {m:.1f}) / {s:.1f} = {norm:.3f}\")\n",
    "    \n",
    "    # Paso 3: Cálculo de predicción\n",
    "    print(f\"\\n3️⃣ Cálculo de predicción:\")\n",
    "    contribuciones = w * x_norm\n",
    "    for i, (feature, peso, x_n, contrib) in enumerate(zip(feature_names, w, x_norm, contribuciones)):\n",
    "        print(f\"   {feature}: {peso:.4f} × {x_n:.3f} = {contrib:.3f}\")\n",
    "    \n",
    "    suma_contrib = np.sum(contribuciones)\n",
    "    precio_final = suma_contrib + b\n",
    "    \n",
    "    print(f\"   Suma de contribuciones: {suma_contrib:.3f}\")\n",
    "    print(f\"   + Bias: {b:.3f}\")\n",
    "    print(f\"   = Precio final: ${precio_final:.0f}k\")\n",
    "    \n",
    "    return precio_final\n",
    "\n",
    "# Ejemplos de predicción\n",
    "ejemplos_casas = [\n",
    "    (1200, 3, 1, 15, \"Casa familiar típica\"),\n",
    "    (2500, 4, 2, 5, \"Casa grande y nueva\"),\n",
    "    (800, 2, 1, 40, \"Casa pequeña, antigua\"),\n",
    "    (3200, 5, 2, 2, \"Mansión nueva\"),\n",
    "]\n",
    "\n",
    "print(f\"\\n🏠 PREDICCIONES PARA CASAS NUEVAS\")\n",
    "print(f\"=\" * 45)\n",
    "print(f\"{'Descripción':<20} {'Features':<20} {'Precio':<12}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for tamaño, hab, pisos, edad, desc in ejemplos_casas:\n",
    "    precio = predecir_precio_casa(tamaño, hab, pisos, edad, w_final, b_final, X_mu, X_sigma)\n",
    "    features_str = f\"{tamaño}sq,{hab}b,{pisos}f,{edad}y\"\n",
    "    print(f\"{desc:<20} {features_str:<20} ${precio:<11.0f}\")\n",
    "\n",
    "# Análisis detallado de un ejemplo\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "ejemplo_detalle = ejemplos_casas[0]  # Casa familiar típica\n",
    "analisis_prediccion_detallado(ejemplo_detalle[0], ejemplo_detalle[1], \n",
    "                             ejemplo_detalle[2], ejemplo_detalle[3], \n",
    "                             w_final, b_final, X_mu, X_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Comparación Final: Antes vs Después"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación final completa\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Datos para comparación (recrear el entrenamiento sin normalización para comparar)\n",
    "w_sin_norm, b_sin_norm, costos_sin_norm, _ = gradient_descent_mejorado(\n",
    "    X_train, y_train, np.zeros(n), 0., 5e-7, 1000, verbose=False\n",
    ")\n",
    "\n",
    "# Subplot 1: Comparación de convergencia\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(costos_sin_norm, 'r-', linewidth=2, label='Sin normalización', alpha=0.8)\n",
    "plt.plot(costos_final, 'g-', linewidth=2, label='Con normalización optimizada', alpha=0.8)\n",
    "plt.title('Comparación de Convergencia')\n",
    "plt.xlabel('Iteraciones')\n",
    "plt.ylabel('Costo')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Subplot 2: Predicciones vs valores reales (modelo optimizado)\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.scatter(predicciones_norm, y_train, alpha=0.7, s=60, color='green')\n",
    "min_val = min(min(predicciones_norm), min(y_train))\n",
    "max_val = max(max(predicciones_norm), max(y_train))\n",
    "plt.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfecto')\n",
    "plt.title('Predicciones vs Reales (Optimizado)')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores Reales')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 3: Distribución de residuos\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.hist(residuos, bins=10, alpha=0.7, edgecolor='black', color='green')\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "plt.title('Distribución de Residuos')\n",
    "plt.xlabel('Residuos ($k)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 4: Importancia de features (pesos normalizados)\n",
    "plt.subplot(2, 3, 4)\n",
    "importancia = np.abs(w_final)\n",
    "indices_ordenados = np.argsort(importancia)[::-1]\n",
    "colores_bars = ['gold', 'silver', 'chocolate', 'lightblue']\n",
    "\n",
    "bars = plt.bar(range(n), importancia[indices_ordenados], \n",
    "               color=[colores_bars[i] for i in indices_ordenados], \n",
    "               alpha=0.7, edgecolor='black')\n",
    "plt.title('Importancia de Features\\n(Magnitud de Pesos Normalizados)')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('|Peso|')\n",
    "plt.xticks(range(n), [feature_names[i].split()[0] for i in indices_ordenados], rotation=45)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for bar, valor in zip(bars, importancia[indices_ordenados]):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "             f'{valor:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Subplot 5: Comparación de métricas\n",
    "plt.subplot(2, 3, 5)\n",
    "# Calcular métricas del modelo sin normalizar\n",
    "pred_sin_norm = [np.dot(X_train[i], w_sin_norm) + b_sin_norm for i in range(len(X_train))]\n",
    "residuos_sin_norm = [pred - real for pred, real in zip(pred_sin_norm, y_train)]\n",
    "mae_sin_norm = np.mean([abs(r) for r in residuos_sin_norm])\n",
    "rmse_sin_norm = np.sqrt(np.mean([r**2 for r in residuos_sin_norm]))\n",
    "\n",
    "metricas_nombres = ['MAE ($k)', 'RMSE ($k)']\n",
    "sin_norm_vals = [mae_sin_norm, rmse_sin_norm]\n",
    "con_norm_vals = [mae, rmse]\n",
    "\n",
    "x_pos = np.arange(len(metricas_nombres))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x_pos - width/2, sin_norm_vals, width, label='Sin normalización', \n",
    "        color='red', alpha=0.7, edgecolor='black')\n",
    "plt.bar(x_pos + width/2, con_norm_vals, width, label='Con normalización', \n",
    "        color='green', alpha=0.7, edgecolor='black')\n",
    "\n",
    "plt.title('Comparación de Métricas')\n",
    "plt.xlabel('Métrica')\n",
    "plt.ylabel('Valor')\n",
    "plt.xticks(x_pos, metricas_nombres)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Subplot 6: Resumen de mejoras\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.text(0.1, 0.9, '🎯 MEJORAS LOGRADAS', fontsize=14, fontweight='bold', \n",
    "         transform=plt.gca().transAxes)\n",
    "\n",
    "mejoras_texto = f\"\"\"\n",
    "⚡ Learning Rate:\n",
    "   • Antes: 5.0e-7\n",
    "   • Después: {alpha_optimo}\n",
    "   • Mejora: {alpha_optimo/5e-7:.0f}x más rápido\n",
    "\n",
    "📊 Convergencia:\n",
    "   • Más estable y predecible\n",
    "   • Sin oscilaciones\n",
    "   • Gradientes balanceados\n",
    "\n",
    "🎯 Rendimiento:\n",
    "   • MAE: {mae_sin_norm:.1f} → {mae:.1f}\n",
    "   • RMSE: {rmse_sin_norm:.1f} → {rmse:.1f}\n",
    "   • R²: {r2:.3f} ({r2*100:.1f}%)\n",
    "\n",
    "✅ Resultado:\n",
    "   Modelo robusto y eficiente\n",
    "   para producción\n",
    "\"\"\"\n",
    "\n",
    "plt.text(0.05, 0.8, mejoras_texto, fontsize=10, \n",
    "         transform=plt.gca().transAxes, verticalalignment='top')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🎉 RESUMEN FINAL\")\n",
    "print(f\"=\" * 30)\n",
    "print(f\"✅ Feature scaling implementado exitosamente\")\n",
    "print(f\"✅ Learning rate optimizado: {alpha_optimo/5e-7:.0f}x más rápido\")\n",
    "print(f\"✅ Modelo con R² = {r2:.3f} ({r2*100:.1f}% de varianza explicada)\")\n",
    "print(f\"✅ Convergencia estable y predecible\")\n",
    "print(f\"✅ Listo para hacer predicciones en nuevos datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 🧪 Ejercicios Prácticos\n",
    "\n",
    "### Ejercicio 1: Implementar Diferentes Técnicas de Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 1: Comparar técnicas de normalización\n",
    "print(f\"🧪 EJERCICIO 1: Comparación de Técnicas de Normalización\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "def robust_normalize_features(X):\n",
    "    \"\"\"\n",
    "    Normalización robusta usando mediana y rango intercuartil\n",
    "    Menos sensible a outliers que Z-score\n",
    "    \"\"\"\n",
    "    median = np.median(X, axis=0)\n",
    "    q75 = np.percentile(X, 75, axis=0)\n",
    "    q25 = np.percentile(X, 25, axis=0)\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    # Evitar división por cero\n",
    "    iqr[iqr == 0] = 1\n",
    "    \n",
    "    X_robust = (X - median) / iqr\n",
    "    return X_robust, median, iqr\n",
    "\n",
    "def unit_vector_normalize(X):\n",
    "    \"\"\"\n",
    "    Normalización de vector unitario (L2 normalization)\n",
    "    Cada ejemplo tiene norma = 1\n",
    "    \"\"\"\n",
    "    norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "    norms[norms == 0] = 1  # Evitar división por cero\n",
    "    return X / norms\n",
    "\n",
    "# Aplicar todas las técnicas\n",
    "tecnicas = {\n",
    "    'Original': (X_train, None, None),\n",
    "    'Z-Score': zscore_normalize_features(X_train),\n",
    "    'Min-Max': minmax_normalize_features(X_train),\n",
    "    'Robust': robust_normalize_features(X_train),\n",
    "    'Unit Vector': (unit_vector_normalize(X_train), None, None)\n",
    "}\n",
    "\n",
    "# Entrenar modelo con cada técnica\n",
    "resultados_tecnicas = {}\n",
    "\n",
    "for nombre, (X_norm_tech, _, _) in tecnicas.items():\n",
    "    if nombre == 'Original':\n",
    "        alpha_tech = 5e-7  # Learning rate pequeño para datos originales\n",
    "    else:\n",
    "        alpha_tech = 0.1   # Learning rate grande para datos normalizados\n",
    "    \n",
    "    try:\n",
    "        w_tech, b_tech, costos_tech, _ = gradient_descent_mejorado(\n",
    "            X_norm_tech, y_train, np.zeros(n), 0., alpha_tech, 500, verbose=False\n",
    "        )\n",
    "        \n",
    "        # Métricas\n",
    "        pred_tech = [np.dot(X_norm_tech[i], w_tech) + b_tech for i in range(len(X_norm_tech))]\n",
    "        mae_tech = np.mean([abs(pred - real) for pred, real in zip(pred_tech, y_train)])\n",
    "        \n",
    "        resultados_tecnicas[nombre] = {\n",
    "            'costo_final': costos_tech[-1],\n",
    "            'mae': mae_tech,\n",
    "            'convergencia': len([c for c in costos_tech if not np.isnan(c)]),\n",
    "            'alpha': alpha_tech\n",
    "        }\n",
    "    except:\n",
    "        resultados_tecnicas[nombre] = {\n",
    "            'costo_final': float('inf'),\n",
    "            'mae': float('inf'),\n",
    "            'convergencia': 0,\n",
    "            'alpha': alpha_tech\n",
    "        }\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"{'Técnica':<15} {'α':<10} {'Costo Final':<12} {'MAE':<8} {'Convergencia':<12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for nombre, resultado in resultados_tecnicas.items():\n",
    "    costo_str = f\"{resultado['costo_final']:.2f}\" if resultado['costo_final'] != float('inf') else \"Falló\"\n",
    "    mae_str = f\"{resultado['mae']:.1f}\" if resultado['mae'] != float('inf') else \"N/A\"\n",
    "    \n",
    "    print(f\"{nombre:<15} {resultado['alpha']:<10.0e} {costo_str:<12} {mae_str:<8} {'✅' if resultado['convergencia'] > 400 else '❌':<12}\")\n",
    "\n",
    "# Encontrar la mejor técnica\n",
    "tecnicas_validas = {k: v for k, v in resultados_tecnicas.items() if v['mae'] != float('inf')}\n",
    "if tecnicas_validas:\n",
    "    mejor_tecnica = min(tecnicas_validas, key=lambda x: tecnicas_validas[x]['mae'])\n",
    "    print(f\"\\n🏆 Mejor técnica: {mejor_tecnica}\")\n",
    "    print(f\"   MAE: {tecnicas_validas[mejor_tecnica]['mae']:.1f}\")\n",
    "    print(f\"   Permite α = {tecnicas_validas[mejor_tecnica]['alpha']:.0e}\")\n",
    "\n",
    "print(f\"\\n💡 Conclusiones:\")\n",
    "print(f\"   • Z-Score y Min-Max suelen ser las mejores opciones\")\n",
    "print(f\"   • Robust normalization es útil con outliers\")\n",
    "print(f\"   • Unit vector es específica para ciertos casos\")\n",
    "print(f\"   • Normalización permite learning rates 10,000x más grandes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio 2: Análisis de Sensibilidad del Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejercicio 2: Búsqueda sistemática de learning rate óptimo\n",
    "print(f\"🧪 EJERCICIO 2: Búsqueda Sistemática de Learning Rate\")\n",
    "print(f\"=\" * 55)\n",
    "\n",
    "def busqueda_learning_rate_detallada(X, y, alpha_min=1e-4, alpha_max=10, num_puntos=20):\n",
    "    \"\"\"\n",
    "    Búsqueda sistemática de learning rate con análisis detallado\n",
    "    \"\"\"\n",
    "    # Generar rango logarítmico de learning rates\n",
    "    alphas = np.logspace(np.log10(alpha_min), np.log10(alpha_max), num_puntos)\n",
    "    \n",
    "    resultados_detallados = []\n",
    "    \n",
    "    for alpha in alphas:\n",
    "        try:\n",
    "            # Entrenamiento corto para evaluación rápida\n",
    "            w, b, costos, _ = gradient_descent_mejorado(\n",
    "                X, y, np.zeros(X.shape[1]), 0., alpha, 200, verbose=False\n",
    "            )\n",
    "            \n",
    "            # Análisis de convergencia\n",
    "            if len(costos) < 50:\n",
    "                status = \"Error temprano\"\n",
    "                velocidad_conv = 0\n",
    "                estabilidad = 0\n",
    "            elif np.isnan(costos[-1]) or np.isinf(costos[-1]):\n",
    "                status = \"Divergencia (NaN/Inf)\"\n",
    "                velocidad_conv = 0\n",
    "                estabilidad = 0\n",
    "            elif costos[-1] > costos[0] * 2:\n",
    "                status = \"Divergencia (Costo aumenta)\"\n",
    "                velocidad_conv = 0\n",
    "                estabilidad = 0\n",
    "            else:\n",
    "                status = \"Convergencia\"\n",
    "                # Velocidad: qué tan rápido baja el costo en las primeras iteraciones\n",
    "                if len(costos) >= 50:\n",
    "                    velocidad_conv = (costos[0] - costos[49]) / costos[0]\n",
    "                else:\n",
    "                    velocidad_conv = (costos[0] - costos[-1]) / costos[0]\n",
    "                \n",
    "                # Estabilidad: variación en las últimas iteraciones\n",
    "                ultimos_costos = costos[-20:] if len(costos) >= 20 else costos\n",
    "                estabilidad = 1 / (1 + np.std(ultimos_costos))\n",
    "            \n",
    "            resultados_detallados.append({\n",
    "                'alpha': alpha,\n",
    "                'status': status,\n",
    "                'costo_final': costos[-1] if costos and not np.isnan(costos[-1]) and not np.isinf(costos[-1]) else float('inf'),\n",
    "                'velocidad_convergencia': velocidad_conv,\n",
    "                'estabilidad': estabilidad,\n",
    "                'score_total': velocidad_conv * estabilidad  # Métrica combinada\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            resultados_detallados.append({\n",
    "                'alpha': alpha,\n",
    "                'status': f\"Error: {str(e)[:20]}\",\n",
    "                'costo_final': float('inf'),\n",
    "                'velocidad_convergencia': 0,\n",
    "                'estabilidad': 0,\n",
    "                'score_total': 0\n",
    "            })\n",
    "    \n",
    "    return resultados_detallados\n",
    "\n",
    "# Realizar búsqueda detallada\n",
    "print(\"Realizando búsqueda sistemática...\")\n",
    "resultados_lr_detallados = busqueda_learning_rate_detallada(X_norm, y_train)\n",
    "\n",
    "# Mostrar resultados tabulados\n",
    "print(f\"\\n{'α':<10} {'Status':<15} {'Costo Final':<12} {'Velocidad':<10} {'Estabilidad':<12} {'Score':<8}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for resultado in resultados_lr_detallados[:10]:  # Mostrar primeros 10\n",
    "    costo_str = f\"{resultado['costo_final']:.2f}\" if resultado['costo_final'] != float('inf') else \"∞\"\n",
    "    print(f\"{resultado['alpha']:<10.1e} {resultado['status']:<15} {costo_str:<12} \"\n",
    "          f\"{resultado['velocidad_convergencia']:<10.3f} {resultado['estabilidad']:<12.3f} \"\n",
    "          f\"{resultado['score_total']:<8.3f}\")\n",
    "\n",
    "# Encontrar el mejor según score combinado\n",
    "resultados_validos_det = [r for r in resultados_lr_detallados if r['status'] == 'Convergencia']\n",
    "if resultados_validos_det:\n",
    "    mejor_lr_detallado = max(resultados_validos_det, key=lambda x: x['score_total'])\n",
    "    print(f\"\\n🎯 Learning rate óptimo (análisis detallado):\")\n",
    "    print(f\"   α = {mejor_lr_detallado['alpha']:.3f}\")\n",
    "    print(f\"   Score total: {mejor_lr_detallado['score_total']:.3f}\")\n",
    "    print(f\"   Velocidad de convergencia: {mejor_lr_detallado['velocidad_convergencia']:.3f}\")\n",
    "    print(f\"   Estabilidad: {mejor_lr_detallado['estabilidad']:.3f}\")\n",
    "\n",
    "# Visualización de la búsqueda\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "alphas_plot = [r['alpha'] for r in resultados_lr_detallados]\n",
    "scores_plot = [r['score_total'] for r in resultados_lr_detallados]\n",
    "colores_status = ['green' if r['status'] == 'Convergencia' else 'red' for r in resultados_lr_detallados]\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(alphas_plot, scores_plot, c=colores_status, alpha=0.7, s=50)\n",
    "plt.xscale('log')\n",
    "plt.title('Score de Learning Rate vs α')\n",
    "plt.xlabel('Learning Rate (α)')\n",
    "plt.ylabel('Score (Velocidad × Estabilidad)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Marcar el mejor\n",
    "if resultados_validos_det:\n",
    "    plt.scatter([mejor_lr_detallado['alpha']], [mejor_lr_detallado['score_total']], \n",
    "               c='blue', s=200, marker='*', edgecolor='black', linewidth=2,\n",
    "               label=f\"Óptimo: α={mejor_lr_detallado['alpha']:.3f}\")\n",
    "    plt.legend()\n",
    "\n",
    "# Heatmap de regiones\n",
    "plt.subplot(1, 2, 2)\n",
    "velocidades = [r['velocidad_convergencia'] for r in resultados_lr_detallados]\n",
    "estabilidades = [r['estabilidad'] for r in resultados_lr_detallados]\n",
    "\n",
    "plt.scatter(velocidades, estabilidades, c=scores_plot, s=100, alpha=0.7, cmap='viridis')\n",
    "plt.colorbar(label='Score Total')\n",
    "plt.title('Velocidad vs Estabilidad')\n",
    "plt.xlabel('Velocidad de Convergencia')\n",
    "plt.ylabel('Estabilidad')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n📊 Conclusiones del análisis sistemático:\")\n",
    "print(f\"   • Zona óptima: α entre 0.03 y 0.3\")\n",
    "print(f\"   • Balance ideal entre velocidad y estabilidad\")\n",
    "print(f\"   • Evitar α < 0.001 (muy lento) y α > 1.0 (inestable)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Resumen y Conceptos Clave\n",
    "\n",
    "### ✅ Lo que has aprendido:\n",
    "\n",
    "#### 1. **Problem Diagnosis**:\n",
    "   - **Features con escalas muy diferentes** causan convergencia lenta\n",
    "   - **Gradientes desbalanceados** requieren learning rates muy pequeños\n",
    "   - **Superficies de costo elípticas** vs **circulares**\n",
    "\n",
    "#### 2. **Feature Scaling Techniques**:\n",
    "   - **Z-Score**: $(x - \\mu) / \\sigma$ → Media=0, Std=1\n",
    "   - **Min-Max**: $(x - x_{min}) / (x_{max} - x_{min})$ → Rango [0,1]\n",
    "   - **Robust**: $(x - median) / IQR$ → Resistente a outliers\n",
    "   - **Unit Vector**: $x / ||x||$ → Norma = 1\n",
    "\n",
    "#### 3. **Impact on Gradient Descent**:\n",
    "   - **Learning rate**: 10,000x más grande con normalización\n",
    "   - **Convergencia**: Más rápida, estable y predecible\n",
    "   - **Gradientes**: Magnitudes similares entre features\n",
    "\n",
    "#### 4. **Learning Rate Optimization**:\n",
    "   - **Systematic search**: Explorar rango logarítmico\n",
    "   - **Balance**: Velocidad vs estabilidad\n",
    "   - **Zona óptima**: 0.01 - 0.3 para datos normalizados\n",
    "\n",
    "#### 5. **Production Ready Model**:\n",
    "   - **Normalización**: Aplicar a nuevos datos usando estadísticas del entrenamiento\n",
    "   - **Pipeline completo**: Normalizar → Predecir → Desnormalizar (si necesario)\n",
    "   - **Métricas**: R² = {r2:.3f} ({r2*100:.1f}% varianza explicada)\n",
    "\n",
    "### 🚀 Mejoras Logradas:\n",
    "- **Velocidad**: {alpha_optimo/5e-7:.0f}x más rápido\n",
    "- **Estabilidad**: Sin oscilaciones ni divergencia\n",
    "- **Rendimiento**: MAE reducido, R² mejorado\n",
    "- **Robustez**: Modelo listo para producción\n",
    "\n",
    "### 🎯 Próximos pasos:\n",
    "- **Feature Engineering**: Crear features polinomiales y transformaciones\n",
    "- **Regularización**: L1/L2 para prevenir overfitting\n",
    "- **Advanced Optimization**: Adam, RMSprop, momentum\n",
    "- **Cross-validation**: Validación robusta del modelo\n",
    "\n",
    "### 💡 Best Practices:\n",
    "1. **Siempre normalizar** cuando features tienen escalas diferentes\n",
    "2. **Guardar estadísticas** (μ, σ) del conjunto de entrenamiento\n",
    "3. **Aplicar misma normalización** a datos nuevos\n",
    "4. **Experimentar con learning rates** en rango logarítmico\n",
    "5. **Monitorear convergencia** con múltiples métricas\n",
    "6. **Validar en datos de prueba** antes de producción\n",
    "\n",
    "### ⚠️ Cuidados importantes:\n",
    "- **Nunca normalizar** usando estadísticas de datos de prueba\n",
    "- **Feature scaling no siempre mejora** todos los algoritmos\n",
    "- **Learning rate muy alto** puede causar divergencia\n",
    "- **Interpretar pesos** de modelo normalizado requiere cuidado\n",
    "\n",
    "**Este notebook te ha dado las herramientas fundamentales para optimizar gradient descent y crear modelos de ML robustos y eficientes.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}